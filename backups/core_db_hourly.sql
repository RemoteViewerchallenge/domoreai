--
-- PostgreSQL database dump
--

\restrict vHk8bogOiRid6mmcTdpYmbj4m5GbNzj7zyTMeXWKI7f8we07X35ig6J4oEG5YZD

-- Dumped from database version 15.15
-- Dumped by pg_dump version 18.1

SET statement_timeout = 0;
SET lock_timeout = 0;
SET idle_in_transaction_session_timeout = 0;
SET transaction_timeout = 0;
SET client_encoding = 'UTF8';
SET standard_conforming_strings = on;
SELECT pg_catalog.set_config('search_path', '', false);
SET check_function_bodies = false;
SET xmloption = content;
SET client_min_messages = warning;
SET row_security = off;

DROP INDEX public."modelcapabilities_modelId_key";
DROP INDEX public."VisionModel_modelId_key";
DROP INDEX public."UnknownModel_modelId_key";
DROP INDEX public."Tool_name_key";
DROP INDEX public."Role_name_key";
DROP INDEX public."RoleCategory_name_key";
DROP INDEX public."RewardModel_modelId_key";
DROP INDEX public."Model_providerId_name_key";
DROP INDEX public."ModelFailure_providerId_modelId_key";
DROP INDEX public."KnowledgeVector_entityType_entityId_idx";
DROP INDEX public."ImageModel_modelId_key";
DROP INDEX public."EmbeddingModel_modelId_key";
DROP INDEX public."ComplianceModel_modelId_key";
DROP INDEX public."ChatModel_modelId_key";
DROP INDEX public."AudioModel_modelId_key";
ALTER TABLE ONLY public.openrouter_models DROP CONSTRAINT openrouter_models_pkey;
ALTER TABLE ONLY public.ollama_models DROP CONSTRAINT ollama_models_pkey;
ALTER TABLE ONLY public.nvidia_models DROP CONSTRAINT nvidia_models_pkey;
ALTER TABLE ONLY public.modelcapabilities DROP CONSTRAINT modelcapabilities_pkey;
ALTER TABLE ONLY public.model_registry DROP CONSTRAINT model_registry_pkey;
ALTER TABLE ONLY public.mistral_models DROP CONSTRAINT mistral_models_pkey;
ALTER TABLE ONLY public.groq_models DROP CONSTRAINT groq_models_pkey;
ALTER TABLE ONLY public.google_models_example DROP CONSTRAINT google_models_example_pkey;
ALTER TABLE ONLY public.cerebras_models DROP CONSTRAINT cerebras_models_pkey;
ALTER TABLE ONLY public._prisma_migrations DROP CONSTRAINT _prisma_migrations_pkey;
ALTER TABLE ONLY public."Workspace" DROP CONSTRAINT "Workspace_pkey";
ALTER TABLE ONLY public."WorkOrderCard" DROP CONSTRAINT "WorkOrderCard_pkey";
ALTER TABLE ONLY public."VisionModel" DROP CONSTRAINT "VisionModel_pkey";
ALTER TABLE ONLY public."UnknownModel" DROP CONSTRAINT "UnknownModel_pkey";
ALTER TABLE ONLY public."Tool" DROP CONSTRAINT "Tool_pkey";
ALTER TABLE ONLY public."Role" DROP CONSTRAINT "Role_pkey";
ALTER TABLE ONLY public."RoleVariant" DROP CONSTRAINT "RoleVariant_pkey";
ALTER TABLE ONLY public."RoleTool" DROP CONSTRAINT "RoleTool_pkey";
ALTER TABLE ONLY public."RoleCategory" DROP CONSTRAINT "RoleCategory_pkey";
ALTER TABLE ONLY public."RoleAssessment" DROP CONSTRAINT "RoleAssessment_pkey";
ALTER TABLE ONLY public."RewardModel" DROP CONSTRAINT "RewardModel_pkey";
ALTER TABLE ONLY public."ProviderConfig" DROP CONSTRAINT "ProviderConfig_pkey";
ALTER TABLE ONLY public."PromptRefinement" DROP CONSTRAINT "PromptRefinement_pkey";
ALTER TABLE ONLY public."Model" DROP CONSTRAINT "Model_pkey";
ALTER TABLE ONLY public."ModelUsage" DROP CONSTRAINT "ModelUsage_pkey";
ALTER TABLE ONLY public."ModelFailure" DROP CONSTRAINT "ModelFailure_pkey";
ALTER TABLE ONLY public."KnowledgeVector" DROP CONSTRAINT "KnowledgeVector_pkey";
ALTER TABLE ONLY public."Job" DROP CONSTRAINT "Job_pkey";
ALTER TABLE ONLY public."ImageModel" DROP CONSTRAINT "ImageModel_pkey";
ALTER TABLE ONLY public."FileIndex" DROP CONSTRAINT "FileIndex_pkey";
ALTER TABLE ONLY public."EmbeddingModel" DROP CONSTRAINT "EmbeddingModel_pkey";
ALTER TABLE ONLY public."CustomButton" DROP CONSTRAINT "CustomButton_pkey";
ALTER TABLE ONLY public."ComponentRole" DROP CONSTRAINT "ComponentRole_pkey";
ALTER TABLE ONLY public."ComplianceModel" DROP CONSTRAINT "ComplianceModel_pkey";
ALTER TABLE ONLY public."ChatModel" DROP CONSTRAINT "ChatModel_pkey";
ALTER TABLE ONLY public."CardConfig" DROP CONSTRAINT "CardConfig_pkey";
ALTER TABLE ONLY public."AudioModel" DROP CONSTRAINT "AudioModel_pkey";
ALTER TABLE ONLY drizzle.__drizzle_migrations DROP CONSTRAINT __drizzle_migrations_pkey;
ALTER TABLE drizzle.__drizzle_migrations ALTER COLUMN id DROP DEFAULT;
DROP TABLE public.openrouter_models;
DROP TABLE public.ollama_models;
DROP TABLE public.nvidia_models;
DROP TABLE public.modelcapabilities;
DROP TABLE public.model_registry;
DROP TABLE public.mistral_models;
DROP TABLE public.groq_models;
DROP TABLE public.google_models_example;
DROP TABLE public.cerebras_models;
DROP TABLE public._prisma_migrations;
DROP TABLE public."Workspace";
DROP TABLE public."WorkOrderCard";
DROP TABLE public."VisionModel";
DROP TABLE public."UnknownModel";
DROP TABLE public."Tool";
DROP TABLE public."RoleVariant";
DROP TABLE public."RoleTool";
DROP TABLE public."RoleCategory";
DROP TABLE public."RoleAssessment";
DROP TABLE public."Role";
DROP TABLE public."RewardModel";
DROP TABLE public."ProviderConfig";
DROP TABLE public."PromptRefinement";
DROP TABLE public."ModelUsage";
DROP TABLE public."ModelFailure";
DROP TABLE public."Model";
DROP TABLE public."KnowledgeVector";
DROP TABLE public."Job";
DROP TABLE public."ImageModel";
DROP TABLE public."FileIndex";
DROP TABLE public."EmbeddingModel";
DROP TABLE public."CustomButton";
DROP TABLE public."ComponentRole";
DROP TABLE public."ComplianceModel";
DROP TABLE public."ChatModel";
DROP TABLE public."CardConfig";
DROP TABLE public."AudioModel";
DROP SEQUENCE drizzle.__drizzle_migrations_id_seq;
DROP TABLE drizzle.__drizzle_migrations;
-- *not* dropping schema, since initdb creates it
DROP SCHEMA drizzle;
--
-- Name: drizzle; Type: SCHEMA; Schema: -; Owner: -
--

CREATE SCHEMA drizzle;


--
-- Name: public; Type: SCHEMA; Schema: -; Owner: -
--

-- *not* creating schema, since initdb creates it


--
-- Name: SCHEMA public; Type: COMMENT; Schema: -; Owner: -
--

COMMENT ON SCHEMA public IS '';


SET default_tablespace = '';

SET default_table_access_method = heap;

--
-- Name: __drizzle_migrations; Type: TABLE; Schema: drizzle; Owner: -
--

CREATE TABLE drizzle.__drizzle_migrations (
    id integer NOT NULL,
    hash text NOT NULL,
    created_at bigint
);


--
-- Name: __drizzle_migrations_id_seq; Type: SEQUENCE; Schema: drizzle; Owner: -
--

CREATE SEQUENCE drizzle.__drizzle_migrations_id_seq
    AS integer
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


--
-- Name: __drizzle_migrations_id_seq; Type: SEQUENCE OWNED BY; Schema: drizzle; Owner: -
--

ALTER SEQUENCE drizzle.__drizzle_migrations_id_seq OWNED BY drizzle.__drizzle_migrations.id;


--
-- Name: AudioModel; Type: TABLE; Schema: public; Owner: -
--

CREATE TABLE public."AudioModel" (
    id text NOT NULL,
    "modelId" text NOT NULL,
    voices jsonb DEFAULT '[]'::jsonb,
    "sampleRates" jsonb DEFAULT '[]'::jsonb
);


--
-- Name: CardConfig; Type: TABLE; Schema: public; Owner: -
--

CREATE TABLE public."CardConfig" (
    "cardId" text NOT NULL
);


--
-- Name: ChatModel; Type: TABLE; Schema: public; Owner: -
--

CREATE TABLE public."ChatModel" (
    id text NOT NULL,
    "modelId" text NOT NULL,
    "contextWindow" integer,
    "supportsTools" boolean DEFAULT true NOT NULL,
    "supportsJson" boolean DEFAULT true NOT NULL
);


--
-- Name: ComplianceModel; Type: TABLE; Schema: public; Owner: -
--

CREATE TABLE public."ComplianceModel" (
    id text NOT NULL,
    "modelId" text NOT NULL,
    categories jsonb DEFAULT '[]'::jsonb
);


--
-- Name: ComponentRole; Type: TABLE; Schema: public; Owner: -
--

CREATE TABLE public."ComponentRole" (
    id text NOT NULL,
    "cardId" text NOT NULL,
    component text NOT NULL,
    "roleId" text
);


--
-- Name: CustomButton; Type: TABLE; Schema: public; Owner: -
--

CREATE TABLE public."CustomButton" (
    id text NOT NULL,
    "cardId" text NOT NULL,
    label text NOT NULL,
    action text NOT NULL,
    "actionData" text NOT NULL,
    icon text
);


--
-- Name: EmbeddingModel; Type: TABLE; Schema: public; Owner: -
--

CREATE TABLE public."EmbeddingModel" (
    id text NOT NULL,
    "modelId" text NOT NULL,
    dimensions integer,
    "maxContext" integer
);


--
-- Name: FileIndex; Type: TABLE; Schema: public; Owner: -
--

CREATE TABLE public."FileIndex" (
    "filePath" text NOT NULL,
    "contentHash" text NOT NULL,
    "updatedAt" timestamp(3) without time zone NOT NULL
);


--
-- Name: ImageModel; Type: TABLE; Schema: public; Owner: -
--

CREATE TABLE public."ImageModel" (
    id text NOT NULL,
    "modelId" text NOT NULL,
    resolutions jsonb DEFAULT '[]'::jsonb,
    styles jsonb DEFAULT '[]'::jsonb
);


--
-- Name: Job; Type: TABLE; Schema: public; Owner: -
--

CREATE TABLE public."Job" (
    id text NOT NULL,
    type text NOT NULL,
    status text DEFAULT 'pending'::text NOT NULL,
    priority text DEFAULT 'medium'::text NOT NULL,
    description text,
    "roleId" text,
    input jsonb DEFAULT '{}'::jsonb,
    output jsonb,
    logs jsonb DEFAULT '[]'::jsonb,
    "createdAt" timestamp(3) without time zone DEFAULT CURRENT_TIMESTAMP NOT NULL,
    "updatedAt" timestamp(3) without time zone NOT NULL,
    "startedAt" timestamp(3) without time zone,
    "completedAt" timestamp(3) without time zone
);


--
-- Name: KnowledgeVector; Type: TABLE; Schema: public; Owner: -
--

CREATE TABLE public."KnowledgeVector" (
    id text NOT NULL,
    "entityType" text NOT NULL,
    "entityId" text NOT NULL,
    content text NOT NULL,
    "modelId" text NOT NULL,
    vector double precision[],
    dimensions integer NOT NULL,
    "createdAt" timestamp(3) without time zone DEFAULT CURRENT_TIMESTAMP NOT NULL
);


--
-- Name: Model; Type: TABLE; Schema: public; Owner: -
--

CREATE TABLE public."Model" (
    id text NOT NULL,
    "providerId" text NOT NULL,
    name text NOT NULL,
    "providerData" jsonb DEFAULT '{}'::jsonb NOT NULL,
    "aiData" jsonb DEFAULT '{}'::jsonb NOT NULL,
    "isActive" boolean DEFAULT true NOT NULL,
    "costPer1k" double precision,
    "firstSeenAt" timestamp(3) without time zone DEFAULT CURRENT_TIMESTAMP NOT NULL,
    "lastSeenAt" timestamp(3) without time zone DEFAULT CURRENT_TIMESTAMP NOT NULL,
    "updatedAt" timestamp(3) without time zone NOT NULL
);


--
-- Name: ModelFailure; Type: TABLE; Schema: public; Owner: -
--

CREATE TABLE public."ModelFailure" (
    id text NOT NULL,
    "providerId" text NOT NULL,
    "modelId" text NOT NULL,
    failures integer DEFAULT 0 NOT NULL,
    "lastFailedAt" timestamp(3) without time zone NOT NULL
);


--
-- Name: ModelUsage; Type: TABLE; Schema: public; Owner: -
--

CREATE TABLE public."ModelUsage" (
    id text NOT NULL,
    "createdAt" timestamp(3) without time zone DEFAULT CURRENT_TIMESTAMP NOT NULL,
    "modelId" text NOT NULL,
    "roleId" text NOT NULL,
    "promptTokens" integer,
    "completionTokens" integer,
    cost double precision DEFAULT 0 NOT NULL,
    "usageMetrics" jsonb DEFAULT '{}'::jsonb,
    "responseHeaders" jsonb,
    metadata jsonb
);


--
-- Name: PromptRefinement; Type: TABLE; Schema: public; Owner: -
--

CREATE TABLE public."PromptRefinement" (
    id text NOT NULL,
    "roleId" text NOT NULL,
    "originalPrompt" text NOT NULL,
    critique text NOT NULL,
    "refinedPrompt" text,
    "createdAt" timestamp(3) without time zone DEFAULT CURRENT_TIMESTAMP NOT NULL
);


--
-- Name: ProviderConfig; Type: TABLE; Schema: public; Owner: -
--

CREATE TABLE public."ProviderConfig" (
    id text NOT NULL,
    label text NOT NULL,
    type text NOT NULL,
    "baseURL" text,
    "isEnabled" boolean DEFAULT true NOT NULL,
    "createdAt" timestamp(3) without time zone DEFAULT CURRENT_TIMESTAMP NOT NULL,
    "updatedAt" timestamp(3) without time zone NOT NULL
);


--
-- Name: RewardModel; Type: TABLE; Schema: public; Owner: -
--

CREATE TABLE public."RewardModel" (
    id text NOT NULL,
    "modelId" text NOT NULL,
    "scoreType" text
);


--
-- Name: Role; Type: TABLE; Schema: public; Owner: -
--

CREATE TABLE public."Role" (
    id text NOT NULL,
    name text NOT NULL,
    description text,
    "basePrompt" text NOT NULL,
    "categoryId" text,
    "createdAt" timestamp(3) without time zone DEFAULT CURRENT_TIMESTAMP NOT NULL,
    "updatedAt" timestamp(3) without time zone NOT NULL,
    metadata jsonb DEFAULT '{}'::jsonb
);


--
-- Name: RoleAssessment; Type: TABLE; Schema: public; Owner: -
--

CREATE TABLE public."RoleAssessment" (
    id text NOT NULL,
    "variantId" text NOT NULL,
    "taskId" text NOT NULL,
    metric text NOT NULL,
    score double precision NOT NULL,
    feedback text,
    "createdAt" timestamp(3) without time zone DEFAULT CURRENT_TIMESTAMP NOT NULL
);


--
-- Name: RoleCategory; Type: TABLE; Schema: public; Owner: -
--

CREATE TABLE public."RoleCategory" (
    id text NOT NULL,
    name text NOT NULL,
    "order" integer DEFAULT 0 NOT NULL
);


--
-- Name: RoleTool; Type: TABLE; Schema: public; Owner: -
--

CREATE TABLE public."RoleTool" (
    "roleId" text NOT NULL,
    "toolId" text NOT NULL
);


--
-- Name: RoleVariant; Type: TABLE; Schema: public; Owner: -
--

CREATE TABLE public."RoleVariant" (
    id text NOT NULL,
    "roleId" text NOT NULL,
    "identityConfig" jsonb DEFAULT '{}'::jsonb NOT NULL,
    "cortexConfig" jsonb DEFAULT '{}'::jsonb NOT NULL,
    "contextConfig" jsonb DEFAULT '{}'::jsonb NOT NULL,
    "governanceConfig" jsonb DEFAULT '{}'::jsonb NOT NULL,
    "isActive" boolean DEFAULT true NOT NULL,
    "createdAt" timestamp(3) without time zone DEFAULT CURRENT_TIMESTAMP NOT NULL,
    "behaviorConfig" jsonb DEFAULT '{}'::jsonb NOT NULL
);


--
-- Name: Tool; Type: TABLE; Schema: public; Owner: -
--

CREATE TABLE public."Tool" (
    id text NOT NULL,
    name text NOT NULL,
    description text NOT NULL,
    instruction text NOT NULL,
    schema text NOT NULL,
    "isEnabled" boolean DEFAULT true NOT NULL,
    "serverId" text
);


--
-- Name: UnknownModel; Type: TABLE; Schema: public; Owner: -
--

CREATE TABLE public."UnknownModel" (
    id text NOT NULL,
    "modelId" text NOT NULL,
    reason text DEFAULT 'uncategorized'::text
);


--
-- Name: VisionModel; Type: TABLE; Schema: public; Owner: -
--

CREATE TABLE public."VisionModel" (
    id text NOT NULL,
    "modelId" text NOT NULL,
    "maxResolution" text,
    "supportsVideo" boolean DEFAULT false NOT NULL
);


--
-- Name: WorkOrderCard; Type: TABLE; Schema: public; Owner: -
--

CREATE TABLE public."WorkOrderCard" (
    id text NOT NULL,
    title text NOT NULL,
    description text,
    "workspaceId" text NOT NULL,
    "contextStats" jsonb,
    "createdAt" timestamp(3) without time zone DEFAULT CURRENT_TIMESTAMP NOT NULL,
    "updatedAt" timestamp(3) without time zone NOT NULL
);


--
-- Name: Workspace; Type: TABLE; Schema: public; Owner: -
--

CREATE TABLE public."Workspace" (
    id text NOT NULL,
    name text NOT NULL,
    "rootPath" text NOT NULL,
    "systemPrompt" text,
    "codeRules" text,
    glossary jsonb DEFAULT '{}'::jsonb,
    "createdAt" timestamp(3) without time zone DEFAULT CURRENT_TIMESTAMP NOT NULL,
    "updatedAt" timestamp(3) without time zone NOT NULL
);


--
-- Name: _prisma_migrations; Type: TABLE; Schema: public; Owner: -
--

CREATE TABLE public._prisma_migrations (
    id character varying(36) NOT NULL,
    checksum character varying(64) NOT NULL,
    finished_at timestamp with time zone,
    migration_name character varying(255) NOT NULL,
    logs text,
    rolled_back_at timestamp with time zone,
    started_at timestamp with time zone DEFAULT now() NOT NULL,
    applied_steps_count integer DEFAULT 0 NOT NULL
);


--
-- Name: cerebras_models; Type: TABLE; Schema: public; Owner: -
--

CREATE TABLE public.cerebras_models (
    _id uuid DEFAULT gen_random_uuid() NOT NULL,
    _loaded_at timestamp with time zone DEFAULT now(),
    id text,
    object text,
    created text,
    owned_by text
);


--
-- Name: google_models_example; Type: TABLE; Schema: public; Owner: -
--

CREATE TABLE public.google_models_example (
    id text NOT NULL,
    "createdAt" timestamp(3) without time zone DEFAULT CURRENT_TIMESTAMP NOT NULL,
    "updatedAt" timestamp(3) without time zone DEFAULT CURRENT_TIMESTAMP NOT NULL,
    name text,
    "displayName" text,
    description text,
    "inputTokenLimit" integer,
    "outputTokenLimit" integer,
    "supportedGenerationMethods" jsonb,
    temperature integer,
    "topP" double precision,
    "topK" integer,
    "maxTemperature" integer
);


--
-- Name: groq_models; Type: TABLE; Schema: public; Owner: -
--

CREATE TABLE public.groq_models (
    _id uuid DEFAULT gen_random_uuid() NOT NULL,
    _loaded_at timestamp with time zone DEFAULT now(),
    id text,
    object text,
    created text,
    owned_by text,
    active text,
    context_window text,
    public_apps text,
    max_completion_tokens text
);


--
-- Name: mistral_models; Type: TABLE; Schema: public; Owner: -
--

CREATE TABLE public.mistral_models (
    _id uuid DEFAULT gen_random_uuid() NOT NULL,
    _loaded_at timestamp with time zone DEFAULT now(),
    id text,
    object text,
    created text,
    owned_by text,
    capabilities text,
    name text,
    description text,
    max_context_length text,
    aliases text,
    deprecation text,
    deprecation_replacement_model text,
    default_model_temperature text,
    type text
);


--
-- Name: model_registry; Type: TABLE; Schema: public; Owner: -
--

CREATE TABLE public.model_registry (
    id text NOT NULL,
    "createdAt" timestamp(3) without time zone DEFAULT CURRENT_TIMESTAMP NOT NULL,
    "updatedAt" timestamp(3) without time zone DEFAULT CURRENT_TIMESTAMP NOT NULL,
    provider_id text,
    model_id text,
    model_name text,
    provider_data jsonb,
    ai_data jsonb,
    specs jsonb,
    is_free boolean,
    cost_per_1k text,
    updated_at text,
    capability_tags jsonb,
    first_seen_at text,
    is_active boolean,
    last_seen_at text,
    source text
);


--
-- Name: modelcapabilities; Type: TABLE; Schema: public; Owner: -
--

CREATE TABLE public.modelcapabilities (
    id text NOT NULL,
    "createdAt" timestamp(3) without time zone DEFAULT CURRENT_TIMESTAMP NOT NULL,
    "updatedAt" timestamp(3) without time zone DEFAULT CURRENT_TIMESTAMP NOT NULL,
    "modelId" text,
    "modelName" text,
    "contextWindow" integer,
    "maxOutput" integer,
    "hasVision" boolean,
    "hasAudioInput" boolean,
    "hasAudioOutput" boolean,
    "isMultimodal" boolean,
    "supportsFunctionCalling" boolean,
    "supportsJsonMode" boolean,
    tokenizer text,
    "paramCount" text,
    "requestsPerMinute" text,
    "tokensPerMinute" text,
    "hasImageGen" boolean,
    "hasTTS" boolean,
    "hasReasoning" boolean,
    "hasEmbedding" boolean,
    "hasOCR" boolean,
    "hasReward" boolean,
    "hasModeration" boolean,
    confidence text,
    source text,
    specs jsonb DEFAULT '{}'::jsonb,
    "primaryTask" text,
    "isLocal" boolean DEFAULT false
);


--
-- Name: nvidia_models; Type: TABLE; Schema: public; Owner: -
--

CREATE TABLE public.nvidia_models (
    _id uuid DEFAULT gen_random_uuid() NOT NULL,
    _loaded_at timestamp with time zone DEFAULT now(),
    id text,
    object text,
    created text,
    owned_by text
);


--
-- Name: ollama_models; Type: TABLE; Schema: public; Owner: -
--

CREATE TABLE public.ollama_models (
    _id uuid DEFAULT gen_random_uuid() NOT NULL,
    _loaded_at timestamp with time zone DEFAULT now(),
    name text,
    model text,
    modified_at text,
    size text,
    digest text,
    details text
);


--
-- Name: openrouter_models; Type: TABLE; Schema: public; Owner: -
--

CREATE TABLE public.openrouter_models (
    _id uuid DEFAULT gen_random_uuid() NOT NULL,
    _loaded_at timestamp with time zone DEFAULT now(),
    id text,
    canonical_slug text,
    hugging_face_id text,
    name text,
    created text,
    description text,
    context_length text,
    architecture text,
    pricing text,
    top_provider text,
    per_request_limits text,
    supported_parameters text,
    default_parameters text,
    expiration_date text
);


--
-- Name: __drizzle_migrations id; Type: DEFAULT; Schema: drizzle; Owner: -
--

ALTER TABLE ONLY drizzle.__drizzle_migrations ALTER COLUMN id SET DEFAULT nextval('drizzle.__drizzle_migrations_id_seq'::regclass);


--
-- Data for Name: __drizzle_migrations; Type: TABLE DATA; Schema: drizzle; Owner: -
--

COPY drizzle.__drizzle_migrations (id, hash, created_at) FROM stdin;
1	214a685433c895e6a8644fc68e901d0af8176fb3d470c4a754198df63e8cd49e	1765015014999
\.


--
-- Data for Name: AudioModel; Type: TABLE DATA; Schema: public; Owner: -
--

COPY public."AudioModel" (id, "modelId", voices, "sampleRates") FROM stdin;
cmkgscncl001vjo6k6qwm3qqq	mistral:voxtral-mini-2507	[]	[]
cmkgscnd6001zjo6k6d752rw1	mistral:voxtral-mini-latest	[]	[]
cmkgscndp0023jo6khqoeknci	mistral:voxtral-small-2507	[]	[]
cmkgscned0028jo6k178vwd60	mistral:voxtral-small-latest	[]	[]
cmkgscnrj004ijo6k6mazx5os	mistral:voxtral-mini-transcribe-2507	[]	[]
cmkgscokb007vjo6k9r5ejh22	groq:canopylabs/orpheus-v1-english	[]	[]
cmkgscokt007xjo6kj8vff95e	groq:whisper-large-v3-turbo	[]	[]
cmkgscol8007zjo6kud4jssr5	groq:whisper-large-v3	[]	[]
cmkgscoq3008ljo6kad1s4jde	groq:canopylabs/orpheus-arabic-saudi	[]	[]
\.


--
-- Data for Name: CardConfig; Type: TABLE DATA; Schema: public; Owner: -
--

COPY public."CardConfig" ("cardId") FROM stdin;
\.


--
-- Data for Name: ChatModel; Type: TABLE DATA; Schema: public; Owner: -
--

COPY public."ChatModel" (id, "modelId", "contextWindow", "supportsTools", "supportsJson") FROM stdin;
cmkgs8zw600438wmodlo2hm8s	openrouter:xiaomi/mimo-v2-flash:free	4096	t	t
cmkgs8ypz00218wmoivjonsjd	mistral:mistral-large-2512	262144	t	t
cmkgs8yel00098wmorp9xb7ta	mistral:open-mistral-nemo	131072	t	t
cmkgs8yez000b8wmotju731v4	mistral:open-mistral-nemo-2407	131072	t	t
cmkgs8yfd000d8wmodk0wc1xv	mistral:mistral-tiny-2407	131072	t	t
cmkgs8yfq000f8wmo0w1rwzg0	mistral:mistral-tiny-latest	131072	t	t
cmkgs8yg3000h8wmovepfg4fc	mistral:mistral-large-2411	131072	t	t
cmkgs8ygh000j8wmoxauj73lw	mistral:pixtral-large-2411	131072	t	t
cmkgs8yh2000n8wmohpevgswt	mistral:pixtral-large-latest	131072	t	t
cmkgs8yhk000r8wmoc6ggh76w	mistral:mistral-large-pixtral-2411	131072	t	t
cmkgs8yi2000v8wmomn84ggp2	mistral:codestral-2508	256000	t	t
cmkgs8yif000x8wmompwhquhi	mistral:codestral-latest	256000	t	t
cmkgs8yit000z8wmowtdddl8f	mistral:devstral-small-2507	131072	t	t
cmkgs8yj900118wmo8p3rvjjz	mistral:devstral-medium-2507	131072	t	t
cmkgs8yjm00138wmog3uv8r93	mistral:devstral-2512	262144	t	t
cmkgs8yjz00158wmog2xn6sv1	mistral:mistral-vibe-cli-latest	262144	t	t
cmkgs8ykb00178wmouzvyjatc	mistral:devstral-medium-latest	262144	t	t
cmkgs8yko00198wmol181icy4	mistral:devstral-latest	262144	t	t
cmkgs8yl2001b8wmort8vemlw	mistral:labs-devstral-small-2512	262144	t	t
cmkgs8ylf001d8wmoohnv87tp	mistral:devstral-small-latest	262144	t	t
cmkgs8yls001f8wmoxbrr9p98	mistral:mistral-small-2506	131072	t	t
cmkgs8ym5001h8wmoyl2sa7cv	mistral:mistral-small-latest	131072	t	t
cmkgs8ymw001l8wmopllrk4h9	mistral:magistral-medium-2509	131072	t	t
cmkgs8yng001n8wmob9mod06l	mistral:magistral-medium-latest	131072	t	t
cmkgs8yns001p8wmosaipacn1	mistral:magistral-small-2509	131072	t	t
cmkgs8yo5001r8wmoxwiv776x	mistral:magistral-small-latest	131072	t	t
cmkgs8yow001v8wmojggujscw	mistral:voxtral-mini-latest	16384	t	t
cmkgs8ypa001x8wmovtn7a4g9	mistral:voxtral-small-2507	32768	t	t
cmkgs8ypm001z8wmoc0jiobdy	mistral:voxtral-small-latest	32768	t	t
cmkgs8yqb00238wmovfonv9sy	mistral:mistral-large-latest	262144	t	t
cmkgs8yqp00258wmoymmemobc	mistral:ministral-3b-2512	131072	t	t
cmkgs8yr100278wmo72jo20vs	mistral:ministral-3b-latest	131072	t	t
cmkgs8yre00298wmozrjpzgbm	mistral:ministral-8b-2512	262144	t	t
cmkgs8yrr002b8wmowo1ruzk8	mistral:ministral-8b-latest	262144	t	t
cmkgs8ys4002d8wmorreyi6m9	mistral:ministral-14b-2512	262144	t	t
cmkgs8ysh002f8wmoalj8oczv	mistral:ministral-14b-latest	262144	t	t
cmkgs8ysv002h8wmofr2wvcbb	mistral:open-mistral-7b	32768	t	t
cmkgs8yt8002j8wmoh1a0msms	mistral:mistral-tiny	32768	t	t
cmkgs8ytl002l8wmo9oma56go	mistral:mistral-tiny-2312	32768	t	t
cmkgs8yty002n8wmocz9xx5mo	mistral:pixtral-12b-2409	131072	t	t
cmkgs8yuh002r8wmoaocf178h	mistral:pixtral-12b	131072	t	t
cmkgs8yuz002v8wmosnm14kmt	mistral:pixtral-12b-latest	131072	t	t
cmkgs8yvi002z8wmodugo7i5x	mistral:ministral-3b-2410	131072	t	t
cmkgs8yvv00318wmogsua6mu1	mistral:ministral-8b-2410	131072	t	t
cmkgs8yw800338wmouxvsmq5f	mistral:codestral-2501	256000	t	t
cmkgs8yxc00398wmo711vtpaa	mistral:mistral-small-2501	32768	t	t
cmkgs8yzl003n8wmofyvcewzh	mistral:mistral-ocr-2512	16384	t	t
cmkgs8yzy003p8wmoq3g1fo5k	mistral:mistral-ocr-latest	16384	t	t
cmkgs8z0b003r8wmoyaxe2e30	mistral:mistral-ocr-2505	16384	t	t
cmkgs8z0o003t8wmoupv9wogb	mistral:mistral-ocr-2503	16384	t	t
cmkgs8z10003v8wmou5vhhqbx	mistral:voxtral-mini-transcribe-2507	16384	t	t
cmkgs8yoj001t8wmo2xgbjdhf	mistral:voxtral-mini-2507	16384	t	t
cmkgs8zvs00418wmo3ei6qprr	openrouter:allenai/molmo-2-8b:free	4096	t	t
cmkgs8zwj00458wmo025sxv2a	openrouter:nvidia/nemotron-3-nano-30b-a3b:free	4096	t	t
cmkgs8zwx00478wmokkhtzrws	openrouter:mistralai/devstral-2512:free	4096	t	t
cmkgs8zxa00498wmo5kgltnw7	openrouter:arcee-ai/trinity-mini:free	4096	t	t
cmkgs8zxo004b8wmohdyfy9vq	openrouter:tngtech/tng-r1t-chimera:free	4096	t	t
cmkgs8zy2004d8wmoedwzsn6r	openrouter:nvidia/nemotron-nano-12b-v2-vl:free	4096	t	t
cmkgs8zym004h8wmorsn1la71	openrouter:qwen/qwen3-next-80b-a3b-instruct:free	4096	t	t
cmkgs8zyy004j8wmo6p5tdh05	openrouter:nvidia/nemotron-nano-9b-v2:free	4096	t	t
cmkgs8zzc004l8wmomy5t2gbp	openrouter:openai/gpt-oss-120b:free	4096	t	t
cmkgs8zzp004n8wmo1drs06hw	openrouter:openai/gpt-oss-20b:free	4096	t	t
cmkgs9003004p8wmodtpxne85	openrouter:z-ai/glm-4.5-air:free	4096	t	t
cmkgs900g004r8wmoyiumr6re	openrouter:qwen/qwen3-coder:free	4096	t	t
cmkgs900t004t8wmojc5s5ddy	openrouter:moonshotai/kimi-k2:free	4096	t	t
cmkgs901k004x8wmo3t2ti14y	openrouter:google/gemma-3n-e2b-it:free	4096	t	t
cmkgs901x004z8wmovjxdpdee	openrouter:tngtech/deepseek-r1t2-chimera:free	4096	t	t
cmkgs902d00518wmoihoecn38	openrouter:deepseek/deepseek-r1-0528:free	4096	t	t
cmkgs902r00538wmo11up5206	openrouter:google/gemma-3n-e4b-it:free	4096	t	t
cmkgs903300558wmolmr3l6x2	openrouter:qwen/qwen3-4b:free	4096	t	t
cmkgs903g00578wmomtt97xuc	openrouter:tngtech/deepseek-r1t-chimera:free	4096	t	t
cmkgs903t00598wmofb3t8z2m	openrouter:mistralai/mistral-small-3.1-24b-instruct:free	4096	t	t
cmkgs9046005b8wmovpty7a5o	openrouter:google/gemma-3-4b-it:free	4096	t	t
cmkgs904k005d8wmo1e4xcob9	openrouter:google/gemma-3-12b-it:free	4096	t	t
cmkgs904y005f8wmooa9zrx0d	openrouter:google/gemma-3-27b-it:free	4096	t	t
cmkgs905c005h8wmo5akexrzw	openrouter:google/gemini-2.0-flash-exp:free	4096	t	t
cmkgs8ydu00058wmoaeikv3bw	mistral:mistral-medium-latest	131072	t	t
cmkgs8ye700078wmobid296r6	mistral:mistral-medium	131072	t	t
cmkgs8ywl00358wmo6mkj2mhy	mistral:codestral-2412	256000	t	t
cmkgs90ln006f8wmoyq1bvdk7	groq:canopylabs/orpheus-arabic-saudi	4000	t	t
cmkgs90nx006t8wmoge6rdgdx	groq:canopylabs/orpheus-v1-english	4000	t	t
cmkgs90om006x8wmo8p3acn5j	groq:moonshotai/kimi-k2-instruct	131072	t	t
cmkgs90l5006b8wmon3vhy5z6	groq:meta-llama/llama-prompt-guard-2-86m	512	t	t
cmkgs90mi006l8wmoqsz6b8nv	groq:groq/compound	131072	t	t
cmkgs90ic005v8wmoqru9kgy5	groq:groq/compound-mini	131072	t	t
cmkgs90m0006h8wmofqkfq8s2	groq:meta-llama/llama-prompt-guard-2-22m	512	t	t
cmkgs90oz006z8wmo0mmsiebn	groq:allam-2-7b	4096	t	t
cmkgs90j0005z8wmodsc5sczj	groq:llama-3.1-8b-instant	131072	t	t
cmkgs90nk006r8wmo7fpvwpqv	groq:openai/gpt-oss-20b	131072	t	t
cmkgs90o9006v8wmogzioynoj	groq:qwen/qwen3-32b	131072	t	t
cmkgs90kn00678wmo25u8c1j9	groq:meta-llama/llama-guard-4-12b	131072	t	t
cmkgs90io005x8wmoowb7n5q1	groq:moonshotai/kimi-k2-instruct-0905	262144	t	t
cmkgs90pc00718wmolvhilpex	groq:openai/gpt-oss-120b	131072	t	t
cmkgs90pp00738wmovdazfcfb	groq:openai/gpt-oss-safeguard-20b	131072	t	t
cmkgs90jy00638wmoi6okrko5	groq:llama-3.3-70b-versatile	131072	t	t
cmkgs90n7006p8wmo3i2460yq	groq:meta-llama/llama-4-maverick-17b-128e-instruct	131072	t	t
cmkgs90mu006n8wmolh6jcmb3	groq:meta-llama/llama-4-scout-17b-16e-instruct	131072	t	t
cmkgs90jf00618wmo0nced4o3	groq:whisper-large-v3-turbo	448	t	t
cmkgs90kb00658wmobi2as4sf	groq:whisper-large-v3	448	t	t
cmkgs92xs00798wmocl3unsae	ollama-local:granite4:micro	4096	t	t
cmkgs92y5007b8wmoykbfb40z	ollama-local:hengwen/watt-tool-8B:latest	4096	t	t
cmkgs92zj007l8wmo8m83sucw	nvidia:mediatek/breeze-7b-instruct	4096	f	t
cmkgs930s007z8wmov9eir1jy	nvidia:meta/llama-3.2-11b-vision-instruct	4096	f	t
cmkgs931n00898wmoxyzr20a6	nvidia:meta/llama-3.2-90b-vision-instruct	4096	f	t
cmkgs932q008l8wmou6fah8pj	nvidia:mistralai/mathstral-7b-v0.1	4096	f	t
cmkgs9016004v8wmoijx9lt1q	openrouter:cognitivecomputations/dolphin-mistral-24b-venice-edition:free	4096	t	t
cmkgs905p005j8wmowvv9opap	openrouter:meta-llama/llama-3.3-70b-instruct:free	4096	t	t
cmkgs9063005l8wmohsxkvh7u	openrouter:meta-llama/llama-3.2-3b-instruct:free	4096	t	t
cmkgs906g005n8wmos1e0hbrv	openrouter:qwen/qwen-2.5-vl-7b-instruct:free	4096	t	t
cmkgs906z005r8wmonkkoekiw	openrouter:nousresearch/hermes-3-llama-3.1-405b:free	4096	t	t
cmkgs907c005t8wmo48ieu4l8	openrouter:meta-llama/llama-3.1-405b-instruct:free	4096	t	t
cmkgs8yd100018wmo7rfkan40	mistral:mistral-medium-2505	131072	t	t
cmkgs8ydg00038wmo1e61f14j	mistral:mistral-medium-2508	131072	t	t
cmkgs8ymj001j8wmo88j9281w	mistral:labs-mistral-small-creative	32768	t	t
cmkgs8ywz00378wmo5j9akxie	mistral:codestral-2411-rc5	256000	t	t
cmkgs9332008p8wmoluhpi0yr	nvidia:mistralai/mistral-medium-3-instruct	4096	f	t
\.


--
-- Data for Name: ComplianceModel; Type: TABLE DATA; Schema: public; Owner: -
--

COPY public."ComplianceModel" (id, "modelId", categories) FROM stdin;
cmkgs8yyt003j8wmoxgcusjpe	mistral:mistral-moderation-2411	[]
cmkgs8yz8003l8wmo5hps009n	mistral:mistral-moderation-latest	[]
cmkgs90ks00698wmog776ujoq	groq:meta-llama/llama-guard-4-12b	[]
cmkgs90la006d8wmojvyyx8gf	groq:meta-llama/llama-prompt-guard-2-86m	[]
cmkgs90m5006j8wmop2bkl2c0	groq:meta-llama/llama-prompt-guard-2-22m	[]
cmkgs90pu00758wmor3lokvec	groq:openai/gpt-oss-safeguard-20b	[]
\.


--
-- Data for Name: ComponentRole; Type: TABLE DATA; Schema: public; Owner: -
--

COPY public."ComponentRole" (id, "cardId", component, "roleId") FROM stdin;
\.


--
-- Data for Name: CustomButton; Type: TABLE DATA; Schema: public; Owner: -
--

COPY public."CustomButton" (id, "cardId", label, action, "actionData", icon) FROM stdin;
\.


--
-- Data for Name: EmbeddingModel; Type: TABLE DATA; Schema: public; Owner: -
--

COPY public."EmbeddingModel" (id, "modelId", dimensions, "maxContext") FROM stdin;
cmkgsbtds003bhzl21ly5ftnu	mistral:mistral-embed-2312	1536	\N
cmkgsbte7003dhzl2wijz3pm5	mistral:mistral-embed	1536	\N
cmkgsbtej003fhzl2jsx3uf89	mistral:codestral-embed	1536	\N
cmkgsbtew003hhzl2qgvcuqwu	mistral:codestral-embed-2505	1536	\N
cmkgsbw080077hzl28pcfpl2n	ollama-local:mxbai-embed-large:latest	1536	\N
\.


--
-- Data for Name: FileIndex; Type: TABLE DATA; Schema: public; Owner: -
--

COPY public."FileIndex" ("filePath", "contentHash", "updatedAt") FROM stdin;
\.


--
-- Data for Name: ImageModel; Type: TABLE DATA; Schema: public; Owner: -
--

COPY public."ImageModel" (id, "modelId", resolutions, styles) FROM stdin;
\.


--
-- Data for Name: Job; Type: TABLE DATA; Schema: public; Owner: -
--

COPY public."Job" (id, type, status, priority, description, "roleId", input, output, logs, "createdAt", "updatedAt", "startedAt", "completedAt") FROM stdin;
\.


--
-- Data for Name: KnowledgeVector; Type: TABLE DATA; Schema: public; Owner: -
--

COPY public."KnowledgeVector" (id, "entityType", "entityId", content, "modelId", vector, dimensions, "createdAt") FROM stdin;
\.


--
-- Data for Name: Model; Type: TABLE DATA; Schema: public; Owner: -
--

COPY public."Model" (id, "providerId", name, "providerData", "aiData", "isActive", "costPer1k", "firstSeenAt", "lastSeenAt", "updatedAt") FROM stdin;
groq:canopylabs/orpheus-arabic-saudi	groq	canopylabs/orpheus-arabic-saudi	{"id": "canopylabs/orpheus-arabic-saudi", "active": true, "object": "model", "created": 1765926439, "owned_by": "Canopy Labs", "public_apps": null, "context_window": 4000, "max_completion_tokens": 50000}	{}	t	0	2026-01-16 11:15:15.2	2026-01-16 11:18:59.872	2026-01-16 11:25:16.912
mistral:voxtral-mini-transcribe-2507	mistral	voxtral-mini-transcribe-2507	{"id": "voxtral-mini-transcribe-2507", "name": "voxtral-mini-transcribe-2507", "type": "base", "object": "model", "aliases": ["voxtral-mini-2507", "voxtral-mini-latest"], "created": 1768562337, "owned_by": "mistralai", "deprecation": null, "description": "A mini transcription model released in July 2025", "capabilities": {"ocr": false, "audio": false, "vision": false, "moderation": false, "fine_tuning": false, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": false}, "max_context_length": 16384, "default_model_temperature": 0, "deprecation_replacement_model": null}	{}	t	0	2026-01-16 11:15:13.975	2026-01-16 11:18:58.757	2026-01-16 11:25:16.912
groq:whisper-large-v3-turbo	groq	whisper-large-v3-turbo	{"id": "whisper-large-v3-turbo", "active": true, "object": "model", "created": 1728413088, "owned_by": "OpenAI", "public_apps": null, "context_window": 448, "max_completion_tokens": 448}	{}	t	0	2026-01-16 11:15:15.122	2026-01-16 11:19:00.03	2026-01-16 11:25:16.912
nvidia:deepseek-ai/deepseek-r1	nvidia	deepseek-ai/deepseek-r1	{"id": "deepseek-ai/deepseek-r1", "object": "model", "created": 735790403, "owned_by": "deepseek-ai"}	{}	t	0	2026-01-16 11:15:15.65	2026-01-16 11:23:53.2	2026-01-16 11:23:53.2
mistral:open-mistral-nemo-2407	mistral	open-mistral-nemo-2407	{"id": "open-mistral-nemo-2407", "name": "open-mistral-nemo", "type": "base", "object": "model", "aliases": ["open-mistral-nemo", "mistral-tiny-2407", "mistral-tiny-latest"], "created": 1768562337, "owned_by": "mistralai", "deprecation": null, "description": "Our best multilingual open source model released July 2024.", "capabilities": {"ocr": false, "audio": false, "vision": false, "moderation": false, "fine_tuning": true, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": true}, "max_context_length": 131072, "default_model_temperature": 0.3, "deprecation_replacement_model": null}	{}	f	0	2026-01-16 11:15:13.258	2026-01-16 11:18:57.89	2026-01-16 11:23:55.425
mistral:devstral-small-2507	mistral	devstral-small-2507	{"id": "devstral-small-2507", "name": "devstral-small-2507", "type": "base", "object": "model", "aliases": [], "created": 1768562337, "owned_by": "mistralai", "deprecation": null, "description": "Our small open-source code-agentic model.", "capabilities": {"ocr": false, "audio": false, "vision": false, "moderation": false, "fine_tuning": false, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": true}, "max_context_length": 131072, "default_model_temperature": 0, "deprecation_replacement_model": null}	{}	f	0	2026-01-16 11:15:13.381	2026-01-16 11:18:58.023	2026-01-16 11:23:55.425
nvidia:deepseek-ai/deepseek-coder-6.7b-instruct	nvidia	deepseek-ai/deepseek-coder-6.7b-instruct	{"id": "deepseek-ai/deepseek-coder-6.7b-instruct", "object": "model", "created": 735790403, "owned_by": "deepseek-ai"}	{}	t	0	2026-01-16 11:15:15.638	2026-01-16 11:23:53.193	2026-01-16 11:23:53.194
mistral:voxtral-mini-latest	mistral	voxtral-mini-latest	{"id": "voxtral-mini-latest", "name": "voxtral-mini-transcribe-2507", "type": "base", "object": "model", "aliases": ["voxtral-mini-transcribe-2507", "voxtral-mini-2507"], "created": 1768562337, "owned_by": "mistralai", "deprecation": null, "description": "A mini transcription model released in July 2025", "capabilities": {"ocr": false, "audio": false, "vision": false, "moderation": false, "fine_tuning": false, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": false}, "max_context_length": 16384, "default_model_temperature": 0, "deprecation_replacement_model": null}	{}	t	0	2026-01-16 11:15:13.569	2026-01-16 11:18:58.79	2026-01-16 11:25:16.912
nvidia:nvidia/embed-qa-4	nvidia	nvidia/embed-qa-4	{"id": "nvidia/embed-qa-4", "object": "model", "created": 735790403, "owned_by": "nvidia"}	{}	t	0	2026-01-16 11:15:16.836	2026-01-16 11:23:53.801	2026-01-16 11:23:53.802
mistral:ministral-3b-latest	mistral	ministral-3b-latest	{"id": "ministral-3b-latest", "name": "ministral-3b-2512", "type": "base", "object": "model", "aliases": ["ministral-3b-2512"], "created": 1768562337, "owned_by": "mistralai", "deprecation": null, "description": "Ministral 3 (a.k.a. Tinystral) 3B Instruct.", "capabilities": {"ocr": false, "audio": false, "vision": true, "moderation": false, "fine_tuning": true, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": true}, "max_context_length": 131072, "default_model_temperature": 0.3, "deprecation_replacement_model": null}	{}	f	0	2026-01-16 11:15:13.639	2026-01-16 11:18:58.337	2026-01-16 11:23:55.425
mistral:ministral-3b-2410	mistral	ministral-3b-2410	{"id": "ministral-3b-2410", "name": "ministral-3b-2410", "type": "base", "object": "model", "aliases": [], "created": 1768562337, "owned_by": "mistralai", "deprecation": "2026-01-31T12:00:00Z", "description": "World's best edge model.", "capabilities": {"ocr": false, "audio": false, "vision": false, "moderation": false, "fine_tuning": true, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": true}, "max_context_length": 131072, "default_model_temperature": 0.3, "deprecation_replacement_model": "ministral-3b-latest"}	{}	f	0	2026-01-16 11:15:13.784	2026-01-16 11:18:58.527	2026-01-16 11:23:55.425
mistral:mistral-moderation-2411	mistral	mistral-moderation-2411	{"id": "mistral-moderation-2411", "name": "mistral-moderation-2411", "type": "base", "object": "model", "aliases": ["mistral-moderation-latest"], "created": 1768562337, "owned_by": "mistralai", "deprecation": null, "description": "Official mistral-moderation-2411 Mistral AI model", "capabilities": {"ocr": false, "audio": false, "vision": false, "moderation": true, "fine_tuning": false, "classification": true, "completion_fim": false, "completion_chat": false, "function_calling": false}, "max_context_length": 8192, "default_model_temperature": null, "deprecation_replacement_model": null}	{}	f	0	2026-01-16 11:15:13.902	2026-01-16 11:18:58.674	2026-01-16 11:23:55.425
nvidia:01-ai/yi-large	nvidia	01-ai/yi-large	{"id": "01-ai/yi-large", "object": "model", "created": 735790403, "owned_by": "01-ai"}	{}	t	0	2026-01-16 11:15:15.481	2026-01-16 11:23:53.117	2026-01-16 11:23:53.119
openrouter:mistralai/devstral-2512:free	openrouter	mistralai/devstral-2512:free	{"id": "mistralai/devstral-2512:free", "name": "Mistral: Devstral 2 2512 (free)", "created": 1765285419, "pricing": {"prompt": "0", "completion": "0"}, "description": "Devstral 2 is a state-of-the-art open-source model by Mistral AI specializing in agentic coding. It is a 123B-parameter dense transformer model supporting a 256K context window.\\n\\nDevstral 2 supports exploring codebases and orchestrating changes across multiple files while maintaining architecture-level context. It tracks framework dependencies, detects failures, and retries with corrections—solving challenges like bug fixing and modernizing legacy systems. The model can be fine-tuned to prioritize specific languages or optimize for large enterprise codebases. It is available under a modified MIT license.", "architecture": {"modality": "text->text", "tokenizer": "Mistral", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}, "top_provider": {"is_moderated": false, "context_length": 262144, "max_completion_tokens": null}, "canonical_slug": "mistralai/devstral-2512", "context_length": 262144, "expiration_date": null, "hugging_face_id": "mistralai/Devstral-2-123B-Instruct-2512", "default_parameters": {"top_p": null, "temperature": 0.3, "frequency_penalty": null}, "per_request_limits": null, "supported_parameters": ["frequency_penalty", "max_tokens", "presence_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_p"]}	{}	f	0	2026-01-16 11:15:14.366	2026-01-16 11:18:55.963	2026-01-16 11:19:02.539
nvidia:abacusai/dracarys-llama-3.1-70b-instruct	nvidia	abacusai/dracarys-llama-3.1-70b-instruct	{"id": "abacusai/dracarys-llama-3.1-70b-instruct", "object": "model", "created": 735790403, "owned_by": "abacusai"}	{}	t	0	2026-01-16 11:15:15.494	2026-01-16 11:23:53.125	2026-01-16 11:23:53.126
nvidia:adept/fuyu-8b	nvidia	adept/fuyu-8b	{"id": "adept/fuyu-8b", "object": "model", "created": 735790403, "owned_by": "adept"}	{}	t	0	2026-01-16 11:15:15.507	2026-01-16 11:23:53.132	2026-01-16 11:23:53.133
nvidia:thudm/chatglm3-6b	nvidia	thudm/chatglm3-6b	{"id": "thudm/chatglm3-6b", "object": "model", "created": 735790403, "owned_by": "thudm"}	{}	t	0	2026-01-16 11:15:17.689	2026-01-16 11:23:54.222	2026-01-16 11:23:54.223
nvidia:ai21labs/jamba-1.5-large-instruct	nvidia	ai21labs/jamba-1.5-large-instruct	{"id": "ai21labs/jamba-1.5-large-instruct", "object": "model", "created": 735790403, "owned_by": "ai21labs"}	{}	t	0	2026-01-16 11:15:15.518	2026-01-16 11:23:53.138	2026-01-16 11:23:53.139
nvidia:ai21labs/jamba-1.5-mini-instruct	nvidia	ai21labs/jamba-1.5-mini-instruct	{"id": "ai21labs/jamba-1.5-mini-instruct", "object": "model", "created": 735790403, "owned_by": "ai21labs"}	{}	t	0	2026-01-16 11:15:15.536	2026-01-16 11:23:53.145	2026-01-16 11:23:53.146
nvidia:aisingapore/sea-lion-7b-instruct	nvidia	aisingapore/sea-lion-7b-instruct	{"id": "aisingapore/sea-lion-7b-instruct", "object": "model", "created": 735790403, "owned_by": "aisingapore"}	{}	t	0	2026-01-16 11:15:15.555	2026-01-16 11:23:53.151	2026-01-16 11:23:53.152
nvidia:baai/bge-m3	nvidia	baai/bge-m3	{"id": "baai/bge-m3", "object": "model", "created": 735790403, "owned_by": "baai"}	{}	t	0	2026-01-16 11:15:15.567	2026-01-16 11:23:53.157	2026-01-16 11:23:53.158
openrouter:nvidia/nemotron-nano-9b-v2:free	openrouter	nvidia/nemotron-nano-9b-v2:free	{"id": "nvidia/nemotron-nano-9b-v2:free", "name": "NVIDIA: Nemotron Nano 9B V2 (free)", "created": 1757106807, "pricing": {"image": "0", "prompt": "0", "request": "0", "completion": "0", "web_search": "0", "internal_reasoning": "0"}, "description": "NVIDIA-Nemotron-Nano-9B-v2 is a large language model (LLM) trained from scratch by NVIDIA, and designed as a unified model for both reasoning and non-reasoning tasks. It responds to user queries and tasks by first generating a reasoning trace and then concluding with a final response. \\n\\nThe model's reasoning capabilities can be controlled via a system prompt. If the user prefers the model to provide its final answer without intermediate reasoning traces, it can be configured to do so.", "architecture": {"modality": "text->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}, "top_provider": {"is_moderated": false, "context_length": 128000, "max_completion_tokens": null}, "canonical_slug": "nvidia/nemotron-nano-9b-v2", "context_length": 128000, "expiration_date": null, "hugging_face_id": "nvidia/NVIDIA-Nemotron-Nano-9B-v2", "default_parameters": {}, "per_request_limits": null, "supported_parameters": ["include_reasoning", "max_tokens", "reasoning", "response_format", "seed", "structured_outputs", "temperature", "tool_choice", "tools", "top_p"]}	{}	f	0	2026-01-16 11:15:14.433	2026-01-16 11:18:56.04	2026-01-16 11:19:02.539
openrouter:nvidia/nemotron-nano-12b-v2-vl:free	openrouter	nvidia/nemotron-nano-12b-v2-vl:free	{"id": "nvidia/nemotron-nano-12b-v2-vl:free", "name": "NVIDIA: Nemotron Nano 12B 2 VL (free)", "created": 1761675565, "pricing": {"image": "0", "prompt": "0", "request": "0", "completion": "0", "web_search": "0", "internal_reasoning": "0"}, "description": "NVIDIA Nemotron Nano 2 VL is a 12-billion-parameter open multimodal reasoning model designed for video understanding and document intelligence. It introduces a hybrid Transformer-Mamba architecture, combining transformer-level accuracy with Mamba’s memory-efficient sequence modeling for significantly higher throughput and lower latency.\\n\\nThe model supports inputs of text and multi-image documents, producing natural-language outputs. It is trained on high-quality NVIDIA-curated synthetic datasets optimized for optical-character recognition, chart reasoning, and multimodal comprehension.\\n\\nNemotron Nano 2 VL achieves leading results on OCRBench v2 and scores ≈ 74 average across MMMU, MathVista, AI2D, OCRBench, OCR-Reasoning, ChartQA, DocVQA, and Video-MME—surpassing prior open VL baselines. With Efficient Video Sampling (EVS), it handles long-form videos while reducing inference cost.\\n\\nOpen-weights, training data, and fine-tuning recipes are released under a permissive NVIDIA open license, with deployment supported across NeMo, NIM, and major inference runtimes.", "architecture": {"modality": "text+image->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["image", "text", "video"], "output_modalities": ["text"]}, "top_provider": {"is_moderated": false, "context_length": 128000, "max_completion_tokens": 128000}, "canonical_slug": "nvidia/nemotron-nano-12b-v2-vl", "context_length": 128000, "expiration_date": null, "hugging_face_id": "nvidia/NVIDIA-Nemotron-Nano-12B-v2-VL-BF16", "default_parameters": {"top_p": null, "temperature": null, "frequency_penalty": null}, "per_request_limits": null, "supported_parameters": ["include_reasoning", "max_tokens", "reasoning", "seed", "temperature", "tool_choice", "tools", "top_p"]}	{}	f	0	2026-01-16 11:15:14.404	2026-01-16 11:18:56.007	2026-01-16 11:19:02.539
openrouter:qwen/qwen3-next-80b-a3b-instruct:free	openrouter	qwen/qwen3-next-80b-a3b-instruct:free	{"id": "qwen/qwen3-next-80b-a3b-instruct:free", "name": "Qwen: Qwen3 Next 80B A3B Instruct (free)", "created": 1757612213, "pricing": {"image": "0", "prompt": "0", "request": "0", "completion": "0", "web_search": "0", "internal_reasoning": "0"}, "description": "Qwen3-Next-80B-A3B-Instruct is an instruction-tuned chat model in the Qwen3-Next series optimized for fast, stable responses without “thinking” traces. It targets complex tasks across reasoning, code generation, knowledge QA, and multilingual use, while remaining robust on alignment and formatting. Compared with prior Qwen3 instruct variants, it focuses on higher throughput and stability on ultra-long inputs and multi-turn dialogues, making it well-suited for RAG, tool use, and agentic workflows that require consistent final answers rather than visible chain-of-thought.\\n\\nThe model employs scaling-efficient training and decoding to improve parameter efficiency and inference speed, and has been validated on a broad set of public benchmarks where it reaches or approaches larger Qwen3 systems in several categories while outperforming earlier mid-sized baselines. It is best used as a general assistant, code helper, and long-context task solver in production settings where deterministic, instruction-following outputs are preferred.", "architecture": {"modality": "text->text", "tokenizer": "Qwen3", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}, "top_provider": {"is_moderated": false, "context_length": 262144, "max_completion_tokens": null}, "canonical_slug": "qwen/qwen3-next-80b-a3b-instruct-2509", "context_length": 262144, "expiration_date": null, "hugging_face_id": "Qwen/Qwen3-Next-80B-A3B-Instruct", "default_parameters": {}, "per_request_limits": null, "supported_parameters": ["frequency_penalty", "max_tokens", "presence_penalty", "response_format", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_k", "top_p"]}	{}	f	0	2026-01-16 11:15:14.421	2026-01-16 11:18:56.026	2026-01-16 11:19:02.539
openrouter:qwen/qwen3-coder:free	openrouter	qwen/qwen3-coder:free	{"id": "qwen/qwen3-coder:free", "name": "Qwen: Qwen3 Coder 480B A35B (free)", "created": 1753230546, "pricing": {"image": "0", "prompt": "0", "request": "0", "completion": "0", "web_search": "0", "internal_reasoning": "0"}, "description": "Qwen3-Coder-480B-A35B-Instruct is a Mixture-of-Experts (MoE) code generation model developed by the Qwen team. It is optimized for agentic coding tasks such as function calling, tool use, and long-context reasoning over repositories. The model features 480 billion total parameters, with 35 billion active per forward pass (8 out of 160 experts).\\n\\nPricing for the Alibaba endpoints varies by context length. Once a request is greater than 128k input tokens, the higher pricing is used.", "architecture": {"modality": "text->text", "tokenizer": "Qwen3", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}, "top_provider": {"is_moderated": false, "context_length": 262000, "max_completion_tokens": 262000}, "canonical_slug": "qwen/qwen3-coder-480b-a35b-07-25", "context_length": 262000, "expiration_date": null, "hugging_face_id": "Qwen/Qwen3-Coder-480B-A35B-Instruct", "default_parameters": {}, "per_request_limits": null, "supported_parameters": ["frequency_penalty", "max_tokens", "presence_penalty", "seed", "stop", "temperature", "tool_choice", "tools", "top_k", "top_p"]}	{}	f	0	2026-01-16 11:15:14.484	2026-01-16 11:18:56.094	2026-01-16 11:19:02.539
openrouter:deepseek/deepseek-r1-0528:free	openrouter	deepseek/deepseek-r1-0528:free	{"id": "deepseek/deepseek-r1-0528:free", "name": "DeepSeek: R1 0528 (free)", "created": 1748455170, "pricing": {"image": "0", "prompt": "0", "request": "0", "completion": "0", "web_search": "0", "internal_reasoning": "0"}, "description": "May 28th update to the [original DeepSeek R1](/deepseek/deepseek-r1) Performance on par with [OpenAI o1](/openai/o1), but open-sourced and with fully open reasoning tokens. It's 671B parameters in size, with 37B active in an inference pass.\\n\\nFully open-source model.", "architecture": {"modality": "text->text", "tokenizer": "DeepSeek", "instruct_type": "deepseek-r1", "input_modalities": ["text"], "output_modalities": ["text"]}, "top_provider": {"is_moderated": false, "context_length": 163840, "max_completion_tokens": null}, "canonical_slug": "deepseek/deepseek-r1-0528", "context_length": 163840, "expiration_date": null, "hugging_face_id": "deepseek-ai/DeepSeek-R1-0528", "default_parameters": {"top_p": null, "temperature": null, "frequency_penalty": null}, "per_request_limits": null, "supported_parameters": ["frequency_penalty", "include_reasoning", "max_tokens", "presence_penalty", "reasoning", "repetition_penalty", "temperature"]}	{}	f	0	2026-01-16 11:15:14.543	2026-01-16 11:18:56.162	2026-01-16 11:19:02.539
openrouter:google/gemma-3n-e4b-it:free	openrouter	google/gemma-3n-e4b-it:free	{"id": "google/gemma-3n-e4b-it:free", "name": "Google: Gemma 3n 4B (free)", "created": 1747776824, "pricing": {"image": "0", "prompt": "0", "request": "0", "completion": "0", "web_search": "0", "internal_reasoning": "0"}, "description": "Gemma 3n E4B-it is optimized for efficient execution on mobile and low-resource devices, such as phones, laptops, and tablets. It supports multimodal inputs—including text, visual data, and audio—enabling diverse tasks such as text generation, speech recognition, translation, and image analysis. Leveraging innovations like Per-Layer Embedding (PLE) caching and the MatFormer architecture, Gemma 3n dynamically manages memory usage and computational load by selectively activating model parameters, significantly reducing runtime resource requirements.\\n\\nThis model supports a wide linguistic range (trained in over 140 languages) and features a flexible 32K token context window. Gemma 3n can selectively load parameters, optimizing memory and computational efficiency based on the task or device capabilities, making it well-suited for privacy-focused, offline-capable applications and on-device AI solutions. [Read more in the blog post](https://developers.googleblog.com/en/introducing-gemma-3n/)", "architecture": {"modality": "text->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}, "top_provider": {"is_moderated": false, "context_length": 8192, "max_completion_tokens": 2048}, "canonical_slug": "google/gemma-3n-e4b-it", "context_length": 8192, "expiration_date": null, "hugging_face_id": "google/gemma-3n-E4B-it", "default_parameters": {}, "per_request_limits": null, "supported_parameters": ["frequency_penalty", "max_tokens", "presence_penalty", "response_format", "seed", "stop", "temperature", "top_p"]}	{}	f	0	2026-01-16 11:15:14.555	2026-01-16 11:18:56.178	2026-01-16 11:19:02.539
nvidia:baichuan-inc/baichuan2-13b-chat	nvidia	baichuan-inc/baichuan2-13b-chat	{"id": "baichuan-inc/baichuan2-13b-chat", "object": "model", "created": 735790403, "owned_by": "baichuan-inc"}	{}	t	0	2026-01-16 11:15:15.578	2026-01-16 11:23:53.163	2026-01-16 11:23:53.163
nvidia:bigcode/starcoder2-15b	nvidia	bigcode/starcoder2-15b	{"id": "bigcode/starcoder2-15b", "object": "model", "created": 735790403, "owned_by": "bigcode"}	{}	t	0	2026-01-16 11:15:15.59	2026-01-16 11:23:53.169	2026-01-16 11:23:53.169
nvidia:bigcode/starcoder2-7b	nvidia	bigcode/starcoder2-7b	{"id": "bigcode/starcoder2-7b", "object": "model", "created": 735790403, "owned_by": "bigcode"}	{}	t	0	2026-01-16 11:15:15.602	2026-01-16 11:23:53.175	2026-01-16 11:23:53.176
nvidia:bytedance/seed-oss-36b-instruct	nvidia	bytedance/seed-oss-36b-instruct	{"id": "bytedance/seed-oss-36b-instruct", "object": "model", "created": 735790403, "owned_by": "bytedance"}	{}	t	0	2026-01-16 11:15:15.614	2026-01-16 11:23:53.181	2026-01-16 11:23:53.181
nvidia:databricks/dbrx-instruct	nvidia	databricks/dbrx-instruct	{"id": "databricks/dbrx-instruct", "object": "model", "created": 735790403, "owned_by": "databricks"}	{}	t	0	2026-01-16 11:15:15.626	2026-01-16 11:23:53.186	2026-01-16 11:23:53.187
openrouter:qwen/qwen-2.5-vl-7b-instruct:free	openrouter	qwen/qwen-2.5-vl-7b-instruct:free	{"id": "qwen/qwen-2.5-vl-7b-instruct:free", "name": "Qwen: Qwen2.5-VL 7B Instruct (free)", "created": 1724803200, "pricing": {"image": "0", "prompt": "0", "request": "0", "completion": "0", "web_search": "0", "internal_reasoning": "0"}, "description": "Qwen2.5 VL 7B is a multimodal LLM from the Qwen Team with the following key enhancements:\\n\\n- SoTA understanding of images of various resolution & ratio: Qwen2.5-VL achieves state-of-the-art performance on visual understanding benchmarks, including MathVista, DocVQA, RealWorldQA, MTVQA, etc.\\n\\n- Understanding videos of 20min+: Qwen2.5-VL can understand videos over 20 minutes for high-quality video-based question answering, dialog, content creation, etc.\\n\\n- Agent that can operate your mobiles, robots, etc.: with the abilities of complex reasoning and decision making, Qwen2.5-VL can be integrated with devices like mobile phones, robots, etc., for automatic operation based on visual environment and text instructions.\\n\\n- Multilingual Support: to serve global users, besides English and Chinese, Qwen2.5-VL now supports the understanding of texts in different languages inside images, including most European languages, Japanese, Korean, Arabic, Vietnamese, etc.\\n\\nFor more details, see this [blog post](https://qwenlm.github.io/blog/qwen2-vl/) and [GitHub repo](https://github.com/QwenLM/Qwen2-VL).\\n\\nUsage of this model is subject to [Tongyi Qianwen LICENSE AGREEMENT](https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE).", "architecture": {"modality": "text+image->text", "tokenizer": "Qwen", "instruct_type": null, "input_modalities": ["text", "image"], "output_modalities": ["text"]}, "top_provider": {"is_moderated": false, "context_length": 32768, "max_completion_tokens": null}, "canonical_slug": "qwen/qwen-2-vl-7b-instruct", "context_length": 32768, "expiration_date": null, "hugging_face_id": "Qwen/Qwen2.5-VL-7B-Instruct", "default_parameters": {}, "per_request_limits": null, "supported_parameters": ["frequency_penalty", "max_tokens", "presence_penalty", "repetition_penalty", "temperature"]}	{}	f	0	2026-01-16 11:15:14.677	2026-01-16 11:18:56.34	2026-01-16 11:19:02.539
groq:openai/gpt-oss-safeguard-20b	groq	openai/gpt-oss-safeguard-20b	{"id": "openai/gpt-oss-safeguard-20b", "active": true, "object": "model", "created": 1761708789, "owned_by": "OpenAI", "public_apps": null, "context_window": 131072, "max_completion_tokens": 65536}	{}	f	0	2026-01-16 11:15:15.025	2026-01-16 11:19:00.08	2026-01-16 11:23:55.425
groq:moonshotai/kimi-k2-instruct	groq	moonshotai/kimi-k2-instruct	{"id": "moonshotai/kimi-k2-instruct", "active": true, "object": "model", "created": 1752435491, "owned_by": "Moonshot AI", "public_apps": null, "context_window": 131072, "max_completion_tokens": 16384}	{}	f	0	2026-01-16 11:15:15.014	2026-01-16 11:18:59.901	2026-01-16 11:23:55.425
groq:moonshotai/kimi-k2-instruct-0905	groq	moonshotai/kimi-k2-instruct-0905	{"id": "moonshotai/kimi-k2-instruct-0905", "active": true, "object": "model", "created": 1757046093, "owned_by": "Moonshot AI", "public_apps": null, "context_window": 262144, "max_completion_tokens": 16384}	{}	f	0	2026-01-16 11:15:15.054	2026-01-16 11:19:00.055	2026-01-16 11:23:55.425
nvidia:deepseek-ai/deepseek-v3.1	nvidia	deepseek-ai/deepseek-v3.1	{"id": "deepseek-ai/deepseek-v3.1", "object": "model", "created": 735790403, "owned_by": "deepseek-ai"}	{}	t	0	2026-01-16 11:15:15.731	2026-01-16 11:23:53.238	2026-01-16 11:23:53.239
nvidia:deepseek-ai/deepseek-v3.1-terminus	nvidia	deepseek-ai/deepseek-v3.1-terminus	{"id": "deepseek-ai/deepseek-v3.1-terminus", "object": "model", "created": 735790403, "owned_by": "deepseek-ai"}	{}	t	0	2026-01-16 11:15:15.744	2026-01-16 11:23:53.244	2026-01-16 11:23:53.245
nvidia:deepseek-ai/deepseek-v3.2	nvidia	deepseek-ai/deepseek-v3.2	{"id": "deepseek-ai/deepseek-v3.2", "object": "model", "created": 735790403, "owned_by": "deepseek-ai"}	{}	t	0	2026-01-16 11:15:15.756	2026-01-16 11:23:53.25	2026-01-16 11:23:53.251
nvidia:google/codegemma-1.1-7b	nvidia	google/codegemma-1.1-7b	{"id": "google/codegemma-1.1-7b", "object": "model", "created": 735790403, "owned_by": "google"}	{}	t	0	2026-01-16 11:15:15.768	2026-01-16 11:23:53.257	2026-01-16 11:23:53.258
nvidia:google/codegemma-7b	nvidia	google/codegemma-7b	{"id": "google/codegemma-7b", "object": "model", "created": 735790403, "owned_by": "google"}	{}	t	0	2026-01-16 11:15:15.779	2026-01-16 11:23:53.263	2026-01-16 11:23:53.264
nvidia:google/deplot	nvidia	google/deplot	{"id": "google/deplot", "object": "model", "created": 735790403, "owned_by": "google"}	{}	t	0	2026-01-16 11:15:15.791	2026-01-16 11:23:53.269	2026-01-16 11:23:53.27
nvidia:google/gemma-2-27b-it	nvidia	google/gemma-2-27b-it	{"id": "google/gemma-2-27b-it", "object": "model", "created": 735790403, "owned_by": "google"}	{}	t	0	2026-01-16 11:15:15.803	2026-01-16 11:23:53.275	2026-01-16 11:23:53.276
nvidia:google/gemma-2-2b-it	nvidia	google/gemma-2-2b-it	{"id": "google/gemma-2-2b-it", "object": "model", "created": 735790403, "owned_by": "google"}	{}	t	0	2026-01-16 11:15:15.815	2026-01-16 11:23:53.283	2026-01-16 11:23:53.283
nvidia:google/gemma-2-9b-it	nvidia	google/gemma-2-9b-it	{"id": "google/gemma-2-9b-it", "object": "model", "created": 735790403, "owned_by": "google"}	{}	t	0	2026-01-16 11:15:15.827	2026-01-16 11:23:53.289	2026-01-16 11:23:53.289
nvidia:google/gemma-2b	nvidia	google/gemma-2b	{"id": "google/gemma-2b", "object": "model", "created": 735790403, "owned_by": "google"}	{}	t	0	2026-01-16 11:15:15.839	2026-01-16 11:23:53.295	2026-01-16 11:23:53.296
nvidia:google/gemma-3-12b-it	nvidia	google/gemma-3-12b-it	{"id": "google/gemma-3-12b-it", "object": "model", "created": 735790403, "owned_by": "google"}	{}	t	0	2026-01-16 11:15:15.852	2026-01-16 11:23:53.301	2026-01-16 11:23:53.302
nvidia:google/gemma-3-1b-it	nvidia	google/gemma-3-1b-it	{"id": "google/gemma-3-1b-it", "object": "model", "created": 735790403, "owned_by": "google"}	{}	t	0	2026-01-16 11:15:15.863	2026-01-16 11:23:53.308	2026-01-16 11:23:53.308
nvidia:google/gemma-3-27b-it	nvidia	google/gemma-3-27b-it	{"id": "google/gemma-3-27b-it", "object": "model", "created": 735790403, "owned_by": "google"}	{}	t	0	2026-01-16 11:15:15.874	2026-01-16 11:23:53.313	2026-01-16 11:23:53.314
nvidia:google/gemma-3-4b-it	nvidia	google/gemma-3-4b-it	{"id": "google/gemma-3-4b-it", "object": "model", "created": 735790403, "owned_by": "google"}	{}	t	0	2026-01-16 11:15:15.886	2026-01-16 11:23:53.319	2026-01-16 11:23:53.32
nvidia:google/gemma-3n-e2b-it	nvidia	google/gemma-3n-e2b-it	{"id": "google/gemma-3n-e2b-it", "object": "model", "created": 735790403, "owned_by": "google"}	{}	t	0	2026-01-16 11:15:15.899	2026-01-16 11:23:53.325	2026-01-16 11:23:53.326
nvidia:google/gemma-3n-e4b-it	nvidia	google/gemma-3n-e4b-it	{"id": "google/gemma-3n-e4b-it", "object": "model", "created": 735790403, "owned_by": "google"}	{}	t	0	2026-01-16 11:15:15.911	2026-01-16 11:23:53.331	2026-01-16 11:23:53.332
nvidia:google/gemma-7b	nvidia	google/gemma-7b	{"id": "google/gemma-7b", "object": "model", "created": 735790403, "owned_by": "google"}	{}	t	0	2026-01-16 11:15:15.923	2026-01-16 11:23:53.338	2026-01-16 11:23:53.339
nvidia:google/paligemma	nvidia	google/paligemma	{"id": "google/paligemma", "object": "model", "created": 735790403, "owned_by": "google"}	{}	t	0	2026-01-16 11:15:15.936	2026-01-16 11:23:53.345	2026-01-16 11:23:53.345
nvidia:google/recurrentgemma-2b	nvidia	google/recurrentgemma-2b	{"id": "google/recurrentgemma-2b", "object": "model", "created": 735790403, "owned_by": "google"}	{}	t	0	2026-01-16 11:15:15.949	2026-01-16 11:23:53.351	2026-01-16 11:23:53.351
nvidia:google/shieldgemma-9b	nvidia	google/shieldgemma-9b	{"id": "google/shieldgemma-9b", "object": "model", "created": 735790403, "owned_by": "google"}	{}	t	0	2026-01-16 11:15:15.962	2026-01-16 11:23:53.356	2026-01-16 11:23:53.357
nvidia:gotocompany/gemma-2-9b-cpt-sahabatai-instruct	nvidia	gotocompany/gemma-2-9b-cpt-sahabatai-instruct	{"id": "gotocompany/gemma-2-9b-cpt-sahabatai-instruct", "object": "model", "created": 735790403, "owned_by": "gotocompany"}	{}	t	0	2026-01-16 11:15:15.98	2026-01-16 11:23:53.362	2026-01-16 11:23:53.363
nvidia:ibm/granite-3.0-3b-a800m-instruct	nvidia	ibm/granite-3.0-3b-a800m-instruct	{"id": "ibm/granite-3.0-3b-a800m-instruct", "object": "model", "created": 735790403, "owned_by": "ibm"}	{}	t	0	2026-01-16 11:15:15.992	2026-01-16 11:23:53.37	2026-01-16 11:23:53.371
nvidia:ibm/granite-3.0-8b-instruct	nvidia	ibm/granite-3.0-8b-instruct	{"id": "ibm/granite-3.0-8b-instruct", "object": "model", "created": 735790403, "owned_by": "ibm"}	{}	t	0	2026-01-16 11:15:16.004	2026-01-16 11:23:53.376	2026-01-16 11:23:53.377
nvidia:ibm/granite-3.3-8b-instruct	nvidia	ibm/granite-3.3-8b-instruct	{"id": "ibm/granite-3.3-8b-instruct", "object": "model", "created": 735790403, "owned_by": "ibm"}	{}	t	0	2026-01-16 11:15:16.017	2026-01-16 11:23:53.382	2026-01-16 11:23:53.383
nvidia:ibm/granite-34b-code-instruct	nvidia	ibm/granite-34b-code-instruct	{"id": "ibm/granite-34b-code-instruct", "object": "model", "created": 735790403, "owned_by": "ibm"}	{}	t	0	2026-01-16 11:15:16.029	2026-01-16 11:23:53.388	2026-01-16 11:23:53.389
nvidia:ibm/granite-8b-code-instruct	nvidia	ibm/granite-8b-code-instruct	{"id": "ibm/granite-8b-code-instruct", "object": "model", "created": 735790403, "owned_by": "ibm"}	{}	t	0	2026-01-16 11:15:16.041	2026-01-16 11:23:53.394	2026-01-16 11:23:53.395
nvidia:ibm/granite-guardian-3.0-8b	nvidia	ibm/granite-guardian-3.0-8b	{"id": "ibm/granite-guardian-3.0-8b", "object": "model", "created": 735790403, "owned_by": "ibm"}	{}	t	0	2026-01-16 11:15:16.054	2026-01-16 11:23:53.401	2026-01-16 11:23:53.402
nvidia:deepseek-ai/deepseek-r1-distill-llama-8b	nvidia	deepseek-ai/deepseek-r1-distill-llama-8b	{"id": "deepseek-ai/deepseek-r1-distill-llama-8b", "object": "model", "created": 735790403, "owned_by": "deepseek-ai"}	{}	t	0	2026-01-16 11:15:15.676	2026-01-16 11:23:53.213	2026-01-16 11:23:53.213
nvidia:deepseek-ai/deepseek-r1-distill-qwen-14b	nvidia	deepseek-ai/deepseek-r1-distill-qwen-14b	{"id": "deepseek-ai/deepseek-r1-distill-qwen-14b", "object": "model", "created": 735790403, "owned_by": "deepseek-ai"}	{}	t	0	2026-01-16 11:15:15.689	2026-01-16 11:23:53.22	2026-01-16 11:23:53.221
nvidia:deepseek-ai/deepseek-r1-distill-qwen-32b	nvidia	deepseek-ai/deepseek-r1-distill-qwen-32b	{"id": "deepseek-ai/deepseek-r1-distill-qwen-32b", "object": "model", "created": 735790403, "owned_by": "deepseek-ai"}	{}	t	0	2026-01-16 11:15:15.702	2026-01-16 11:23:53.226	2026-01-16 11:23:53.227
nvidia:deepseek-ai/deepseek-r1-distill-qwen-7b	nvidia	deepseek-ai/deepseek-r1-distill-qwen-7b	{"id": "deepseek-ai/deepseek-r1-distill-qwen-7b", "object": "model", "created": 735790403, "owned_by": "deepseek-ai"}	{}	t	0	2026-01-16 11:15:15.713	2026-01-16 11:23:53.232	2026-01-16 11:23:53.233
nvidia:mediatek/breeze-7b-instruct	nvidia	mediatek/breeze-7b-instruct	{"id": "mediatek/breeze-7b-instruct", "object": "model", "created": 735790403, "owned_by": "mediatek"}	{}	t	0	2026-01-16 11:15:16.135	2026-01-16 11:23:53.44	2026-01-16 11:23:53.441
nvidia:meta/codellama-70b	nvidia	meta/codellama-70b	{"id": "meta/codellama-70b", "object": "model", "created": 735790403, "owned_by": "meta"}	{}	t	0	2026-01-16 11:15:16.148	2026-01-16 11:23:53.447	2026-01-16 11:23:53.447
nvidia:meta/llama-3.1-405b-instruct	nvidia	meta/llama-3.1-405b-instruct	{"id": "meta/llama-3.1-405b-instruct", "object": "model", "created": 735790403, "owned_by": "meta"}	{}	t	0	2026-01-16 11:15:16.161	2026-01-16 11:23:53.454	2026-01-16 11:23:53.454
nvidia:meta/llama-3.1-70b-instruct	nvidia	meta/llama-3.1-70b-instruct	{"id": "meta/llama-3.1-70b-instruct", "object": "model", "created": 735790403, "owned_by": "meta"}	{}	t	0	2026-01-16 11:15:16.173	2026-01-16 11:23:53.46	2026-01-16 11:23:53.461
nvidia:meta/llama-3.1-8b-instruct	nvidia	meta/llama-3.1-8b-instruct	{"id": "meta/llama-3.1-8b-instruct", "object": "model", "created": 735790403, "owned_by": "meta"}	{}	t	0	2026-01-16 11:15:16.187	2026-01-16 11:23:53.467	2026-01-16 11:23:53.467
nvidia:meta/llama-3.2-11b-vision-instruct	nvidia	meta/llama-3.2-11b-vision-instruct	{"id": "meta/llama-3.2-11b-vision-instruct", "object": "model", "created": 735790403, "owned_by": "meta"}	{}	t	0	2026-01-16 11:15:16.2	2026-01-16 11:23:53.474	2026-01-16 11:23:53.474
nvidia:meta/llama-3.2-1b-instruct	nvidia	meta/llama-3.2-1b-instruct	{"id": "meta/llama-3.2-1b-instruct", "object": "model", "created": 735790403, "owned_by": "meta"}	{}	t	0	2026-01-16 11:15:16.217	2026-01-16 11:23:53.48	2026-01-16 11:23:53.481
nvidia:meta/llama-3.2-3b-instruct	nvidia	meta/llama-3.2-3b-instruct	{"id": "meta/llama-3.2-3b-instruct", "object": "model", "created": 735790403, "owned_by": "meta"}	{}	t	0	2026-01-16 11:15:16.229	2026-01-16 11:23:53.487	2026-01-16 11:23:53.488
nvidia:meta/llama-3.2-90b-vision-instruct	nvidia	meta/llama-3.2-90b-vision-instruct	{"id": "meta/llama-3.2-90b-vision-instruct", "object": "model", "created": 735790403, "owned_by": "meta"}	{}	t	0	2026-01-16 11:15:16.245	2026-01-16 11:23:53.494	2026-01-16 11:23:53.494
nvidia:meta/llama-3.3-70b-instruct	nvidia	meta/llama-3.3-70b-instruct	{"id": "meta/llama-3.3-70b-instruct", "object": "model", "created": 735790403, "owned_by": "meta"}	{}	t	0	2026-01-16 11:15:16.269	2026-01-16 11:23:53.5	2026-01-16 11:23:53.5
nvidia:meta/llama-4-maverick-17b-128e-instruct	nvidia	meta/llama-4-maverick-17b-128e-instruct	{"id": "meta/llama-4-maverick-17b-128e-instruct", "object": "model", "created": 735790403, "owned_by": "meta"}	{}	t	0	2026-01-16 11:15:16.281	2026-01-16 11:23:53.506	2026-01-16 11:23:53.507
nvidia:meta/llama-4-scout-17b-16e-instruct	nvidia	meta/llama-4-scout-17b-16e-instruct	{"id": "meta/llama-4-scout-17b-16e-instruct", "object": "model", "created": 735790403, "owned_by": "meta"}	{}	t	0	2026-01-16 11:15:16.294	2026-01-16 11:23:53.512	2026-01-16 11:23:53.513
nvidia:meta/llama-guard-4-12b	nvidia	meta/llama-guard-4-12b	{"id": "meta/llama-guard-4-12b", "object": "model", "created": 735790403, "owned_by": "meta"}	{}	t	0	2026-01-16 11:15:16.306	2026-01-16 11:23:53.519	2026-01-16 11:23:53.52
nvidia:meta/llama2-70b	nvidia	meta/llama2-70b	{"id": "meta/llama2-70b", "object": "model", "created": 735790403, "owned_by": "meta"}	{}	t	0	2026-01-16 11:15:16.323	2026-01-16 11:23:53.526	2026-01-16 11:23:53.526
nvidia:meta/llama3-70b-instruct	nvidia	meta/llama3-70b-instruct	{"id": "meta/llama3-70b-instruct", "object": "model", "created": 735790403, "owned_by": "meta"}	{}	t	0	2026-01-16 11:15:16.335	2026-01-16 11:23:53.532	2026-01-16 11:23:53.533
nvidia:meta/llama3-8b-instruct	nvidia	meta/llama3-8b-instruct	{"id": "meta/llama3-8b-instruct", "object": "model", "created": 735790403, "owned_by": "meta"}	{}	t	0	2026-01-16 11:15:16.348	2026-01-16 11:23:53.539	2026-01-16 11:23:53.539
nvidia:microsoft/kosmos-2	nvidia	microsoft/kosmos-2	{"id": "microsoft/kosmos-2", "object": "model", "created": 735790403, "owned_by": "microsoft"}	{}	t	0	2026-01-16 11:15:16.359	2026-01-16 11:23:53.545	2026-01-16 11:23:53.546
nvidia:microsoft/phi-3-medium-128k-instruct	nvidia	microsoft/phi-3-medium-128k-instruct	{"id": "microsoft/phi-3-medium-128k-instruct", "object": "model", "created": 735790403, "owned_by": "microsoft"}	{}	t	0	2026-01-16 11:15:16.371	2026-01-16 11:23:53.551	2026-01-16 11:23:53.552
nvidia:microsoft/phi-3-medium-4k-instruct	nvidia	microsoft/phi-3-medium-4k-instruct	{"id": "microsoft/phi-3-medium-4k-instruct", "object": "model", "created": 735790403, "owned_by": "microsoft"}	{}	t	0	2026-01-16 11:15:16.384	2026-01-16 11:23:53.558	2026-01-16 11:23:53.558
nvidia:microsoft/phi-3-mini-128k-instruct	nvidia	microsoft/phi-3-mini-128k-instruct	{"id": "microsoft/phi-3-mini-128k-instruct", "object": "model", "created": 735790403, "owned_by": "microsoft"}	{}	t	0	2026-01-16 11:15:16.396	2026-01-16 11:23:53.564	2026-01-16 11:23:53.564
nvidia:microsoft/phi-3-mini-4k-instruct	nvidia	microsoft/phi-3-mini-4k-instruct	{"id": "microsoft/phi-3-mini-4k-instruct", "object": "model", "created": 735790403, "owned_by": "microsoft"}	{}	t	0	2026-01-16 11:15:16.407	2026-01-16 11:23:53.57	2026-01-16 11:23:53.571
nvidia:microsoft/phi-3-small-128k-instruct	nvidia	microsoft/phi-3-small-128k-instruct	{"id": "microsoft/phi-3-small-128k-instruct", "object": "model", "created": 735790403, "owned_by": "microsoft"}	{}	t	0	2026-01-16 11:15:16.419	2026-01-16 11:23:53.577	2026-01-16 11:23:53.578
nvidia:microsoft/phi-3-small-8k-instruct	nvidia	microsoft/phi-3-small-8k-instruct	{"id": "microsoft/phi-3-small-8k-instruct", "object": "model", "created": 735790403, "owned_by": "microsoft"}	{}	t	0	2026-01-16 11:15:16.432	2026-01-16 11:23:53.584	2026-01-16 11:23:53.584
nvidia:microsoft/phi-3-vision-128k-instruct	nvidia	microsoft/phi-3-vision-128k-instruct	{"id": "microsoft/phi-3-vision-128k-instruct", "object": "model", "created": 735790403, "owned_by": "microsoft"}	{}	t	0	2026-01-16 11:15:16.444	2026-01-16 11:23:53.59	2026-01-16 11:23:53.591
nvidia:microsoft/phi-3.5-mini-instruct	nvidia	microsoft/phi-3.5-mini-instruct	{"id": "microsoft/phi-3.5-mini-instruct", "object": "model", "created": 735790403, "owned_by": "microsoft"}	{}	t	0	2026-01-16 11:15:16.462	2026-01-16 11:23:53.596	2026-01-16 11:23:53.597
nvidia:igenius/italia_10b_instruct_16k	nvidia	igenius/italia_10b_instruct_16k	{"id": "igenius/italia_10b_instruct_16k", "object": "model", "created": 735790403, "owned_by": "igenius"}	{}	t	0	2026-01-16 11:15:16.085	2026-01-16 11:23:53.413	2026-01-16 11:23:53.414
nvidia:institute-of-science-tokyo/llama-3.1-swallow-70b-instruct-v0.1	nvidia	institute-of-science-tokyo/llama-3.1-swallow-70b-instruct-v0.1	{"id": "institute-of-science-tokyo/llama-3.1-swallow-70b-instruct-v0.1", "object": "model", "created": 735790403, "owned_by": "institute-of-science-tokyo"}	{}	t	0	2026-01-16 11:15:16.098	2026-01-16 11:23:53.42	2026-01-16 11:23:53.421
nvidia:institute-of-science-tokyo/llama-3.1-swallow-8b-instruct-v0.1	nvidia	institute-of-science-tokyo/llama-3.1-swallow-8b-instruct-v0.1	{"id": "institute-of-science-tokyo/llama-3.1-swallow-8b-instruct-v0.1", "object": "model", "created": 735790403, "owned_by": "institute-of-science-tokyo"}	{}	t	0	2026-01-16 11:15:16.111	2026-01-16 11:23:53.427	2026-01-16 11:23:53.428
nvidia:marin/marin-8b-instruct	nvidia	marin/marin-8b-instruct	{"id": "marin/marin-8b-instruct", "object": "model", "created": 735790403, "owned_by": "marin"}	{}	t	0	2026-01-16 11:15:16.122	2026-01-16 11:23:53.434	2026-01-16 11:23:53.434
nvidia:mistralai/codestral-22b-instruct-v0.1	nvidia	mistralai/codestral-22b-instruct-v0.1	{"id": "mistralai/codestral-22b-instruct-v0.1", "object": "model", "created": 735790403, "owned_by": "mistralai"}	{}	t	0	2026-01-16 11:15:16.561	2026-01-16 11:23:53.65	2026-01-16 11:23:53.651
nvidia:mistralai/devstral-2-123b-instruct-2512	nvidia	mistralai/devstral-2-123b-instruct-2512	{"id": "mistralai/devstral-2-123b-instruct-2512", "object": "model", "created": 735790403, "owned_by": "mistralai"}	{}	t	0	2026-01-16 11:15:16.572	2026-01-16 11:23:53.657	2026-01-16 11:23:53.658
nvidia:mistralai/mamba-codestral-7b-v0.1	nvidia	mistralai/mamba-codestral-7b-v0.1	{"id": "mistralai/mamba-codestral-7b-v0.1", "object": "model", "created": 735790403, "owned_by": "mistralai"}	{}	t	0	2026-01-16 11:15:16.596	2026-01-16 11:23:53.669	2026-01-16 11:23:53.67
nvidia:mistralai/mathstral-7b-v0.1	nvidia	mistralai/mathstral-7b-v0.1	{"id": "mistralai/mathstral-7b-v0.1", "object": "model", "created": 735790403, "owned_by": "mistralai"}	{}	t	0	2026-01-16 11:15:16.609	2026-01-16 11:23:53.676	2026-01-16 11:23:53.677
nvidia:mistralai/ministral-14b-instruct-2512	nvidia	mistralai/ministral-14b-instruct-2512	{"id": "mistralai/ministral-14b-instruct-2512", "object": "model", "created": 735790403, "owned_by": "mistralai"}	{}	t	0	2026-01-16 11:15:16.621	2026-01-16 11:23:53.683	2026-01-16 11:23:53.683
nvidia:mistralai/mistral-7b-instruct-v0.2	nvidia	mistralai/mistral-7b-instruct-v0.2	{"id": "mistralai/mistral-7b-instruct-v0.2", "object": "model", "created": 735790403, "owned_by": "mistralai"}	{}	t	0	2026-01-16 11:15:16.632	2026-01-16 11:23:53.689	2026-01-16 11:23:53.69
nvidia:mistralai/mistral-large	nvidia	mistralai/mistral-large	{"id": "mistralai/mistral-large", "object": "model", "created": 735790403, "owned_by": "mistralai"}	{}	t	0	2026-01-16 11:15:16.656	2026-01-16 11:23:53.702	2026-01-16 11:23:53.703
nvidia:mistralai/mistral-large-2-instruct	nvidia	mistralai/mistral-large-2-instruct	{"id": "mistralai/mistral-large-2-instruct", "object": "model", "created": 735790403, "owned_by": "mistralai"}	{}	t	0	2026-01-16 11:15:16.667	2026-01-16 11:23:53.709	2026-01-16 11:23:53.71
nvidia:mistralai/mistral-medium-3-instruct	nvidia	mistralai/mistral-medium-3-instruct	{"id": "mistralai/mistral-medium-3-instruct", "object": "model", "created": 735790403, "owned_by": "mistralai"}	{}	t	0	2026-01-16 11:15:16.69	2026-01-16 11:23:53.722	2026-01-16 11:23:53.723
nvidia:mistralai/mistral-small-24b-instruct	nvidia	mistralai/mistral-small-24b-instruct	{"id": "mistralai/mistral-small-24b-instruct", "object": "model", "created": 735790403, "owned_by": "mistralai"}	{}	t	0	2026-01-16 11:15:16.714	2026-01-16 11:23:53.735	2026-01-16 11:23:53.736
nvidia:mistralai/mistral-small-3.1-24b-instruct-2503	nvidia	mistralai/mistral-small-3.1-24b-instruct-2503	{"id": "mistralai/mistral-small-3.1-24b-instruct-2503", "object": "model", "created": 735790403, "owned_by": "mistralai"}	{}	t	0	2026-01-16 11:15:16.727	2026-01-16 11:23:53.742	2026-01-16 11:23:53.743
nvidia:mistralai/mixtral-8x22b-instruct-v0.1	nvidia	mistralai/mixtral-8x22b-instruct-v0.1	{"id": "mistralai/mixtral-8x22b-instruct-v0.1", "object": "model", "created": 735790403, "owned_by": "mistralai"}	{}	t	0	2026-01-16 11:15:16.739	2026-01-16 11:23:53.748	2026-01-16 11:23:53.749
nvidia:mistralai/mixtral-8x22b-v0.1	nvidia	mistralai/mixtral-8x22b-v0.1	{"id": "mistralai/mixtral-8x22b-v0.1", "object": "model", "created": 735790403, "owned_by": "mistralai"}	{}	t	0	2026-01-16 11:15:16.751	2026-01-16 11:23:53.756	2026-01-16 11:23:53.757
nvidia:moonshotai/kimi-k2-instruct	nvidia	moonshotai/kimi-k2-instruct	{"id": "moonshotai/kimi-k2-instruct", "object": "model", "created": 735790403, "owned_by": "moonshotai"}	{}	t	0	2026-01-16 11:15:16.775	2026-01-16 11:23:53.769	2026-01-16 11:23:53.77
nvidia:moonshotai/kimi-k2-instruct-0905	nvidia	moonshotai/kimi-k2-instruct-0905	{"id": "moonshotai/kimi-k2-instruct-0905", "object": "model", "created": 735790403, "owned_by": "moonshotai"}	{}	t	0	2026-01-16 11:15:16.786	2026-01-16 11:23:53.776	2026-01-16 11:23:53.777
nvidia:nv-mistralai/mistral-nemo-12b-instruct	nvidia	nv-mistralai/mistral-nemo-12b-instruct	{"id": "nv-mistralai/mistral-nemo-12b-instruct", "object": "model", "created": 735790403, "owned_by": "nv-mistralai"}	{}	t	0	2026-01-16 11:15:16.811	2026-01-16 11:23:53.789	2026-01-16 11:23:53.79
nvidia:nvidia/cosmos-reason2-8b	nvidia	nvidia/cosmos-reason2-8b	{"id": "nvidia/cosmos-reason2-8b", "object": "model", "created": 735790403, "owned_by": "nvidia"}	{}	t	0	2026-01-16 11:15:16.824	2026-01-16 11:23:53.795	2026-01-16 11:23:53.796
nvidia:microsoft/phi-3.5-vision-instruct	nvidia	microsoft/phi-3.5-vision-instruct	{"id": "microsoft/phi-3.5-vision-instruct", "object": "model", "created": 735790403, "owned_by": "microsoft"}	{}	t	0	2026-01-16 11:15:16.485	2026-01-16 11:23:53.61	2026-01-16 11:23:53.61
nvidia:microsoft/phi-4-mini-flash-reasoning	nvidia	microsoft/phi-4-mini-flash-reasoning	{"id": "microsoft/phi-4-mini-flash-reasoning", "object": "model", "created": 735790403, "owned_by": "microsoft"}	{}	t	0	2026-01-16 11:15:16.502	2026-01-16 11:23:53.616	2026-01-16 11:23:53.617
nvidia:microsoft/phi-4-mini-instruct	nvidia	microsoft/phi-4-mini-instruct	{"id": "microsoft/phi-4-mini-instruct", "object": "model", "created": 735790403, "owned_by": "microsoft"}	{}	t	0	2026-01-16 11:15:16.514	2026-01-16 11:23:53.623	2026-01-16 11:23:53.623
nvidia:microsoft/phi-4-multimodal-instruct	nvidia	microsoft/phi-4-multimodal-instruct	{"id": "microsoft/phi-4-multimodal-instruct", "object": "model", "created": 735790403, "owned_by": "microsoft"}	{}	t	0	2026-01-16 11:15:16.526	2026-01-16 11:23:53.63	2026-01-16 11:23:53.631
nvidia:minimaxai/minimax-m2	nvidia	minimaxai/minimax-m2	{"id": "minimaxai/minimax-m2", "object": "model", "created": 735790403, "owned_by": "minimaxai"}	{}	t	0	2026-01-16 11:15:16.538	2026-01-16 11:23:53.637	2026-01-16 11:23:53.638
nvidia:minimaxai/minimax-m2.1	nvidia	minimaxai/minimax-m2.1	{"id": "minimaxai/minimax-m2.1", "object": "model", "created": 735790403, "owned_by": "minimaxai"}	{}	t	0	2026-01-16 11:15:16.549	2026-01-16 11:23:53.643	2026-01-16 11:23:53.644
nvidia:mistralai/mistral-nemotron	nvidia	mistralai/mistral-nemotron	{"id": "mistralai/mistral-nemotron", "object": "model", "created": 735790403, "owned_by": "mistralai"}	{}	t	0	2026-01-16 11:15:16.702	2026-01-16 11:23:53.729	2026-01-16 11:23:53.73
nvidia:moonshotai/kimi-k2-thinking	nvidia	moonshotai/kimi-k2-thinking	{"id": "moonshotai/kimi-k2-thinking", "object": "model", "created": 735790403, "owned_by": "moonshotai"}	{}	t	0	2026-01-16 11:15:16.799	2026-01-16 11:23:53.782	2026-01-16 11:23:53.783
nvidia:nvidia/llama-3.1-nemotron-nano-4b-v1.1	nvidia	nvidia/llama-3.1-nemotron-nano-4b-v1.1	{"id": "nvidia/llama-3.1-nemotron-nano-4b-v1.1", "object": "model", "created": 735790403, "owned_by": "nvidia"}	{}	t	0	2026-01-16 11:15:16.925	2026-01-16 11:23:53.841	2026-01-16 11:23:53.842
nvidia:nvidia/llama-3.1-nemotron-nano-8b-v1	nvidia	nvidia/llama-3.1-nemotron-nano-8b-v1	{"id": "nvidia/llama-3.1-nemotron-nano-8b-v1", "object": "model", "created": 735790403, "owned_by": "nvidia"}	{}	t	0	2026-01-16 11:15:16.939	2026-01-16 11:23:53.847	2026-01-16 11:23:53.848
nvidia:nvidia/llama-3.1-nemotron-nano-vl-8b-v1	nvidia	nvidia/llama-3.1-nemotron-nano-vl-8b-v1	{"id": "nvidia/llama-3.1-nemotron-nano-vl-8b-v1", "object": "model", "created": 735790403, "owned_by": "nvidia"}	{}	t	0	2026-01-16 11:15:16.95	2026-01-16 11:23:53.854	2026-01-16 11:23:53.855
nvidia:nvidia/llama-3.1-nemotron-safety-guard-8b-v3	nvidia	nvidia/llama-3.1-nemotron-safety-guard-8b-v3	{"id": "nvidia/llama-3.1-nemotron-safety-guard-8b-v3", "object": "model", "created": 735790403, "owned_by": "nvidia"}	{}	t	0	2026-01-16 11:15:16.968	2026-01-16 11:23:53.861	2026-01-16 11:23:53.862
nvidia:nvidia/llama-3.1-nemotron-ultra-253b-v1	nvidia	nvidia/llama-3.1-nemotron-ultra-253b-v1	{"id": "nvidia/llama-3.1-nemotron-ultra-253b-v1", "object": "model", "created": 735790403, "owned_by": "nvidia"}	{}	t	0	2026-01-16 11:15:16.985	2026-01-16 11:23:53.867	2026-01-16 11:23:53.868
nvidia:nvidia/llama-3.2-nemoretriever-1b-vlm-embed-v1	nvidia	nvidia/llama-3.2-nemoretriever-1b-vlm-embed-v1	{"id": "nvidia/llama-3.2-nemoretriever-1b-vlm-embed-v1", "object": "model", "created": 735790403, "owned_by": "nvidia"}	{}	t	0	2026-01-16 11:15:16.997	2026-01-16 11:23:53.874	2026-01-16 11:23:53.874
nvidia:nvidia/llama-3.2-nemoretriever-300m-embed-v1	nvidia	nvidia/llama-3.2-nemoretriever-300m-embed-v1	{"id": "nvidia/llama-3.2-nemoretriever-300m-embed-v1", "object": "model", "created": 735790403, "owned_by": "nvidia"}	{}	t	0	2026-01-16 11:15:17.015	2026-01-16 11:23:53.88	2026-01-16 11:23:53.881
nvidia:nvidia/llama-3.2-nemoretriever-300m-embed-v2	nvidia	nvidia/llama-3.2-nemoretriever-300m-embed-v2	{"id": "nvidia/llama-3.2-nemoretriever-300m-embed-v2", "object": "model", "created": 735790403, "owned_by": "nvidia"}	{}	t	0	2026-01-16 11:15:17.027	2026-01-16 11:23:53.887	2026-01-16 11:23:53.887
nvidia:nvidia/llama-3.2-nv-embedqa-1b-v1	nvidia	nvidia/llama-3.2-nv-embedqa-1b-v1	{"id": "nvidia/llama-3.2-nv-embedqa-1b-v1", "object": "model", "created": 735790403, "owned_by": "nvidia"}	{}	t	0	2026-01-16 11:15:17.039	2026-01-16 11:23:53.893	2026-01-16 11:23:53.894
nvidia:nvidia/llama-3.2-nv-embedqa-1b-v2	nvidia	nvidia/llama-3.2-nv-embedqa-1b-v2	{"id": "nvidia/llama-3.2-nv-embedqa-1b-v2", "object": "model", "created": 735790403, "owned_by": "nvidia"}	{}	t	0	2026-01-16 11:15:17.051	2026-01-16 11:23:53.9	2026-01-16 11:23:53.901
nvidia:nvidia/llama-3.3-nemotron-super-49b-v1	nvidia	nvidia/llama-3.3-nemotron-super-49b-v1	{"id": "nvidia/llama-3.3-nemotron-super-49b-v1", "object": "model", "created": 735790403, "owned_by": "nvidia"}	{}	t	0	2026-01-16 11:15:17.063	2026-01-16 11:23:53.907	2026-01-16 11:23:53.907
nvidia:nvidia/llama-3.3-nemotron-super-49b-v1.5	nvidia	nvidia/llama-3.3-nemotron-super-49b-v1.5	{"id": "nvidia/llama-3.3-nemotron-super-49b-v1.5", "object": "model", "created": 735790403, "owned_by": "nvidia"}	{}	t	0	2026-01-16 11:15:17.074	2026-01-16 11:23:53.913	2026-01-16 11:23:53.913
nvidia:nvidia/llama3-chatqa-1.5-70b	nvidia	nvidia/llama3-chatqa-1.5-70b	{"id": "nvidia/llama3-chatqa-1.5-70b", "object": "model", "created": 735790403, "owned_by": "nvidia"}	{}	t	0	2026-01-16 11:15:17.086	2026-01-16 11:23:53.919	2026-01-16 11:23:53.92
nvidia:nvidia/llama3-chatqa-1.5-8b	nvidia	nvidia/llama3-chatqa-1.5-8b	{"id": "nvidia/llama3-chatqa-1.5-8b", "object": "model", "created": 735790403, "owned_by": "nvidia"}	{}	t	0	2026-01-16 11:15:17.098	2026-01-16 11:23:53.926	2026-01-16 11:23:53.927
nvidia:nvidia/mistral-nemo-minitron-8b-8k-instruct	nvidia	nvidia/mistral-nemo-minitron-8b-8k-instruct	{"id": "nvidia/mistral-nemo-minitron-8b-8k-instruct", "object": "model", "created": 735790403, "owned_by": "nvidia"}	{}	t	0	2026-01-16 11:15:17.11	2026-01-16 11:23:53.933	2026-01-16 11:23:53.933
nvidia:nvidia/mistral-nemo-minitron-8b-base	nvidia	nvidia/mistral-nemo-minitron-8b-base	{"id": "nvidia/mistral-nemo-minitron-8b-base", "object": "model", "created": 735790403, "owned_by": "nvidia"}	{}	t	0	2026-01-16 11:15:17.123	2026-01-16 11:23:53.939	2026-01-16 11:23:53.94
nvidia:nvidia/nemoretriever-parse	nvidia	nvidia/nemoretriever-parse	{"id": "nvidia/nemoretriever-parse", "object": "model", "created": 735790403, "owned_by": "nvidia"}	{}	t	0	2026-01-16 11:15:17.135	2026-01-16 11:23:53.946	2026-01-16 11:23:53.946
nvidia:nvidia/nemotron-3-nano-30b-a3b	nvidia	nvidia/nemotron-3-nano-30b-a3b	{"id": "nvidia/nemotron-3-nano-30b-a3b", "object": "model", "created": 735790403, "owned_by": "nvidia"}	{}	t	0	2026-01-16 11:15:17.147	2026-01-16 11:23:53.952	2026-01-16 11:23:53.953
nvidia:nvidia/nemotron-4-340b-instruct	nvidia	nvidia/nemotron-4-340b-instruct	{"id": "nvidia/nemotron-4-340b-instruct", "object": "model", "created": 735790403, "owned_by": "nvidia"}	{}	t	0	2026-01-16 11:15:17.159	2026-01-16 11:23:53.959	2026-01-16 11:23:53.959
nvidia:nvidia/nemotron-4-340b-reward	nvidia	nvidia/nemotron-4-340b-reward	{"id": "nvidia/nemotron-4-340b-reward", "object": "model", "created": 735790403, "owned_by": "nvidia"}	{}	t	0	2026-01-16 11:15:17.171	2026-01-16 11:23:53.965	2026-01-16 11:23:53.966
nvidia:nvidia/nemotron-mini-4b-instruct	nvidia	nvidia/nemotron-mini-4b-instruct	{"id": "nvidia/nemotron-mini-4b-instruct", "object": "model", "created": 735790403, "owned_by": "nvidia"}	{}	t	0	2026-01-16 11:15:17.196	2026-01-16 11:23:53.979	2026-01-16 11:23:53.979
nvidia:nvidia/nemotron-nano-12b-v2-vl	nvidia	nvidia/nemotron-nano-12b-v2-vl	{"id": "nvidia/nemotron-nano-12b-v2-vl", "object": "model", "created": 735790403, "owned_by": "nvidia"}	{}	t	0	2026-01-16 11:15:17.208	2026-01-16 11:23:53.985	2026-01-16 11:23:53.986
nvidia:nvidia/nemotron-nano-3-30b-a3b	nvidia	nvidia/nemotron-nano-3-30b-a3b	{"id": "nvidia/nemotron-nano-3-30b-a3b", "object": "model", "created": 735790403, "owned_by": "nvidia"}	{}	t	0	2026-01-16 11:15:17.225	2026-01-16 11:23:53.991	2026-01-16 11:23:53.992
nvidia:nvidia/llama-3.1-nemoguard-8b-topic-control	nvidia	nvidia/llama-3.1-nemoguard-8b-topic-control	{"id": "nvidia/llama-3.1-nemoguard-8b-topic-control", "object": "model", "created": 735790403, "owned_by": "nvidia"}	{}	t	0	2026-01-16 11:15:16.866	2026-01-16 11:23:53.815	2026-01-16 11:23:53.815
nvidia:nvidia/llama-3.1-nemotron-51b-instruct	nvidia	nvidia/llama-3.1-nemotron-51b-instruct	{"id": "nvidia/llama-3.1-nemotron-51b-instruct", "object": "model", "created": 735790403, "owned_by": "nvidia"}	{}	t	0	2026-01-16 11:15:16.887	2026-01-16 11:23:53.821	2026-01-16 11:23:53.822
nvidia:nvidia/llama-3.1-nemotron-70b-instruct	nvidia	nvidia/llama-3.1-nemotron-70b-instruct	{"id": "nvidia/llama-3.1-nemotron-70b-instruct", "object": "model", "created": 735790403, "owned_by": "nvidia"}	{}	t	0	2026-01-16 11:15:16.9	2026-01-16 11:23:53.828	2026-01-16 11:23:53.829
nvidia:nvidia/llama-3.1-nemotron-70b-reward	nvidia	nvidia/llama-3.1-nemotron-70b-reward	{"id": "nvidia/llama-3.1-nemotron-70b-reward", "object": "model", "created": 735790403, "owned_by": "nvidia"}	{}	t	0	2026-01-16 11:15:16.912	2026-01-16 11:23:53.834	2026-01-16 11:23:53.835
nvidia:openai/gpt-oss-120b	nvidia	openai/gpt-oss-120b	{"id": "openai/gpt-oss-120b", "object": "model", "created": 735790403, "owned_by": "openai"}	{}	t	0	2026-01-16 11:15:17.416	2026-01-16 11:23:54.091	2026-01-16 11:23:54.091
nvidia:openai/gpt-oss-20b	nvidia	openai/gpt-oss-20b	{"id": "openai/gpt-oss-20b", "object": "model", "created": 735790403, "owned_by": "openai"}	{}	t	0	2026-01-16 11:15:17.439	2026-01-16 11:23:54.104	2026-01-16 11:23:54.104
nvidia:nvidia/nv-embedqa-mistral-7b-v2	nvidia	nvidia/nv-embedqa-mistral-7b-v2	{"id": "nvidia/nv-embedqa-mistral-7b-v2", "object": "model", "created": 735790403, "owned_by": "nvidia"}	{}	t	0	2026-01-16 11:15:17.299	2026-01-16 11:23:54.029	2026-01-16 11:23:54.03
nvidia:nvidia/nvclip	nvidia	nvidia/nvclip	{"id": "nvidia/nvclip", "object": "model", "created": 735790403, "owned_by": "nvidia"}	{}	t	0	2026-01-16 11:15:17.312	2026-01-16 11:23:54.037	2026-01-16 11:23:54.038
nvidia:nvidia/nvidia-nemotron-nano-9b-v2	nvidia	nvidia/nvidia-nemotron-nano-9b-v2	{"id": "nvidia/nvidia-nemotron-nano-9b-v2", "object": "model", "created": 735790403, "owned_by": "nvidia"}	{}	t	0	2026-01-16 11:15:17.324	2026-01-16 11:23:54.044	2026-01-16 11:23:54.044
nvidia:nvidia/riva-translate-4b-instruct	nvidia	nvidia/riva-translate-4b-instruct	{"id": "nvidia/riva-translate-4b-instruct", "object": "model", "created": 735790403, "owned_by": "nvidia"}	{}	t	0	2026-01-16 11:15:17.336	2026-01-16 11:23:54.051	2026-01-16 11:23:54.051
nvidia:nvidia/riva-translate-4b-instruct-v1.1	nvidia	nvidia/riva-translate-4b-instruct-v1.1	{"id": "nvidia/riva-translate-4b-instruct-v1.1", "object": "model", "created": 735790403, "owned_by": "nvidia"}	{}	t	0	2026-01-16 11:15:17.353	2026-01-16 11:23:54.058	2026-01-16 11:23:54.059
nvidia:nvidia/streampetr	nvidia	nvidia/streampetr	{"id": "nvidia/streampetr", "object": "model", "created": 735790403, "owned_by": "nvidia"}	{}	t	0	2026-01-16 11:15:17.371	2026-01-16 11:23:54.065	2026-01-16 11:23:54.066
nvidia:nvidia/usdcode-llama-3.1-70b-instruct	nvidia	nvidia/usdcode-llama-3.1-70b-instruct	{"id": "nvidia/usdcode-llama-3.1-70b-instruct", "object": "model", "created": 735790403, "owned_by": "nvidia"}	{}	t	0	2026-01-16 11:15:17.385	2026-01-16 11:23:54.071	2026-01-16 11:23:54.072
nvidia:nvidia/vila	nvidia	nvidia/vila	{"id": "nvidia/vila", "object": "model", "created": 735790403, "owned_by": "nvidia"}	{}	t	0	2026-01-16 11:15:17.401	2026-01-16 11:23:54.078	2026-01-16 11:23:54.078
nvidia:opengpt-x/teuken-7b-instruct-commercial-v0.4	nvidia	opengpt-x/teuken-7b-instruct-commercial-v0.4	{"id": "opengpt-x/teuken-7b-instruct-commercial-v0.4", "object": "model", "created": 735790403, "owned_by": "opengpt-x"}	{}	t	0	2026-01-16 11:15:17.466	2026-01-16 11:23:54.11	2026-01-16 11:23:54.111
nvidia:qwen/qwen2-7b-instruct	nvidia	qwen/qwen2-7b-instruct	{"id": "qwen/qwen2-7b-instruct", "object": "model", "created": 735790403, "owned_by": "qwen"}	{}	t	0	2026-01-16 11:15:17.481	2026-01-16 11:23:54.116	2026-01-16 11:23:54.117
nvidia:qwen/qwen2.5-7b-instruct	nvidia	qwen/qwen2.5-7b-instruct	{"id": "qwen/qwen2.5-7b-instruct", "object": "model", "created": 735790403, "owned_by": "qwen"}	{}	t	0	2026-01-16 11:15:17.494	2026-01-16 11:23:54.122	2026-01-16 11:23:54.123
nvidia:qwen/qwen2.5-coder-32b-instruct	nvidia	qwen/qwen2.5-coder-32b-instruct	{"id": "qwen/qwen2.5-coder-32b-instruct", "object": "model", "created": 735790403, "owned_by": "qwen"}	{}	t	0	2026-01-16 11:15:17.507	2026-01-16 11:23:54.129	2026-01-16 11:23:54.13
nvidia:qwen/qwen2.5-coder-7b-instruct	nvidia	qwen/qwen2.5-coder-7b-instruct	{"id": "qwen/qwen2.5-coder-7b-instruct", "object": "model", "created": 735790403, "owned_by": "qwen"}	{}	t	0	2026-01-16 11:15:17.52	2026-01-16 11:23:54.135	2026-01-16 11:23:54.136
nvidia:qwen/qwen3-235b-a22b	nvidia	qwen/qwen3-235b-a22b	{"id": "qwen/qwen3-235b-a22b", "object": "model", "created": 735790403, "owned_by": "qwen"}	{}	t	0	2026-01-16 11:15:17.533	2026-01-16 11:23:54.142	2026-01-16 11:23:54.143
nvidia:qwen/qwen3-coder-480b-a35b-instruct	nvidia	qwen/qwen3-coder-480b-a35b-instruct	{"id": "qwen/qwen3-coder-480b-a35b-instruct", "object": "model", "created": 735790403, "owned_by": "qwen"}	{}	t	0	2026-01-16 11:15:17.546	2026-01-16 11:23:54.148	2026-01-16 11:23:54.149
nvidia:qwen/qwen3-next-80b-a3b-instruct	nvidia	qwen/qwen3-next-80b-a3b-instruct	{"id": "qwen/qwen3-next-80b-a3b-instruct", "object": "model", "created": 735790403, "owned_by": "qwen"}	{}	t	0	2026-01-16 11:15:17.558	2026-01-16 11:23:54.155	2026-01-16 11:23:54.156
nvidia:qwen/qwen3-next-80b-a3b-thinking	nvidia	qwen/qwen3-next-80b-a3b-thinking	{"id": "qwen/qwen3-next-80b-a3b-thinking", "object": "model", "created": 735790403, "owned_by": "qwen"}	{}	t	0	2026-01-16 11:15:17.571	2026-01-16 11:23:54.162	2026-01-16 11:23:54.163
nvidia:qwen/qwq-32b	nvidia	qwen/qwq-32b	{"id": "qwen/qwq-32b", "object": "model", "created": 735790403, "owned_by": "qwen"}	{}	t	0	2026-01-16 11:15:17.585	2026-01-16 11:23:54.168	2026-01-16 11:23:54.169
nvidia:rakuten/rakutenai-7b-chat	nvidia	rakuten/rakutenai-7b-chat	{"id": "rakuten/rakutenai-7b-chat", "object": "model", "created": 735790403, "owned_by": "rakuten"}	{}	t	0	2026-01-16 11:15:17.598	2026-01-16 11:23:54.175	2026-01-16 11:23:54.176
nvidia:rakuten/rakutenai-7b-instruct	nvidia	rakuten/rakutenai-7b-instruct	{"id": "rakuten/rakutenai-7b-instruct", "object": "model", "created": 735790403, "owned_by": "rakuten"}	{}	t	0	2026-01-16 11:15:17.611	2026-01-16 11:23:54.182	2026-01-16 11:23:54.182
nvidia:sarvamai/sarvam-m	nvidia	sarvamai/sarvam-m	{"id": "sarvamai/sarvam-m", "object": "model", "created": 735790403, "owned_by": "sarvamai"}	{}	t	0	2026-01-16 11:15:17.624	2026-01-16 11:23:54.188	2026-01-16 11:23:54.189
nvidia:snowflake/arctic-embed-l	nvidia	snowflake/arctic-embed-l	{"id": "snowflake/arctic-embed-l", "object": "model", "created": 735790403, "owned_by": "snowflake"}	{}	t	0	2026-01-16 11:15:17.639	2026-01-16 11:23:54.195	2026-01-16 11:23:54.196
nvidia:speakleash/bielik-11b-v2.3-instruct	nvidia	speakleash/bielik-11b-v2.3-instruct	{"id": "speakleash/bielik-11b-v2.3-instruct", "object": "model", "created": 735790403, "owned_by": "speakleash"}	{}	t	0	2026-01-16 11:15:17.652	2026-01-16 11:23:54.201	2026-01-16 11:23:54.202
nvidia:speakleash/bielik-11b-v2.6-instruct	nvidia	speakleash/bielik-11b-v2.6-instruct	{"id": "speakleash/bielik-11b-v2.6-instruct", "object": "model", "created": 735790403, "owned_by": "speakleash"}	{}	t	0	2026-01-16 11:15:17.664	2026-01-16 11:23:54.207	2026-01-16 11:23:54.208
nvidia:stockmark/stockmark-2-100b-instruct	nvidia	stockmark/stockmark-2-100b-instruct	{"id": "stockmark/stockmark-2-100b-instruct", "object": "model", "created": 735790403, "owned_by": "stockmark"}	{}	t	0	2026-01-16 11:15:17.676	2026-01-16 11:23:54.216	2026-01-16 11:23:54.216
nvidia:nvidia/neva-22b	nvidia	nvidia/neva-22b	{"id": "nvidia/neva-22b", "object": "model", "created": 735790403, "owned_by": "nvidia"}	{}	t	0	2026-01-16 11:15:17.249	2026-01-16 11:23:54.003	2026-01-16 11:23:54.004
nvidia:nvidia/nv-embed-v1	nvidia	nvidia/nv-embed-v1	{"id": "nvidia/nv-embed-v1", "object": "model", "created": 735790403, "owned_by": "nvidia"}	{}	t	0	2026-01-16 11:15:17.261	2026-01-16 11:23:54.01	2026-01-16 11:23:54.011
nvidia:nvidia/nv-embedcode-7b-v1	nvidia	nvidia/nv-embedcode-7b-v1	{"id": "nvidia/nv-embedcode-7b-v1", "object": "model", "created": 735790403, "owned_by": "nvidia"}	{}	t	0	2026-01-16 11:15:17.274	2026-01-16 11:23:54.017	2026-01-16 11:23:54.018
nvidia:nvidia/nv-embedqa-e5-v5	nvidia	nvidia/nv-embedqa-e5-v5	{"id": "nvidia/nv-embedqa-e5-v5", "object": "model", "created": 735790403, "owned_by": "nvidia"}	{}	t	0	2026-01-16 11:15:17.286	2026-01-16 11:23:54.023	2026-01-16 11:23:54.024
nvidia:z-ai/glm4.7	nvidia	z-ai/glm4.7	{"id": "z-ai/glm4.7", "object": "model", "created": 735790403, "owned_by": "z-ai"}	{}	t	0	2026-01-16 11:15:17.812	2026-01-16 11:23:54.285	2026-01-16 11:23:54.286
ollama-local:hengwen/watt-tool-8B:latest	ollama-local	hengwen/watt-tool-8B:latest	{"name": "hengwen/watt-tool-8B:latest", "size": 4921248041, "model": "hengwen/watt-tool-8B:latest", "digest": "99577f6734df650a77b291a49de7562d4757c264a2d8228e77098dd49aa4f8a0", "details": {"family": "llama", "format": "gguf", "families": ["llama"], "parent_model": "", "parameter_size": "8.0B", "quantization_level": "Q4_K_M"}, "modified_at": "2025-10-25T01:14:38.031573994-06:00"}	{}	t	0	2026-01-16 11:15:18.49	2026-01-16 11:23:55.412	2026-01-16 11:23:55.412
cerebras:zai-glm-4.6	cerebras	zai-glm-4.6	{"id": "zai-glm-4.6", "object": "model", "created": 0, "owned_by": "Cerebras"}	{}	f	0	2026-01-16 11:15:18.402	2026-01-16 11:19:02.439	2026-01-16 11:23:55.425
nvidia:upstage/solar-10.7b-instruct	nvidia	upstage/solar-10.7b-instruct	{"id": "upstage/solar-10.7b-instruct", "object": "model", "created": 735790403, "owned_by": "upstage"}	{}	t	0	2026-01-16 11:15:17.727	2026-01-16 11:23:54.241	2026-01-16 11:23:54.242
nvidia:utter-project/eurollm-9b-instruct	nvidia	utter-project/eurollm-9b-instruct	{"id": "utter-project/eurollm-9b-instruct", "object": "model", "created": 735790403, "owned_by": "utter-project"}	{}	t	0	2026-01-16 11:15:17.739	2026-01-16 11:23:54.248	2026-01-16 11:23:54.249
nvidia:writer/palmyra-med-70b	nvidia	writer/palmyra-med-70b	{"id": "writer/palmyra-med-70b", "object": "model", "created": 735790403, "owned_by": "writer"}	{}	t	0	2026-01-16 11:15:17.775	2026-01-16 11:23:54.267	2026-01-16 11:23:54.268
nvidia:writer/palmyra-med-70b-32k	nvidia	writer/palmyra-med-70b-32k	{"id": "writer/palmyra-med-70b-32k", "object": "model", "created": 735790403, "owned_by": "writer"}	{}	t	0	2026-01-16 11:15:17.788	2026-01-16 11:23:54.273	2026-01-16 11:23:54.274
nvidia:zyphra/zamba2-7b-instruct	nvidia	zyphra/zamba2-7b-instruct	{"id": "zyphra/zamba2-7b-instruct", "object": "model", "created": 735790403, "owned_by": "zyphra"}	{}	t	0	2026-01-16 11:15:17.824	2026-01-16 11:23:54.291	2026-01-16 11:23:54.292
ollama-local:granite4:micro	ollama-local	granite4:micro	{"name": "granite4:micro", "size": 2099521385, "model": "granite4:micro", "digest": "89962fcc75239ac434cdebceb6b7e0669397f92eaef9c487774b718bc36a3e5f", "details": {"family": "granite", "format": "gguf", "families": ["granite"], "parent_model": "", "parameter_size": "3.4B", "quantization_level": "Q4_K_M"}, "modified_at": "2025-10-31T03:08:40.326521572-06:00"}	{}	t	0	2026-01-16 11:15:18.477	2026-01-16 11:23:55.397	2026-01-16 11:23:55.398
nvidia:writer/palmyra-creative-122b	nvidia	writer/palmyra-creative-122b	{"id": "writer/palmyra-creative-122b", "object": "model", "created": 735790403, "owned_by": "writer"}	{}	t	0	2026-01-16 11:15:17.751	2026-01-16 11:23:54.254	2026-01-16 11:23:54.255
cerebras:gpt-oss-120b	cerebras	gpt-oss-120b	{"id": "gpt-oss-120b", "object": "model", "created": 0, "owned_by": "Cerebras"}	{}	f	0	2026-01-16 11:15:18.389	2026-01-16 11:19:02.452	2026-01-16 11:23:55.425
nvidia:writer/palmyra-fin-70b-32k	nvidia	writer/palmyra-fin-70b-32k	{"id": "writer/palmyra-fin-70b-32k", "object": "model", "created": 735790403, "owned_by": "writer"}	{}	t	0	2026-01-16 11:15:17.763	2026-01-16 11:23:54.261	2026-01-16 11:23:54.261
nvidia:yentinglin/llama-3-taiwan-70b-instruct	nvidia	yentinglin/llama-3-taiwan-70b-instruct	{"id": "yentinglin/llama-3-taiwan-70b-instruct", "object": "model", "created": 735790403, "owned_by": "yentinglin"}	{}	t	0	2026-01-16 11:15:17.8	2026-01-16 11:23:54.279	2026-01-16 11:23:54.28
cerebras:llama-3.3-70b	cerebras	llama-3.3-70b	{"id": "llama-3.3-70b", "object": "model", "created": 0, "owned_by": "Cerebras"}	{}	f	0	2026-01-16 11:15:18.377	2026-01-16 11:19:02.427	2026-01-16 11:23:55.425
cerebras:zai-glm-4.7	cerebras	zai-glm-4.7	{"id": "zai-glm-4.7", "object": "model", "created": 0, "owned_by": "Cerebras"}	{}	f	0	2026-01-16 11:15:18.353	2026-01-16 11:19:02.458	2026-01-16 11:23:55.425
mistral:voxtral-mini-2507	mistral	voxtral-mini-2507	{"id": "voxtral-mini-2507", "name": "voxtral-mini-transcribe-2507", "type": "base", "object": "model", "aliases": ["voxtral-mini-transcribe-2507", "voxtral-mini-latest"], "created": 1768562337, "owned_by": "mistralai", "deprecation": null, "description": "A mini transcription model released in July 2025", "capabilities": {"ocr": false, "audio": false, "vision": false, "moderation": false, "fine_tuning": false, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": false}, "max_context_length": 16384, "default_model_temperature": 0, "deprecation_replacement_model": null}	{}	t	0	2026-01-16 11:15:13.557	2026-01-16 11:18:58.774	2026-01-16 11:25:16.912
mistral:voxtral-small-latest	mistral	voxtral-small-latest	{"id": "voxtral-small-latest", "name": "voxtral-small-2507", "type": "base", "object": "model", "aliases": ["voxtral-small-2507"], "created": 1768562337, "owned_by": "mistralai", "deprecation": null, "description": "A small audio understanding model released in July 2025", "capabilities": {"ocr": false, "audio": true, "vision": false, "moderation": false, "fine_tuning": false, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": true}, "max_context_length": 32768, "default_model_temperature": 0.2, "deprecation_replacement_model": null}	{}	t	0	2026-01-16 11:15:13.593	2026-01-16 11:18:58.277	2026-01-16 11:25:16.912
mistral:mistral-embed-2312	mistral	mistral-embed-2312	{"id": "mistral-embed-2312", "name": "mistral-embed-2312", "type": "base", "object": "model", "aliases": ["mistral-embed"], "created": 1768562337, "owned_by": "mistralai", "deprecation": null, "description": "Official mistral-embed-2312 Mistral AI model", "capabilities": {"ocr": false, "audio": false, "vision": false, "moderation": false, "fine_tuning": false, "classification": false, "completion_fim": false, "completion_chat": false, "function_calling": false}, "max_context_length": 8192, "default_model_temperature": null, "deprecation_replacement_model": null}	{}	f	0	2026-01-16 11:15:13.853	2026-01-16 11:18:58.623	2026-01-16 11:23:55.425
mistral:ministral-14b-latest	mistral	ministral-14b-latest	{"id": "ministral-14b-latest", "name": "ministral-14b-2512", "type": "base", "object": "model", "aliases": ["ministral-14b-2512"], "created": 1768562337, "owned_by": "mistralai", "deprecation": null, "description": "Ministral 3 (a.k.a. Tinystral) 14B Instruct.", "capabilities": {"ocr": false, "audio": false, "vision": true, "moderation": false, "fine_tuning": true, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": true}, "max_context_length": 262144, "default_model_temperature": 0.3, "deprecation_replacement_model": null}	{}	f	0	2026-01-16 11:15:13.687	2026-01-16 11:18:58.391	2026-01-16 11:23:55.425
mistral:mistral-tiny	mistral	mistral-tiny	{"id": "mistral-tiny", "name": "open-mistral-7b", "type": "base", "object": "model", "aliases": ["open-mistral-7b", "mistral-tiny-2312"], "created": 1768562337, "owned_by": "mistralai", "deprecation": "2026-01-31T12:00:00Z", "description": "Our first dense model released September 2023.", "capabilities": {"ocr": false, "audio": false, "vision": false, "moderation": false, "fine_tuning": true, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": true}, "max_context_length": 32768, "default_model_temperature": 0.7, "deprecation_replacement_model": "ministral-8b-latest"}	{}	f	0	2026-01-16 11:15:13.711	2026-01-16 11:18:58.44	2026-01-16 11:23:55.425
mistral:mistral-large-latest	mistral	mistral-large-latest	{"id": "mistral-large-latest", "name": "mistral-large-2512", "type": "base", "object": "model", "aliases": [], "created": 1768562337, "owned_by": "mistralai", "deprecation": null, "description": "Official mistral-large-2512 Mistral AI model", "capabilities": {"ocr": false, "audio": false, "vision": true, "moderation": false, "fine_tuning": true, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": true}, "max_context_length": 262144, "default_model_temperature": 0.3, "deprecation_replacement_model": null}	{}	f	0	2026-01-16 11:15:13.616	2026-01-16 11:18:58.309	2026-01-16 11:23:55.425
mistral:pixtral-large-2411	mistral	pixtral-large-2411	{"id": "pixtral-large-2411", "name": "pixtral-large-2411", "type": "base", "object": "model", "aliases": ["pixtral-large-latest", "mistral-large-pixtral-2411"], "created": 1768562337, "owned_by": "mistralai", "deprecation": null, "description": "Official pixtral-large-2411 Mistral AI model", "capabilities": {"ocr": false, "audio": false, "vision": true, "moderation": false, "fine_tuning": false, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": true}, "max_context_length": 131072, "default_model_temperature": 0.7, "deprecation_replacement_model": null}	{}	f	0	2026-01-16 11:15:13.307	2026-01-16 11:18:57.946	2026-01-16 11:23:55.425
openrouter:google/gemma-3n-e2b-it:free	openrouter	google/gemma-3n-e2b-it:free	{"id": "google/gemma-3n-e2b-it:free", "name": "Google: Gemma 3n 2B (free)", "created": 1752074904, "pricing": {"image": "0", "prompt": "0", "request": "0", "completion": "0", "web_search": "0", "internal_reasoning": "0"}, "description": "Gemma 3n E2B IT is a multimodal, instruction-tuned model developed by Google DeepMind, designed to operate efficiently at an effective parameter size of 2B while leveraging a 6B architecture. Based on the MatFormer architecture, it supports nested submodels and modular composition via the Mix-and-Match framework. Gemma 3n models are optimized for low-resource deployment, offering 32K context length and strong multilingual and reasoning performance across common benchmarks. This variant is trained on a diverse corpus including code, math, web, and multimodal data.", "architecture": {"modality": "text->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}, "top_provider": {"is_moderated": false, "context_length": 8192, "max_completion_tokens": 2048}, "canonical_slug": "google/gemma-3n-e2b-it", "context_length": 8192, "expiration_date": null, "hugging_face_id": "google/gemma-3n-E2B-it", "default_parameters": {}, "per_request_limits": null, "supported_parameters": ["frequency_penalty", "max_tokens", "presence_penalty", "response_format", "seed", "stop", "temperature", "top_p"]}	{}	f	0	2026-01-16 11:15:14.52	2026-01-16 11:18:56.136	2026-01-16 11:19:02.539
mistral:mistral-medium-latest	mistral	mistral-medium-latest	{"id": "mistral-medium-latest", "name": "mistral-medium-2508", "type": "base", "object": "model", "aliases": ["mistral-medium-2508", "mistral-medium"], "created": 1768562337, "owned_by": "mistralai", "deprecation": null, "description": "Update on Mistral Medium 3 with improved capabilities.", "capabilities": {"ocr": false, "audio": false, "vision": true, "moderation": false, "fine_tuning": true, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": true}, "max_context_length": 131072, "default_model_temperature": 0.3, "deprecation_replacement_model": null}	{}	f	0	2026-01-16 11:15:13.22	2026-01-16 11:18:57.845	2026-01-16 11:23:55.425
mistral:mistral-medium	mistral	mistral-medium	{"id": "mistral-medium", "name": "mistral-medium-2508", "type": "base", "object": "model", "aliases": ["mistral-medium-2508", "mistral-medium-latest"], "created": 1768562337, "owned_by": "mistralai", "deprecation": null, "description": "Update on Mistral Medium 3 with improved capabilities.", "capabilities": {"ocr": false, "audio": false, "vision": true, "moderation": false, "fine_tuning": true, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": true}, "max_context_length": 131072, "default_model_temperature": 0.3, "deprecation_replacement_model": null}	{}	f	0	2026-01-16 11:15:13.233	2026-01-16 11:18:57.859	2026-01-16 11:23:55.425
mistral:open-mistral-nemo	mistral	open-mistral-nemo	{"id": "open-mistral-nemo", "name": "open-mistral-nemo", "type": "base", "object": "model", "aliases": ["open-mistral-nemo-2407", "mistral-tiny-2407", "mistral-tiny-latest"], "created": 1768562337, "owned_by": "mistralai", "deprecation": null, "description": "Our best multilingual open source model released July 2024.", "capabilities": {"ocr": false, "audio": false, "vision": false, "moderation": false, "fine_tuning": true, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": true}, "max_context_length": 131072, "default_model_temperature": 0.3, "deprecation_replacement_model": null}	{}	f	0	2026-01-16 11:15:13.245	2026-01-16 11:18:57.875	2026-01-16 11:23:55.425
mistral:devstral-medium-2507	mistral	devstral-medium-2507	{"id": "devstral-medium-2507", "name": "devstral-medium-2507", "type": "base", "object": "model", "aliases": [], "created": 1768562337, "owned_by": "mistralai", "deprecation": null, "description": "Our medium code-agentic model.", "capabilities": {"ocr": false, "audio": false, "vision": false, "moderation": false, "fine_tuning": false, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": true}, "max_context_length": 131072, "default_model_temperature": 0, "deprecation_replacement_model": null}	{}	f	0	2026-01-16 11:15:13.393	2026-01-16 11:18:58.037	2026-01-16 11:23:55.425
mistral:devstral-2512	mistral	devstral-2512	{"id": "devstral-2512", "name": "devstral-2512", "type": "base", "object": "model", "aliases": [], "created": 1768562337, "owned_by": "mistralai", "deprecation": null, "description": "Official devstral-2512 Mistral AI model", "capabilities": {"ocr": false, "audio": false, "vision": false, "moderation": false, "fine_tuning": false, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": true}, "max_context_length": 262144, "default_model_temperature": 0.2, "deprecation_replacement_model": null}	{}	f	0	2026-01-16 11:15:13.404	2026-01-16 11:18:58.05	2026-01-16 11:23:55.425
mistral:mistral-vibe-cli-latest	mistral	mistral-vibe-cli-latest	{"id": "mistral-vibe-cli-latest", "name": "devstral-2512", "type": "base", "object": "model", "aliases": [], "created": 1768562337, "owned_by": "mistralai", "deprecation": null, "description": "Official devstral-2512 Mistral AI model", "capabilities": {"ocr": false, "audio": false, "vision": false, "moderation": false, "fine_tuning": false, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": true}, "max_context_length": 262144, "default_model_temperature": 0.2, "deprecation_replacement_model": null}	{}	f	0	2026-01-16 11:15:13.415	2026-01-16 11:18:58.063	2026-01-16 11:23:55.425
mistral:devstral-medium-latest	mistral	devstral-medium-latest	{"id": "devstral-medium-latest", "name": "devstral-2512", "type": "base", "object": "model", "aliases": [], "created": 1768562337, "owned_by": "mistralai", "deprecation": null, "description": "Official devstral-2512 Mistral AI model", "capabilities": {"ocr": false, "audio": false, "vision": false, "moderation": false, "fine_tuning": false, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": true}, "max_context_length": 262144, "default_model_temperature": 0.2, "deprecation_replacement_model": null}	{}	f	0	2026-01-16 11:15:13.426	2026-01-16 11:18:58.077	2026-01-16 11:23:55.425
mistral:devstral-latest	mistral	devstral-latest	{"id": "devstral-latest", "name": "devstral-2512", "type": "base", "object": "model", "aliases": [], "created": 1768562337, "owned_by": "mistralai", "deprecation": null, "description": "Official devstral-2512 Mistral AI model", "capabilities": {"ocr": false, "audio": false, "vision": false, "moderation": false, "fine_tuning": false, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": true}, "max_context_length": 262144, "default_model_temperature": 0.2, "deprecation_replacement_model": null}	{}	f	0	2026-01-16 11:15:13.438	2026-01-16 11:18:58.09	2026-01-16 11:23:55.425
mistral:labs-devstral-small-2512	mistral	labs-devstral-small-2512	{"id": "labs-devstral-small-2512", "name": "labs-devstral-small-2512", "type": "base", "object": "model", "aliases": [], "created": 1768562337, "owned_by": "mistralai", "deprecation": null, "description": "Official labs-devstral-small-2512 Mistral AI model", "capabilities": {"ocr": false, "audio": false, "vision": true, "moderation": false, "fine_tuning": false, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": true}, "max_context_length": 262144, "default_model_temperature": 0.2, "deprecation_replacement_model": null}	{}	f	0	2026-01-16 11:15:13.449	2026-01-16 11:18:58.104	2026-01-16 11:23:55.425
mistral:mistral-large-pixtral-2411	mistral	mistral-large-pixtral-2411	{"id": "mistral-large-pixtral-2411", "name": "pixtral-large-2411", "type": "base", "object": "model", "aliases": ["pixtral-large-2411", "pixtral-large-latest"], "created": 1768562337, "owned_by": "mistralai", "deprecation": null, "description": "Official pixtral-large-2411 Mistral AI model", "capabilities": {"ocr": false, "audio": false, "vision": true, "moderation": false, "fine_tuning": false, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": true}, "max_context_length": 131072, "default_model_temperature": 0.7, "deprecation_replacement_model": null}	{}	f	0	2026-01-16 11:15:13.341	2026-01-16 11:18:57.98	2026-01-16 11:23:55.425
mistral:codestral-2508	mistral	codestral-2508	{"id": "codestral-2508", "name": "codestral-2508", "type": "base", "object": "model", "aliases": ["codestral-latest"], "created": 1768562337, "owned_by": "mistralai", "deprecation": null, "description": "Our cutting-edge language model for coding released August 2025.", "capabilities": {"ocr": false, "audio": false, "vision": false, "moderation": false, "fine_tuning": false, "classification": false, "completion_fim": true, "completion_chat": true, "function_calling": true}, "max_context_length": 256000, "default_model_temperature": 0.3, "deprecation_replacement_model": null}	{}	f	0	2026-01-16 11:15:13.358	2026-01-16 11:18:57.996	2026-01-16 11:23:55.425
mistral:codestral-latest	mistral	codestral-latest	{"id": "codestral-latest", "name": "codestral-2508", "type": "base", "object": "model", "aliases": ["codestral-2508"], "created": 1768562337, "owned_by": "mistralai", "deprecation": null, "description": "Our cutting-edge language model for coding released August 2025.", "capabilities": {"ocr": false, "audio": false, "vision": false, "moderation": false, "fine_tuning": false, "classification": false, "completion_fim": true, "completion_chat": true, "function_calling": true}, "max_context_length": 256000, "default_model_temperature": 0.3, "deprecation_replacement_model": null}	{}	f	0	2026-01-16 11:15:13.369	2026-01-16 11:18:58.009	2026-01-16 11:23:55.425
mistral:magistral-medium-2509	mistral	magistral-medium-2509	{"id": "magistral-medium-2509", "name": "magistral-medium-2509", "type": "base", "object": "model", "aliases": ["magistral-medium-latest"], "created": 1768562337, "owned_by": "mistralai", "deprecation": null, "description": "Our frontier-class reasoning model release candidate September 2025.", "capabilities": {"ocr": false, "audio": false, "vision": true, "moderation": false, "fine_tuning": true, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": true}, "max_context_length": 131072, "default_model_temperature": 0.7, "deprecation_replacement_model": null}	{}	f	0	2026-01-16 11:15:13.507	2026-01-16 11:18:58.172	2026-01-16 11:23:55.425
mistral:magistral-medium-latest	mistral	magistral-medium-latest	{"id": "magistral-medium-latest", "name": "magistral-medium-2509", "type": "base", "object": "model", "aliases": ["magistral-medium-2509"], "created": 1768562337, "owned_by": "mistralai", "deprecation": null, "description": "Our frontier-class reasoning model release candidate September 2025.", "capabilities": {"ocr": false, "audio": false, "vision": true, "moderation": false, "fine_tuning": true, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": true}, "max_context_length": 131072, "default_model_temperature": 0.7, "deprecation_replacement_model": null}	{}	f	0	2026-01-16 11:15:13.518	2026-01-16 11:18:58.185	2026-01-16 11:23:55.425
groq:whisper-large-v3	groq	whisper-large-v3	{"id": "whisper-large-v3", "active": true, "object": "model", "created": 1693721698, "owned_by": "OpenAI", "public_apps": null, "context_window": 448, "max_completion_tokens": 448}	{}	t	0	2026-01-16 11:15:15.043	2026-01-16 11:19:00.106	2026-01-16 11:25:16.912
groq:canopylabs/orpheus-v1-english	groq	canopylabs/orpheus-v1-english	{"id": "canopylabs/orpheus-v1-english", "active": true, "object": "model", "created": 1766186316, "owned_by": "Canopy Labs", "public_apps": null, "context_window": 4000, "max_completion_tokens": 50000}	{}	t	0	2026-01-16 11:15:15.146	2026-01-16 11:18:59.886	2026-01-16 11:25:16.912
nvidia:tiiuae/falcon3-7b-instruct	nvidia	tiiuae/falcon3-7b-instruct	{"id": "tiiuae/falcon3-7b-instruct", "object": "model", "created": 735790403, "owned_by": "tiiuae"}	{}	t	0	2026-01-16 11:15:17.703	2026-01-16 11:23:54.228	2026-01-16 11:23:54.229
nvidia:deepseek-ai/deepseek-r1-0528	nvidia	deepseek-ai/deepseek-r1-0528	{"id": "deepseek-ai/deepseek-r1-0528", "object": "model", "created": 735790403, "owned_by": "deepseek-ai"}	{}	t	0	2026-01-16 11:15:15.663	2026-01-16 11:23:53.206	2026-01-16 11:23:53.207
nvidia:igenius/colosseum_355b_instruct_16k	nvidia	igenius/colosseum_355b_instruct_16k	{"id": "igenius/colosseum_355b_instruct_16k", "object": "model", "created": 735790403, "owned_by": "igenius"}	{}	t	0	2026-01-16 11:15:16.073	2026-01-16 11:23:53.407	2026-01-16 11:23:53.408
nvidia:microsoft/phi-3.5-moe-instruct	nvidia	microsoft/phi-3.5-moe-instruct	{"id": "microsoft/phi-3.5-moe-instruct", "object": "model", "created": 735790403, "owned_by": "microsoft"}	{}	t	0	2026-01-16 11:15:16.474	2026-01-16 11:23:53.603	2026-01-16 11:23:53.604
nvidia:tokyotech-llm/llama-3-swallow-70b-instruct-v0.1	nvidia	tokyotech-llm/llama-3-swallow-70b-instruct-v0.1	{"id": "tokyotech-llm/llama-3-swallow-70b-instruct-v0.1", "object": "model", "created": 735790403, "owned_by": "tokyotech-llm"}	{}	t	0	2026-01-16 11:15:17.715	2026-01-16 11:23:54.235	2026-01-16 11:23:54.236
ollama-local:mxbai-embed-large:latest	ollama-local	mxbai-embed-large:latest	{"name": "mxbai-embed-large:latest", "size": 669615493, "model": "mxbai-embed-large:latest", "digest": "468836162de7f81e041c43663fedbbba921dcea9b9fefea135685a39b2d83dd8", "details": {"family": "bert", "format": "gguf", "families": ["bert"], "parent_model": "", "parameter_size": "334M", "quantization_level": "F16"}, "modified_at": "2025-12-02T05:03:52.164917351-07:00"}	{}	t	0	2026-01-16 11:15:18.464	2026-01-16 11:23:55.385	2026-01-16 11:23:55.386
groq:meta-llama/llama-prompt-guard-2-86m	groq	meta-llama/llama-prompt-guard-2-86m	{"id": "meta-llama/llama-prompt-guard-2-86m", "active": true, "object": "model", "created": 1748632165, "owned_by": "Meta", "public_apps": null, "context_window": 512, "max_completion_tokens": 512}	{}	f	0	2026-01-16 11:15:15.171	2026-01-16 11:18:59.914	2026-01-16 11:23:55.425
groq:meta-llama/llama-4-maverick-17b-128e-instruct	groq	meta-llama/llama-4-maverick-17b-128e-instruct	{"id": "meta-llama/llama-4-maverick-17b-128e-instruct", "active": true, "object": "model", "created": 1743877158, "owned_by": "Meta", "public_apps": null, "context_window": 131072, "max_completion_tokens": 8192}	{}	f	0	2026-01-16 11:15:14.962	2026-01-16 11:19:00.116	2026-01-16 11:23:55.425
groq:groq/compound-mini	groq	groq/compound-mini	{"id": "groq/compound-mini", "active": true, "object": "model", "created": 1756949707, "owned_by": "Groq", "public_apps": null, "context_window": 131072, "max_completion_tokens": 8192}	{}	f	0	2026-01-16 11:15:15.002	2026-01-16 11:18:59.942	2026-01-16 11:23:55.425
groq:llama-3.1-8b-instant	groq	llama-3.1-8b-instant	{"id": "llama-3.1-8b-instant", "active": true, "object": "model", "created": 1693721698, "owned_by": "Meta", "public_apps": null, "context_window": 131072, "max_completion_tokens": 131072}	{}	f	0	2026-01-16 11:15:15.159	2026-01-16 11:18:59.982	2026-01-16 11:23:55.425
mistral:mistral-small-2506	mistral	mistral-small-2506	{"id": "mistral-small-2506", "name": "mistral-small-2506", "type": "base", "object": "model", "aliases": ["mistral-small-latest"], "created": 1768562337, "owned_by": "mistralai", "deprecation": null, "description": "Our latest enterprise-grade small model with the latest version released June 2025.", "capabilities": {"ocr": false, "audio": false, "vision": true, "moderation": false, "fine_tuning": false, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": true}, "max_context_length": 131072, "default_model_temperature": 0.3, "deprecation_replacement_model": null}	{}	f	0	2026-01-16 11:15:13.472	2026-01-16 11:18:58.131	2026-01-16 11:23:55.425
groq:llama-3.3-70b-versatile	groq	llama-3.3-70b-versatile	{"id": "llama-3.3-70b-versatile", "active": true, "object": "model", "created": 1733447754, "owned_by": "Meta", "public_apps": null, "context_window": 131072, "max_completion_tokens": 32768}	{}	f	0	2026-01-16 11:15:14.974	2026-01-16 11:19:00.094	2026-01-16 11:23:55.425
nvidia:mistralai/mistral-large-3-675b-instruct-2512	nvidia	mistralai/mistral-large-3-675b-instruct-2512	{"id": "mistralai/mistral-large-3-675b-instruct-2512", "object": "model", "created": 735790403, "owned_by": "mistralai"}	{}	t	0	2026-01-16 11:15:16.679	2026-01-16 11:23:53.715	2026-01-16 11:23:53.716
nvidia:nvidia/llama-3.1-nemoguard-8b-content-safety	nvidia	nvidia/llama-3.1-nemoguard-8b-content-safety	{"id": "nvidia/llama-3.1-nemoguard-8b-content-safety", "object": "model", "created": 735790403, "owned_by": "nvidia"}	{}	t	0	2026-01-16 11:15:16.849	2026-01-16 11:23:53.808	2026-01-16 11:23:53.809
nvidia:nvidia/nemotron-4-mini-hindi-4b-instruct	nvidia	nvidia/nemotron-4-mini-hindi-4b-instruct	{"id": "nvidia/nemotron-4-mini-hindi-4b-instruct", "object": "model", "created": 735790403, "owned_by": "nvidia"}	{}	t	0	2026-01-16 11:15:17.183	2026-01-16 11:23:53.972	2026-01-16 11:23:53.973
nvidia:nvidia/nemotron-parse	nvidia	nvidia/nemotron-parse	{"id": "nvidia/nemotron-parse", "object": "model", "created": 735790403, "owned_by": "nvidia"}	{}	t	0	2026-01-16 11:15:17.237	2026-01-16 11:23:53.997	2026-01-16 11:23:53.998
groq:openai/gpt-oss-20b	groq	openai/gpt-oss-20b	{"id": "openai/gpt-oss-20b", "active": true, "object": "model", "created": 1754407957, "owned_by": "OpenAI", "public_apps": null, "context_window": 131072, "max_completion_tokens": 65536}	{}	f	0	2026-01-16 11:15:15.108	2026-01-16 11:18:59.994	2026-01-16 11:23:55.425
groq:meta-llama/llama-prompt-guard-2-22m	groq	meta-llama/llama-prompt-guard-2-22m	{"id": "meta-llama/llama-prompt-guard-2-22m", "active": true, "object": "model", "created": 1748632101, "owned_by": "Meta", "public_apps": null, "context_window": 512, "max_completion_tokens": 512}	{}	f	0	2026-01-16 11:15:15.091	2026-01-16 11:18:59.955	2026-01-16 11:23:55.425
mistral:voxtral-small-2507	mistral	voxtral-small-2507	{"id": "voxtral-small-2507", "name": "voxtral-small-2507", "type": "base", "object": "model", "aliases": ["voxtral-small-latest"], "created": 1768562337, "owned_by": "mistralai", "deprecation": null, "description": "A small audio understanding model released in July 2025", "capabilities": {"ocr": false, "audio": true, "vision": false, "moderation": false, "fine_tuning": false, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": true}, "max_context_length": 32768, "default_model_temperature": 0.2, "deprecation_replacement_model": null}	{}	t	0	2026-01-16 11:15:13.581	2026-01-16 11:18:58.261	2026-01-16 11:25:16.912
mistral:magistral-small-2509	mistral	magistral-small-2509	{"id": "magistral-small-2509", "name": "magistral-small-2509", "type": "base", "object": "model", "aliases": ["magistral-small-latest"], "created": 1768562337, "owned_by": "mistralai", "deprecation": null, "description": "Our efficient reasoning model released September 2025.", "capabilities": {"ocr": false, "audio": false, "vision": true, "moderation": false, "fine_tuning": true, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": true}, "max_context_length": 131072, "default_model_temperature": 0.7, "deprecation_replacement_model": null}	{}	f	0	2026-01-16 11:15:13.53	2026-01-16 11:18:58.199	2026-01-16 11:23:55.425
mistral:magistral-small-latest	mistral	magistral-small-latest	{"id": "magistral-small-latest", "name": "magistral-small-2509", "type": "base", "object": "model", "aliases": ["magistral-small-2509"], "created": 1768562337, "owned_by": "mistralai", "deprecation": null, "description": "Our efficient reasoning model released September 2025.", "capabilities": {"ocr": false, "audio": false, "vision": true, "moderation": false, "fine_tuning": true, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": true}, "max_context_length": 131072, "default_model_temperature": 0.7, "deprecation_replacement_model": null}	{}	f	0	2026-01-16 11:15:13.542	2026-01-16 11:18:58.213	2026-01-16 11:23:55.425
mistral:devstral-small-latest	mistral	devstral-small-latest	{"id": "devstral-small-latest", "name": "labs-devstral-small-2512", "type": "base", "object": "model", "aliases": [], "created": 1768562337, "owned_by": "mistralai", "deprecation": null, "description": "Official labs-devstral-small-2512 Mistral AI model", "capabilities": {"ocr": false, "audio": false, "vision": true, "moderation": false, "fine_tuning": false, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": true}, "max_context_length": 262144, "default_model_temperature": 0.2, "deprecation_replacement_model": null}	{}	f	0	2026-01-16 11:15:13.46	2026-01-16 11:18:58.117	2026-01-16 11:23:55.425
mistral:mistral-small-latest	mistral	mistral-small-latest	{"id": "mistral-small-latest", "name": "mistral-small-2506", "type": "base", "object": "model", "aliases": ["mistral-small-2506"], "created": 1768562337, "owned_by": "mistralai", "deprecation": null, "description": "Our latest enterprise-grade small model with the latest version released June 2025.", "capabilities": {"ocr": false, "audio": false, "vision": true, "moderation": false, "fine_tuning": false, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": true}, "max_context_length": 131072, "default_model_temperature": 0.3, "deprecation_replacement_model": null}	{}	f	0	2026-01-16 11:15:13.483	2026-01-16 11:18:58.145	2026-01-16 11:23:55.425
mistral:labs-mistral-small-creative	mistral	labs-mistral-small-creative	{"id": "labs-mistral-small-creative", "name": "labs-mistral-small-creative", "type": "base", "object": "model", "aliases": [], "created": 1768562337, "owned_by": "mistralai", "deprecation": null, "description": "Official labs-mistral-small-creative Mistral AI model", "capabilities": {"ocr": false, "audio": false, "vision": false, "moderation": false, "fine_tuning": true, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": true}, "max_context_length": 32768, "default_model_temperature": 0.3, "deprecation_replacement_model": null}	{}	f	0	2026-01-16 11:15:13.495	2026-01-16 11:18:58.158	2026-01-16 11:23:55.425
cerebras:qwen-3-32b	cerebras	qwen-3-32b	{"id": "qwen-3-32b", "object": "model", "created": 0, "owned_by": "Cerebras"}	{}	f	0	2026-01-16 11:15:18.341	2026-01-16 11:19:02.446	2026-01-16 11:23:55.425
groq:meta-llama/llama-guard-4-12b	groq	meta-llama/llama-guard-4-12b	{"id": "meta-llama/llama-guard-4-12b", "active": true, "object": "model", "created": 1746743847, "owned_by": "Meta", "public_apps": null, "context_window": 131072, "max_completion_tokens": 1024}	{}	f	0	2026-01-16 11:15:14.985	2026-01-16 11:19:00.039	2026-01-16 11:23:55.425
mistral:mistral-medium-2508	mistral	mistral-medium-2508	{"id": "mistral-medium-2508", "name": "mistral-medium-2508", "type": "base", "object": "model", "aliases": ["mistral-medium-latest", "mistral-medium"], "created": 1768562337, "owned_by": "mistralai", "deprecation": null, "description": "Update on Mistral Medium 3 with improved capabilities.", "capabilities": {"ocr": false, "audio": false, "vision": true, "moderation": false, "fine_tuning": true, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": true}, "max_context_length": 131072, "default_model_temperature": 0.3, "deprecation_replacement_model": null}	{}	f	0	2026-01-16 11:15:13.208	2026-01-16 11:18:57.83	2026-01-16 11:23:55.425
mistral:pixtral-large-latest	mistral	pixtral-large-latest	{"id": "pixtral-large-latest", "name": "pixtral-large-2411", "type": "base", "object": "model", "aliases": ["pixtral-large-2411", "mistral-large-pixtral-2411"], "created": 1768562337, "owned_by": "mistralai", "deprecation": null, "description": "Official pixtral-large-2411 Mistral AI model", "capabilities": {"ocr": false, "audio": false, "vision": true, "moderation": false, "fine_tuning": false, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": true}, "max_context_length": 131072, "default_model_temperature": 0.7, "deprecation_replacement_model": null}	{}	f	0	2026-01-16 11:15:13.324	2026-01-16 11:18:57.962	2026-01-16 11:23:55.425
cerebras:qwen-3-235b-a22b-instruct-2507	cerebras	qwen-3-235b-a22b-instruct-2507	{"id": "qwen-3-235b-a22b-instruct-2507", "object": "model", "created": 0, "owned_by": "Cerebras"}	{}	f	0	2026-01-16 11:15:18.364	2026-01-16 11:19:02.464	2026-01-16 11:23:55.425
openrouter:openai/gpt-oss-120b:free	openrouter	openai/gpt-oss-120b:free	{"id": "openai/gpt-oss-120b:free", "name": "OpenAI: gpt-oss-120b (free)", "created": 1754414231, "pricing": {"image": "0", "prompt": "0", "request": "0", "completion": "0", "web_search": "0", "internal_reasoning": "0"}, "description": "gpt-oss-120b is an open-weight, 117B-parameter Mixture-of-Experts (MoE) language model from OpenAI designed for high-reasoning, agentic, and general-purpose production use cases. It activates 5.1B parameters per forward pass and is optimized to run on a single H100 GPU with native MXFP4 quantization. The model supports configurable reasoning depth, full chain-of-thought access, and native tool use, including function calling, browsing, and structured output generation.", "architecture": {"modality": "text->text", "tokenizer": "GPT", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}, "top_provider": {"is_moderated": true, "context_length": 131072, "max_completion_tokens": null}, "canonical_slug": "openai/gpt-oss-120b", "context_length": 131072, "expiration_date": null, "hugging_face_id": "openai/gpt-oss-120b", "default_parameters": {"top_p": null, "temperature": null, "frequency_penalty": null}, "per_request_limits": null, "supported_parameters": ["include_reasoning", "max_tokens", "reasoning", "seed", "stop", "temperature", "tool_choice", "tools"]}	{}	f	0	2026-01-16 11:15:14.446	2026-01-16 11:18:56.054	2026-01-16 11:19:02.539
nvidia:mistralai/magistral-small-2506	nvidia	mistralai/magistral-small-2506	{"id": "mistralai/magistral-small-2506", "object": "model", "created": 735790403, "owned_by": "mistralai"}	{}	t	0	2026-01-16 11:15:16.584	2026-01-16 11:23:53.663	2026-01-16 11:23:53.664
nvidia:mistralai/mistral-7b-instruct-v0.3	nvidia	mistralai/mistral-7b-instruct-v0.3	{"id": "mistralai/mistral-7b-instruct-v0.3", "object": "model", "created": 735790403, "owned_by": "mistralai"}	{}	t	0	2026-01-16 11:15:16.644	2026-01-16 11:23:53.695	2026-01-16 11:23:53.696
nvidia:mistralai/mixtral-8x7b-instruct-v0.1	nvidia	mistralai/mixtral-8x7b-instruct-v0.1	{"id": "mistralai/mixtral-8x7b-instruct-v0.1", "object": "model", "created": 735790403, "owned_by": "mistralai"}	{}	t	0	2026-01-16 11:15:16.763	2026-01-16 11:23:53.763	2026-01-16 11:23:53.764
openrouter:moonshotai/kimi-k2:free	openrouter	moonshotai/kimi-k2:free	{"id": "moonshotai/kimi-k2:free", "name": "MoonshotAI: Kimi K2 0711 (free)", "created": 1752263252, "pricing": {"image": "0", "prompt": "0", "request": "0", "completion": "0", "web_search": "0", "internal_reasoning": "0"}, "description": "Kimi K2 Instruct is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active per forward pass. It is optimized for agentic capabilities, including advanced tool use, reasoning, and code synthesis. Kimi K2 excels across a broad range of benchmarks, particularly in coding (LiveCodeBench, SWE-bench), reasoning (ZebraLogic, GPQA), and tool-use (Tau2, AceBench) tasks. It supports long-context inference up to 128K tokens and is designed with a novel training stack that includes the MuonClip optimizer for stable large-scale MoE training.", "architecture": {"modality": "text->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}, "top_provider": {"is_moderated": true, "context_length": 32768, "max_completion_tokens": null}, "canonical_slug": "moonshotai/kimi-k2", "context_length": 32768, "expiration_date": null, "hugging_face_id": "moonshotai/Kimi-K2-Instruct", "default_parameters": {}, "per_request_limits": null, "supported_parameters": ["max_tokens", "seed", "stop", "temperature"]}	{}	f	0	2026-01-16 11:15:14.496	2026-01-16 11:18:56.109	2026-01-16 11:19:02.539
openrouter:cognitivecomputations/dolphin-mistral-24b-venice-edition:free	openrouter	cognitivecomputations/dolphin-mistral-24b-venice-edition:free	{"id": "cognitivecomputations/dolphin-mistral-24b-venice-edition:free", "name": "Venice: Uncensored (free)", "created": 1752094966, "pricing": {"image": "0", "prompt": "0", "request": "0", "completion": "0", "web_search": "0", "internal_reasoning": "0"}, "description": "Venice Uncensored Dolphin Mistral 24B Venice Edition is a fine-tuned variant of Mistral-Small-24B-Instruct-2501, developed by dphn.ai in collaboration with Venice.ai. This model is designed as an “uncensored” instruct-tuned LLM, preserving user control over alignment, system prompts, and behavior. Intended for advanced and unrestricted use cases, Venice Uncensored emphasizes steerability and transparent behavior, removing default safety and alignment layers typically found in mainstream assistant models.", "architecture": {"modality": "text->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}, "top_provider": {"is_moderated": false, "context_length": 32768, "max_completion_tokens": null}, "canonical_slug": "venice/uncensored", "context_length": 32768, "expiration_date": null, "hugging_face_id": "cognitivecomputations/Dolphin-Mistral-24B-Venice-Edition", "default_parameters": {}, "per_request_limits": null, "supported_parameters": ["frequency_penalty", "max_tokens", "presence_penalty", "response_format", "stop", "structured_outputs", "temperature", "top_k", "top_p"]}	{}	f	0	2026-01-16 11:15:14.508	2026-01-16 11:18:56.122	2026-01-16 11:19:02.539
openrouter:qwen/qwen3-4b:free	openrouter	qwen/qwen3-4b:free	{"id": "qwen/qwen3-4b:free", "name": "Qwen: Qwen3 4B (free)", "created": 1746031104, "pricing": {"image": "0", "prompt": "0", "request": "0", "completion": "0", "web_search": "0", "internal_reasoning": "0"}, "description": "Qwen3-4B is a 4 billion parameter dense language model from the Qwen3 series, designed to support both general-purpose and reasoning-intensive tasks. It introduces a dual-mode architecture—thinking and non-thinking—allowing dynamic switching between high-precision logical reasoning and efficient dialogue generation. This makes it well-suited for multi-turn chat, instruction following, and complex agent workflows.", "architecture": {"modality": "text->text", "tokenizer": "Qwen3", "instruct_type": "qwen3", "input_modalities": ["text"], "output_modalities": ["text"]}, "top_provider": {"is_moderated": false, "context_length": 40960, "max_completion_tokens": null}, "canonical_slug": "qwen/qwen3-4b-04-28", "context_length": 40960, "expiration_date": null, "hugging_face_id": "Qwen/Qwen3-4B", "default_parameters": {}, "per_request_limits": null, "supported_parameters": ["frequency_penalty", "include_reasoning", "max_tokens", "presence_penalty", "reasoning", "response_format", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_k", "top_p"]}	{}	f	0	2026-01-16 11:15:14.567	2026-01-16 11:18:56.193	2026-01-16 11:19:02.539
openrouter:tngtech/deepseek-r1t-chimera:free	openrouter	tngtech/deepseek-r1t-chimera:free	{"id": "tngtech/deepseek-r1t-chimera:free", "name": "TNG: DeepSeek R1T Chimera (free)", "created": 1745760875, "pricing": {"image": "0", "prompt": "0", "request": "0", "completion": "0", "web_search": "0", "internal_reasoning": "0"}, "description": "DeepSeek-R1T-Chimera is created by merging DeepSeek-R1 and DeepSeek-V3 (0324), combining the reasoning capabilities of R1 with the token efficiency improvements of V3. It is based on a DeepSeek-MoE Transformer architecture and is optimized for general text generation tasks.\\n\\nThe model merges pretrained weights from both source models to balance performance across reasoning, efficiency, and instruction-following tasks. It is released under the MIT license and intended for research and commercial use.", "architecture": {"modality": "text->text", "tokenizer": "DeepSeek", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}, "top_provider": {"is_moderated": false, "context_length": 163840, "max_completion_tokens": null}, "canonical_slug": "tngtech/deepseek-r1t-chimera", "context_length": 163840, "expiration_date": null, "hugging_face_id": "tngtech/DeepSeek-R1T-Chimera", "default_parameters": {}, "per_request_limits": null, "supported_parameters": ["frequency_penalty", "include_reasoning", "max_tokens", "presence_penalty", "reasoning", "repetition_penalty", "seed", "stop", "temperature", "top_k", "top_p"]}	{}	f	0	2026-01-16 11:15:14.579	2026-01-16 11:18:56.209	2026-01-16 11:19:02.539
openrouter:google/gemma-3-27b-it:free	openrouter	google/gemma-3-27b-it:free	{"id": "google/gemma-3-27b-it:free", "name": "Google: Gemma 3 27B (free)", "created": 1741756359, "pricing": {"image": "0", "prompt": "0", "request": "0", "completion": "0", "web_search": "0", "internal_reasoning": "0"}, "description": "Gemma 3 introduces multimodality, supporting vision-language input and text outputs. It handles context windows up to 128k tokens, understands over 140 languages, and offers improved math, reasoning, and chat capabilities, including structured outputs and function calling. Gemma 3 27B is Google's latest open source model, successor to [Gemma 2](google/gemma-2-27b-it)", "architecture": {"modality": "text+image->text", "tokenizer": "Gemini", "instruct_type": "gemma", "input_modalities": ["text", "image"], "output_modalities": ["text"]}, "top_provider": {"is_moderated": false, "context_length": 131072, "max_completion_tokens": null}, "canonical_slug": "google/gemma-3-27b-it", "context_length": 131072, "expiration_date": null, "hugging_face_id": "google/gemma-3-27b-it", "default_parameters": {"top_p": null, "temperature": null, "frequency_penalty": null}, "per_request_limits": null, "supported_parameters": ["frequency_penalty", "max_tokens", "presence_penalty", "repetition_penalty", "response_format", "seed", "stop", "temperature", "tool_choice", "tools", "top_p"]}	{}	f	0	2026-01-16 11:15:14.627	2026-01-16 11:18:56.27	2026-01-16 11:19:02.539
openrouter:google/gemini-2.0-flash-exp:free	openrouter	google/gemini-2.0-flash-exp:free	{"id": "google/gemini-2.0-flash-exp:free", "name": "Google: Gemini 2.0 Flash Experimental (free)", "created": 1733937523, "pricing": {"image": "0", "prompt": "0", "request": "0", "completion": "0", "web_search": "0", "internal_reasoning": "0"}, "description": "Gemini Flash 2.0 offers a significantly faster time to first token (TTFT) compared to [Gemini Flash 1.5](/google/gemini-flash-1.5), while maintaining quality on par with larger models like [Gemini Pro 1.5](/google/gemini-pro-1.5). It introduces notable enhancements in multimodal understanding, coding capabilities, complex instruction following, and function calling. These advancements come together to deliver more seamless and robust agentic experiences.", "architecture": {"modality": "text+image->text", "tokenizer": "Gemini", "instruct_type": null, "input_modalities": ["text", "image"], "output_modalities": ["text"]}, "top_provider": {"is_moderated": false, "context_length": 1048576, "max_completion_tokens": 8192}, "canonical_slug": "google/gemini-2.0-flash-exp", "context_length": 1048576, "expiration_date": "2026-02-06", "hugging_face_id": "", "default_parameters": {}, "per_request_limits": null, "supported_parameters": ["max_tokens", "response_format", "seed", "stop", "temperature", "tool_choice", "tools", "top_p"]}	{}	f	0	2026-01-16 11:15:14.639	2026-01-16 11:18:56.286	2026-01-16 11:19:02.539
openrouter:google/gemma-3-12b-it:free	openrouter	google/gemma-3-12b-it:free	{"id": "google/gemma-3-12b-it:free", "name": "Google: Gemma 3 12B (free)", "created": 1741902625, "pricing": {"image": "0", "prompt": "0", "request": "0", "completion": "0", "web_search": "0", "internal_reasoning": "0"}, "description": "Gemma 3 introduces multimodality, supporting vision-language input and text outputs. It handles context windows up to 128k tokens, understands over 140 languages, and offers improved math, reasoning, and chat capabilities, including structured outputs and function calling. Gemma 3 12B is the second largest in the family of Gemma 3 models after [Gemma 3 27B](google/gemma-3-27b-it)", "architecture": {"modality": "text+image->text", "tokenizer": "Gemini", "instruct_type": "gemma", "input_modalities": ["text", "image"], "output_modalities": ["text"]}, "top_provider": {"is_moderated": false, "context_length": 32768, "max_completion_tokens": 8192}, "canonical_slug": "google/gemma-3-12b-it", "context_length": 32768, "expiration_date": null, "hugging_face_id": "google/gemma-3-12b-it", "default_parameters": {}, "per_request_limits": null, "supported_parameters": ["max_tokens", "seed", "stop", "temperature", "top_p"]}	{}	f	0	2026-01-16 11:15:14.615	2026-01-16 11:18:56.256	2026-01-16 11:19:02.539
openrouter:google/gemma-3-4b-it:free	openrouter	google/gemma-3-4b-it:free	{"id": "google/gemma-3-4b-it:free", "name": "Google: Gemma 3 4B (free)", "created": 1741905510, "pricing": {"image": "0", "prompt": "0", "request": "0", "completion": "0", "web_search": "0", "internal_reasoning": "0"}, "description": "Gemma 3 introduces multimodality, supporting vision-language input and text outputs. It handles context windows up to 128k tokens, understands over 140 languages, and offers improved math, reasoning, and chat capabilities, including structured outputs and function calling.", "architecture": {"modality": "text+image->text", "tokenizer": "Gemini", "instruct_type": "gemma", "input_modalities": ["text", "image"], "output_modalities": ["text"]}, "top_provider": {"is_moderated": false, "context_length": 32768, "max_completion_tokens": 8192}, "canonical_slug": "google/gemma-3-4b-it", "context_length": 32768, "expiration_date": null, "hugging_face_id": "google/gemma-3-4b-it", "default_parameters": {}, "per_request_limits": null, "supported_parameters": ["max_tokens", "response_format", "seed", "stop", "temperature", "top_p"]}	{}	f	0	2026-01-16 11:15:14.603	2026-01-16 11:18:56.241	2026-01-16 11:19:02.539
openrouter:openai/gpt-oss-20b:free	openrouter	openai/gpt-oss-20b:free	{"id": "openai/gpt-oss-20b:free", "name": "OpenAI: gpt-oss-20b (free)", "created": 1754414229, "pricing": {"image": "0", "prompt": "0", "request": "0", "completion": "0", "web_search": "0", "internal_reasoning": "0"}, "description": "gpt-oss-20b is an open-weight 21B parameter model released by OpenAI under the Apache 2.0 license. It uses a Mixture-of-Experts (MoE) architecture with 3.6B active parameters per forward pass, optimized for lower-latency inference and deployability on consumer or single-GPU hardware. The model is trained in OpenAI’s Harmony response format and supports reasoning level configuration, fine-tuning, and agentic capabilities including function calling, tool use, and structured outputs.", "architecture": {"modality": "text->text", "tokenizer": "GPT", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}, "top_provider": {"is_moderated": true, "context_length": 131072, "max_completion_tokens": null}, "canonical_slug": "openai/gpt-oss-20b", "context_length": 131072, "expiration_date": null, "hugging_face_id": "openai/gpt-oss-20b", "default_parameters": {"top_p": null, "temperature": null, "frequency_penalty": null}, "per_request_limits": null, "supported_parameters": ["include_reasoning", "max_tokens", "reasoning", "seed", "stop", "temperature", "tool_choice", "tools"]}	{}	f	0	2026-01-16 11:15:14.459	2026-01-16 11:18:56.067	2026-01-16 11:19:02.539
openrouter:z-ai/glm-4.5-air:free	openrouter	z-ai/glm-4.5-air:free	{"id": "z-ai/glm-4.5-air:free", "name": "Z.AI: GLM 4.5 Air (free)", "created": 1753471258, "pricing": {"image": "0", "prompt": "0", "request": "0", "completion": "0", "web_search": "0", "internal_reasoning": "0"}, "description": "GLM-4.5-Air is the lightweight variant of our latest flagship model family, also purpose-built for agent-centric applications. Like GLM-4.5, it adopts the Mixture-of-Experts (MoE) architecture but with a more compact parameter size. GLM-4.5-Air also supports hybrid inference modes, offering a \\"thinking mode\\" for advanced reasoning and tool use, and a \\"non-thinking mode\\" for real-time interaction. Users can control the reasoning behaviour with the `reasoning` `enabled` boolean. [Learn more in our docs](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config)", "architecture": {"modality": "text->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}, "top_provider": {"is_moderated": false, "context_length": 131072, "max_completion_tokens": 96000}, "canonical_slug": "z-ai/glm-4.5-air", "context_length": 131072, "expiration_date": null, "hugging_face_id": "zai-org/GLM-4.5-Air", "default_parameters": {"top_p": null, "temperature": 0.75, "frequency_penalty": null}, "per_request_limits": null, "supported_parameters": ["include_reasoning", "max_tokens", "reasoning", "temperature", "tool_choice", "tools", "top_p"]}	{}	f	0	2026-01-16 11:15:14.472	2026-01-16 11:18:56.08	2026-01-16 11:19:02.539
mistral:ministral-8b-2512	mistral	ministral-8b-2512	{"id": "ministral-8b-2512", "name": "ministral-8b-2512", "type": "base", "object": "model", "aliases": ["ministral-8b-latest"], "created": 1768562337, "owned_by": "mistralai", "deprecation": null, "description": "Ministral 3 (a.k.a. Tinystral) 8B Instruct.", "capabilities": {"ocr": false, "audio": false, "vision": true, "moderation": false, "fine_tuning": true, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": true}, "max_context_length": 262144, "default_model_temperature": 0.3, "deprecation_replacement_model": null}	{}	f	0	2026-01-16 11:15:13.651	2026-01-16 11:18:58.351	2026-01-16 11:23:55.425
openrouter:mistralai/mistral-small-3.1-24b-instruct:free	openrouter	mistralai/mistral-small-3.1-24b-instruct:free	{"id": "mistralai/mistral-small-3.1-24b-instruct:free", "name": "Mistral: Mistral Small 3.1 24B (free)", "created": 1742238937, "pricing": {"image": "0", "prompt": "0", "request": "0", "completion": "0", "web_search": "0", "internal_reasoning": "0"}, "description": "Mistral Small 3.1 24B Instruct is an upgraded variant of Mistral Small 3 (2501), featuring 24 billion parameters with advanced multimodal capabilities. It provides state-of-the-art performance in text-based reasoning and vision tasks, including image analysis, programming, mathematical reasoning, and multilingual support across dozens of languages. Equipped with an extensive 128k token context window and optimized for efficient local inference, it supports use cases such as conversational agents, function calling, long-document comprehension, and privacy-sensitive deployments. The updated version is [Mistral Small 3.2](mistralai/mistral-small-3.2-24b-instruct)", "architecture": {"modality": "text+image->text", "tokenizer": "Mistral", "instruct_type": null, "input_modalities": ["text", "image"], "output_modalities": ["text"]}, "top_provider": {"is_moderated": false, "context_length": 128000, "max_completion_tokens": null}, "canonical_slug": "mistralai/mistral-small-3.1-24b-instruct-2503", "context_length": 128000, "expiration_date": null, "hugging_face_id": "mistralai/Mistral-Small-3.1-24B-Instruct-2503", "default_parameters": {"temperature": 0.3}, "per_request_limits": null, "supported_parameters": ["frequency_penalty", "max_tokens", "presence_penalty", "response_format", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_k", "top_p"]}	{}	f	0	2026-01-16 11:15:14.591	2026-01-16 11:18:56.226	2026-01-16 11:19:02.539
openrouter:meta-llama/llama-3.3-70b-instruct:free	openrouter	meta-llama/llama-3.3-70b-instruct:free	{"id": "meta-llama/llama-3.3-70b-instruct:free", "name": "Meta: Llama 3.3 70B Instruct (free)", "created": 1733506137, "pricing": {"image": "0", "prompt": "0", "request": "0", "completion": "0", "web_search": "0", "internal_reasoning": "0"}, "description": "The Meta Llama 3.3 multilingual large language model (LLM) is a pretrained and instruction tuned generative model in 70B (text in/text out). The Llama 3.3 instruction tuned text only model is optimized for multilingual dialogue use cases and outperforms many of the available open source and closed chat models on common industry benchmarks.\\n\\nSupported languages: English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai.\\n\\n[Model Card](https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/MODEL_CARD.md)", "architecture": {"modality": "text->text", "tokenizer": "Llama3", "instruct_type": "llama3", "input_modalities": ["text"], "output_modalities": ["text"]}, "top_provider": {"is_moderated": false, "context_length": 131072, "max_completion_tokens": null}, "canonical_slug": "meta-llama/llama-3.3-70b-instruct", "context_length": 131072, "expiration_date": null, "hugging_face_id": "meta-llama/Llama-3.3-70B-Instruct", "default_parameters": {}, "per_request_limits": null, "supported_parameters": ["frequency_penalty", "max_tokens", "presence_penalty", "repetition_penalty", "seed", "stop", "temperature", "tool_choice", "tools", "top_k", "top_p"]}	{}	f	0	2026-01-16 11:15:14.651	2026-01-16 11:18:56.304	2026-01-16 11:19:02.539
openrouter:meta-llama/llama-3.1-405b-instruct:free	openrouter	meta-llama/llama-3.1-405b-instruct:free	{"id": "meta-llama/llama-3.1-405b-instruct:free", "name": "Meta: Llama 3.1 405B Instruct (free)", "created": 1721692800, "pricing": {"image": "0", "prompt": "0", "request": "0", "completion": "0", "web_search": "0", "internal_reasoning": "0"}, "description": "The highly anticipated 400B class of Llama3 is here! Clocking in at 128k context with impressive eval scores, the Meta AI team continues to push the frontier of open-source LLMs.\\n\\nMeta's latest class of model (Llama 3.1) launched with a variety of sizes & flavors. This 405B instruct-tuned version is optimized for high quality dialogue usecases.\\n\\nIt has demonstrated strong performance compared to leading closed-source models including GPT-4o and Claude 3.5 Sonnet in evaluations.\\n\\nTo read more about the model release, [click here](https://ai.meta.com/blog/meta-llama-3-1/). Usage of this model is subject to [Meta's Acceptable Use Policy](https://llama.meta.com/llama3/use-policy/).", "architecture": {"modality": "text->text", "tokenizer": "Llama3", "instruct_type": "llama3", "input_modalities": ["text"], "output_modalities": ["text"]}, "top_provider": {"is_moderated": false, "context_length": 131072, "max_completion_tokens": null}, "canonical_slug": "meta-llama/llama-3.1-405b-instruct", "context_length": 131072, "expiration_date": null, "hugging_face_id": "meta-llama/Meta-Llama-3.1-405B-Instruct", "default_parameters": {}, "per_request_limits": null, "supported_parameters": ["frequency_penalty", "max_tokens", "presence_penalty", "repetition_penalty", "temperature"]}	{}	f	0	2026-01-16 11:15:14.708	2026-01-16 11:18:56.376	2026-01-16 11:19:02.539
openrouter:xiaomi/mimo-v2-flash:free	openrouter	xiaomi/mimo-v2-flash:free	{"id": "xiaomi/mimo-v2-flash:free", "name": "Xiaomi: MiMo-V2-Flash (free)", "created": 1765731308, "pricing": {"image": "0", "prompt": "0", "request": "0", "completion": "0", "web_search": "0", "internal_reasoning": "0"}, "description": "MiMo-V2-Flash is an open-source foundation language model developed by Xiaomi. It is a Mixture-of-Experts model with 309B total parameters and 15B active parameters, adopting hybrid attention architecture. MiMo-V2-Flash supports a hybrid-thinking toggle and a 256K context window, and excels at reasoning, coding, and agent scenarios. On SWE-bench Verified and SWE-bench Multilingual, MiMo-V2-Flash ranks as the top #1 open-source model globally, delivering performance comparable to Claude Sonnet 4.5 while costing only about 3.5% as much.\\n\\nNote: when integrating with agentic tools such as Claude Code, Cline, or Roo Code, **turn off reasoning mode** for the best and fastest performance—this model is deeply optimized for this scenario.\\n\\nUsers can control the reasoning behaviour with the `reasoning` `enabled` boolean. [Learn more in our docs](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config).", "architecture": {"modality": "text->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}, "top_provider": {"is_moderated": false, "context_length": 262144, "max_completion_tokens": 65536}, "canonical_slug": "xiaomi/mimo-v2-flash-20251210", "context_length": 262144, "expiration_date": null, "hugging_face_id": "XiaomiMiMo/MiMo-V2-Flash", "default_parameters": {"top_p": 0.95, "temperature": null, "frequency_penalty": null}, "per_request_limits": null, "supported_parameters": ["frequency_penalty", "include_reasoning", "max_tokens", "presence_penalty", "reasoning", "response_format", "stop", "temperature", "tool_choice", "tools", "top_p"]}	{}	f	0	2026-01-16 11:15:14.341	2026-01-16 11:18:55.934	2026-01-16 11:19:02.539
mistral:ministral-8b-latest	mistral	ministral-8b-latest	{"id": "ministral-8b-latest", "name": "ministral-8b-2512", "type": "base", "object": "model", "aliases": ["ministral-8b-2512"], "created": 1768562337, "owned_by": "mistralai", "deprecation": null, "description": "Ministral 3 (a.k.a. Tinystral) 8B Instruct.", "capabilities": {"ocr": false, "audio": false, "vision": true, "moderation": false, "fine_tuning": true, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": true}, "max_context_length": 262144, "default_model_temperature": 0.3, "deprecation_replacement_model": null}	{}	f	0	2026-01-16 11:15:13.663	2026-01-16 11:18:58.364	2026-01-16 11:23:55.425
openrouter:tngtech/deepseek-r1t2-chimera:free	openrouter	tngtech/deepseek-r1t2-chimera:free	{"id": "tngtech/deepseek-r1t2-chimera:free", "name": "TNG: DeepSeek R1T2 Chimera (free)", "created": 1751986985, "pricing": {"image": "0", "prompt": "0", "request": "0", "completion": "0", "web_search": "0", "internal_reasoning": "0"}, "description": "DeepSeek-TNG-R1T2-Chimera is the second-generation Chimera model from TNG Tech. It is a 671 B-parameter mixture-of-experts text-generation model assembled from DeepSeek-AI’s R1-0528, R1, and V3-0324 checkpoints with an Assembly-of-Experts merge. The tri-parent design yields strong reasoning performance while running roughly 20 % faster than the original R1 and more than 2× faster than R1-0528 under vLLM, giving a favorable cost-to-intelligence trade-off. The checkpoint supports contexts up to 60 k tokens in standard use (tested to ~130 k) and maintains consistent <think> token behaviour, making it suitable for long-context analysis, dialogue and other open-ended generation tasks.", "architecture": {"modality": "text->text", "tokenizer": "DeepSeek", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}, "top_provider": {"is_moderated": false, "context_length": 163840, "max_completion_tokens": null}, "canonical_slug": "tngtech/deepseek-r1t2-chimera", "context_length": 163840, "expiration_date": null, "hugging_face_id": "tngtech/DeepSeek-TNG-R1T2-Chimera", "default_parameters": {}, "per_request_limits": null, "supported_parameters": ["frequency_penalty", "include_reasoning", "max_tokens", "presence_penalty", "reasoning", "repetition_penalty", "seed", "stop", "temperature", "top_k", "top_p"]}	{}	f	0	2026-01-16 11:15:14.531	2026-01-16 11:18:56.149	2026-01-16 11:19:02.539
openrouter:allenai/molmo-2-8b:free	openrouter	allenai/molmo-2-8b:free	{"id": "allenai/molmo-2-8b:free", "name": "AllenAI: Molmo2 8B (free)", "created": 1767996672, "pricing": {"image": "0", "prompt": "0", "request": "0", "completion": "0", "web_search": "0", "internal_reasoning": "0"}, "description": "Molmo2-8B is an open vision-language model developed by the Allen Institute for AI (Ai2) as part of the Molmo2 family, supporting image, video, and multi-image understanding and grounding. It is based on Qwen3-8B and uses SigLIP 2 as its vision backbone, outperforming other open-weight, open-data models on short videos, counting, and captioning, while remaining competitive on long-video tasks.", "architecture": {"modality": "text+image->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["text", "image", "video"], "output_modalities": ["text"]}, "top_provider": {"is_moderated": false, "context_length": 36864, "max_completion_tokens": 36864}, "canonical_slug": "allenai/molmo-2-8b-20260109", "context_length": 36864, "expiration_date": null, "hugging_face_id": "allenai/Molmo2-8B", "default_parameters": {"top_p": null, "temperature": null, "frequency_penalty": null}, "per_request_limits": null, "supported_parameters": ["frequency_penalty", "logit_bias", "max_tokens", "presence_penalty", "repetition_penalty", "response_format", "seed", "stop", "temperature", "top_k", "top_p"]}	{}	f	0	2026-01-16 11:15:14.328	2026-01-16 11:18:55.918	2026-01-16 11:19:02.539
openrouter:nvidia/nemotron-3-nano-30b-a3b:free	openrouter	nvidia/nemotron-3-nano-30b-a3b:free	{"id": "nvidia/nemotron-3-nano-30b-a3b:free", "name": "NVIDIA: Nemotron 3 Nano 30B A3B (free)", "created": 1765731275, "pricing": {"image": "0", "prompt": "0", "request": "0", "completion": "0", "web_search": "0", "internal_reasoning": "0"}, "description": "NVIDIA Nemotron 3 Nano 30B A3B is a small language MoE model with highest compute efficiency and accuracy for developers to build specialized agentic AI systems.\\n\\nThe model is fully open with open-weights, datasets and recipes so developers can easily\\ncustomize, optimize, and deploy the model on their infrastructure for maximum privacy and\\nsecurity.\\n\\nNote: For the free endpoint, all prompts and output are logged to improve the provider's model and its product and services. Please do not upload any personal, confidential, or otherwise sensitive information. This is a trial use only. Do not use for production or business-critical systems.", "architecture": {"modality": "text->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}, "top_provider": {"is_moderated": false, "context_length": 256000, "max_completion_tokens": null}, "canonical_slug": "nvidia/nemotron-3-nano-30b-a3b", "context_length": 256000, "expiration_date": null, "hugging_face_id": "nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16", "default_parameters": {"top_p": null, "temperature": null, "frequency_penalty": null}, "per_request_limits": null, "supported_parameters": ["include_reasoning", "max_tokens", "reasoning", "seed", "temperature", "tool_choice", "tools", "top_p"]}	{}	f	0	2026-01-16 11:15:14.353	2026-01-16 11:18:55.948	2026-01-16 11:19:02.539
openrouter:nousresearch/hermes-3-llama-3.1-405b:free	openrouter	nousresearch/hermes-3-llama-3.1-405b:free	{"id": "nousresearch/hermes-3-llama-3.1-405b:free", "name": "Nous: Hermes 3 405B Instruct (free)", "created": 1723766400, "pricing": {"image": "0", "prompt": "0", "request": "0", "completion": "0", "web_search": "0", "internal_reasoning": "0"}, "description": "Hermes 3 is a generalist language model with many improvements over Hermes 2, including advanced agentic capabilities, much better roleplaying, reasoning, multi-turn conversation, long context coherence, and improvements across the board.\\n\\nHermes 3 405B is a frontier-level, full-parameter finetune of the Llama-3.1 405B foundation model, focused on aligning LLMs to the user, with powerful steering capabilities and control given to the end user.\\n\\nThe Hermes 3 series builds and expands on the Hermes 2 set of capabilities, including more powerful and reliable function calling and structured output capabilities, generalist assistant capabilities, and improved code generation skills.\\n\\nHermes 3 is competitive, if not superior, to Llama-3.1 Instruct models at general capabilities, with varying strengths and weaknesses attributable between the two.", "architecture": {"modality": "text->text", "tokenizer": "Llama3", "instruct_type": "chatml", "input_modalities": ["text"], "output_modalities": ["text"]}, "top_provider": {"is_moderated": false, "context_length": 131072, "max_completion_tokens": null}, "canonical_slug": "nousresearch/hermes-3-llama-3.1-405b", "context_length": 131072, "expiration_date": null, "hugging_face_id": "NousResearch/Hermes-3-Llama-3.1-405B", "default_parameters": {}, "per_request_limits": null, "supported_parameters": ["frequency_penalty", "max_tokens", "presence_penalty", "stop", "temperature", "top_k", "top_p"]}	{}	f	0	2026-01-16 11:15:14.696	2026-01-16 11:18:56.359	2026-01-16 11:19:02.539
mistral:ministral-14b-2512	mistral	ministral-14b-2512	{"id": "ministral-14b-2512", "name": "ministral-14b-2512", "type": "base", "object": "model", "aliases": ["ministral-14b-latest"], "created": 1768562337, "owned_by": "mistralai", "deprecation": null, "description": "Ministral 3 (a.k.a. Tinystral) 14B Instruct.", "capabilities": {"ocr": false, "audio": false, "vision": true, "moderation": false, "fine_tuning": true, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": true}, "max_context_length": 262144, "default_model_temperature": 0.3, "deprecation_replacement_model": null}	{}	f	0	2026-01-16 11:15:13.675	2026-01-16 11:18:58.377	2026-01-16 11:23:55.425
openrouter:arcee-ai/trinity-mini:free	openrouter	arcee-ai/trinity-mini:free	{"id": "arcee-ai/trinity-mini:free", "name": "Arcee AI: Trinity Mini (free)", "created": 1764601720, "pricing": {"image": "0", "prompt": "0", "request": "0", "completion": "0", "web_search": "0", "internal_reasoning": "0"}, "description": "Trinity Mini is a 26B-parameter (3B active) sparse mixture-of-experts language model featuring 128 experts with 8 active per token. Engineered for efficient reasoning over long contexts (131k) with robust function calling and multi-step agent workflows.", "architecture": {"modality": "text->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}, "top_provider": {"is_moderated": false, "context_length": 131072, "max_completion_tokens": null}, "canonical_slug": "arcee-ai/trinity-mini-20251201", "context_length": 131072, "expiration_date": null, "hugging_face_id": "arcee-ai/Trinity-Mini", "default_parameters": {"top_p": 0.75, "temperature": 0.15, "frequency_penalty": null}, "per_request_limits": null, "supported_parameters": ["include_reasoning", "max_tokens", "reasoning", "response_format", "structured_outputs", "temperature", "tool_choice", "tools", "top_k", "top_p"]}	{}	f	0	2026-01-16 11:15:14.378	2026-01-16 11:18:55.977	2026-01-16 11:19:02.539
openrouter:tngtech/tng-r1t-chimera:free	openrouter	tngtech/tng-r1t-chimera:free	{"id": "tngtech/tng-r1t-chimera:free", "name": "TNG: R1T Chimera (free)", "created": 1764184161, "pricing": {"image": "0", "prompt": "0", "request": "0", "completion": "0", "web_search": "0", "internal_reasoning": "0"}, "description": "TNG-R1T-Chimera is an experimental LLM with a faible for creative storytelling and character interaction. It is a derivate of the original TNG/DeepSeek-R1T-Chimera released in April 2025 and is available exclusively via Chutes and OpenRouter.\\n\\nCharacteristics and improvements include:\\n\\nWe think that it has a creative and pleasant personality.\\nIt has a preliminary EQ-Bench3 value of about 1305.\\nIt is quite a bit more intelligent than the original, albeit a slightly slower.\\nIt is much more think-token consistent, i.e. reasoning and answer blocks are properly delineated.\\nTool calling is much improved.\\n\\nTNG Tech, the model authors, ask that users follow the careful guidelines that Microsoft has created for their \\"MAI-DS-R1\\" DeepSeek-based model. These guidelines are available on Hugging Face (https://huggingface.co/microsoft/MAI-DS-R1).", "architecture": {"modality": "text->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}, "top_provider": {"is_moderated": false, "context_length": 163840, "max_completion_tokens": 65536}, "canonical_slug": "tngtech/tng-r1t-chimera", "context_length": 163840, "expiration_date": null, "hugging_face_id": null, "default_parameters": {"top_p": null, "temperature": null, "frequency_penalty": null}, "per_request_limits": null, "supported_parameters": ["frequency_penalty", "include_reasoning", "max_tokens", "presence_penalty", "reasoning", "repetition_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_k", "top_p"]}	{}	f	0	2026-01-16 11:15:14.391	2026-01-16 11:18:55.992	2026-01-16 11:19:02.539
openrouter:meta-llama/llama-3.2-3b-instruct:free	openrouter	meta-llama/llama-3.2-3b-instruct:free	{"id": "meta-llama/llama-3.2-3b-instruct:free", "name": "Meta: Llama 3.2 3B Instruct (free)", "created": 1727222400, "pricing": {"image": "0", "prompt": "0", "request": "0", "completion": "0", "web_search": "0", "internal_reasoning": "0"}, "description": "Llama 3.2 3B is a 3-billion-parameter multilingual large language model, optimized for advanced natural language processing tasks like dialogue generation, reasoning, and summarization. Designed with the latest transformer architecture, it supports eight languages, including English, Spanish, and Hindi, and is adaptable for additional languages.\\n\\nTrained on 9 trillion tokens, the Llama 3.2 3B model excels in instruction-following, complex reasoning, and tool use. Its balanced performance makes it ideal for applications needing accuracy and efficiency in text generation across multilingual settings.\\n\\nClick here for the [original model card](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD.md).\\n\\nUsage of this model is subject to [Meta's Acceptable Use Policy](https://www.llama.com/llama3/use-policy/).", "architecture": {"modality": "text->text", "tokenizer": "Llama3", "instruct_type": "llama3", "input_modalities": ["text"], "output_modalities": ["text"]}, "top_provider": {"is_moderated": false, "context_length": 131072, "max_completion_tokens": null}, "canonical_slug": "meta-llama/llama-3.2-3b-instruct", "context_length": 131072, "expiration_date": null, "hugging_face_id": "meta-llama/Llama-3.2-3B-Instruct", "default_parameters": {}, "per_request_limits": null, "supported_parameters": ["frequency_penalty", "max_tokens", "presence_penalty", "stop", "temperature", "top_k", "top_p"]}	{}	f	0	2026-01-16 11:15:14.665	2026-01-16 11:18:56.323	2026-01-16 11:19:02.539
mistral:mistral-tiny-2407	mistral	mistral-tiny-2407	{"id": "mistral-tiny-2407", "name": "open-mistral-nemo", "type": "base", "object": "model", "aliases": ["open-mistral-nemo", "open-mistral-nemo-2407", "mistral-tiny-latest"], "created": 1768562337, "owned_by": "mistralai", "deprecation": null, "description": "Our best multilingual open source model released July 2024.", "capabilities": {"ocr": false, "audio": false, "vision": false, "moderation": false, "fine_tuning": true, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": true}, "max_context_length": 131072, "default_model_temperature": 0.3, "deprecation_replacement_model": null}	{}	f	0	2026-01-16 11:15:13.27	2026-01-16 11:18:57.904	2026-01-16 11:23:55.425
mistral:mistral-tiny-latest	mistral	mistral-tiny-latest	{"id": "mistral-tiny-latest", "name": "open-mistral-nemo", "type": "base", "object": "model", "aliases": ["open-mistral-nemo", "open-mistral-nemo-2407", "mistral-tiny-2407"], "created": 1768562337, "owned_by": "mistralai", "deprecation": null, "description": "Our best multilingual open source model released July 2024.", "capabilities": {"ocr": false, "audio": false, "vision": false, "moderation": false, "fine_tuning": true, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": true}, "max_context_length": 131072, "default_model_temperature": 0.3, "deprecation_replacement_model": null}	{}	f	0	2026-01-16 11:15:13.282	2026-01-16 11:18:57.918	2026-01-16 11:23:55.425
mistral:mistral-large-2411	mistral	mistral-large-2411	{"id": "mistral-large-2411", "name": "mistral-large-2411", "type": "base", "object": "model", "aliases": [], "created": 1768562337, "owned_by": "mistralai", "deprecation": null, "description": "Our top-tier reasoning model for high-complexity tasks with the lastest version released November 2024.", "capabilities": {"ocr": false, "audio": false, "vision": false, "moderation": false, "fine_tuning": true, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": true}, "max_context_length": 131072, "default_model_temperature": 0.7, "deprecation_replacement_model": null}	{}	f	0	2026-01-16 11:15:13.295	2026-01-16 11:18:57.932	2026-01-16 11:23:55.425
mistral:ministral-3b-2512	mistral	ministral-3b-2512	{"id": "ministral-3b-2512", "name": "ministral-3b-2512", "type": "base", "object": "model", "aliases": ["ministral-3b-latest"], "created": 1768562337, "owned_by": "mistralai", "deprecation": null, "description": "Ministral 3 (a.k.a. Tinystral) 3B Instruct.", "capabilities": {"ocr": false, "audio": false, "vision": true, "moderation": false, "fine_tuning": true, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": true}, "max_context_length": 131072, "default_model_temperature": 0.3, "deprecation_replacement_model": null}	{}	f	0	2026-01-16 11:15:13.628	2026-01-16 11:18:58.323	2026-01-16 11:23:55.425
mistral:mistral-large-2512	mistral	mistral-large-2512	{"id": "mistral-large-2512", "name": "mistral-large-2512", "type": "base", "object": "model", "aliases": [], "created": 1768562337, "owned_by": "mistralai", "deprecation": null, "description": "Official mistral-large-2512 Mistral AI model", "capabilities": {"ocr": false, "audio": false, "vision": true, "moderation": false, "fine_tuning": true, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": true}, "max_context_length": 262144, "default_model_temperature": 0.3, "deprecation_replacement_model": null}	{}	f	0	2026-01-16 11:15:13.604	2026-01-16 11:18:58.296	2026-01-16 11:23:55.425
mistral:ministral-8b-2410	mistral	ministral-8b-2410	{"id": "ministral-8b-2410", "name": "ministral-8b-2410", "type": "base", "object": "model", "aliases": [], "created": 1768562337, "owned_by": "mistralai", "deprecation": "2026-01-31T12:00:00Z", "description": "Powerful edge model with extremely high performance/price ratio.", "capabilities": {"ocr": false, "audio": false, "vision": false, "moderation": false, "fine_tuning": true, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": true}, "max_context_length": 131072, "default_model_temperature": 0.3, "deprecation_replacement_model": "ministral-8b-latest"}	{}	f	0	2026-01-16 11:15:13.795	2026-01-16 11:18:58.543	2026-01-16 11:23:55.425
mistral:codestral-2501	mistral	codestral-2501	{"id": "codestral-2501", "name": "codestral-2501", "type": "base", "object": "model", "aliases": ["codestral-2412", "codestral-2411-rc5"], "created": 1768562337, "owned_by": "mistralai", "deprecation": "2026-01-31T12:00:00Z", "description": "Our cutting-edge language model for coding released December 2024.", "capabilities": {"ocr": false, "audio": false, "vision": false, "moderation": false, "fine_tuning": true, "classification": false, "completion_fim": true, "completion_chat": true, "function_calling": true}, "max_context_length": 256000, "default_model_temperature": 0.3, "deprecation_replacement_model": "codestral-latest"}	{}	f	0	2026-01-16 11:15:13.806	2026-01-16 11:18:58.557	2026-01-16 11:23:55.425
mistral:codestral-2412	mistral	codestral-2412	{"id": "codestral-2412", "name": "codestral-2501", "type": "base", "object": "model", "aliases": ["codestral-2501", "codestral-2411-rc5"], "created": 1768562337, "owned_by": "mistralai", "deprecation": "2026-01-31T12:00:00Z", "description": "Our cutting-edge language model for coding released December 2024.", "capabilities": {"ocr": false, "audio": false, "vision": false, "moderation": false, "fine_tuning": true, "classification": false, "completion_fim": true, "completion_chat": true, "function_calling": true}, "max_context_length": 256000, "default_model_temperature": 0.3, "deprecation_replacement_model": "codestral-latest"}	{}	f	0	2026-01-16 11:15:13.818	2026-01-16 11:18:58.573	2026-01-16 11:23:55.425
mistral:codestral-2411-rc5	mistral	codestral-2411-rc5	{"id": "codestral-2411-rc5", "name": "codestral-2501", "type": "base", "object": "model", "aliases": ["codestral-2501", "codestral-2412"], "created": 1768562337, "owned_by": "mistralai", "deprecation": "2026-01-31T12:00:00Z", "description": "Our cutting-edge language model for coding released December 2024.", "capabilities": {"ocr": false, "audio": false, "vision": false, "moderation": false, "fine_tuning": true, "classification": false, "completion_fim": true, "completion_chat": true, "function_calling": true}, "max_context_length": 256000, "default_model_temperature": 0.3, "deprecation_replacement_model": "codestral-latest"}	{}	f	0	2026-01-16 11:15:13.83	2026-01-16 11:18:58.59	2026-01-16 11:23:55.425
mistral:mistral-small-2501	mistral	mistral-small-2501	{"id": "mistral-small-2501", "name": "mistral-small-2501", "type": "base", "object": "model", "aliases": [], "created": 1768562337, "owned_by": "mistralai", "deprecation": "2026-01-31T12:00:00Z", "description": "Our latest enterprise-grade small model with the latest version released January 2025. ", "capabilities": {"ocr": false, "audio": false, "vision": false, "moderation": false, "fine_tuning": true, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": true}, "max_context_length": 32768, "default_model_temperature": 0.3, "deprecation_replacement_model": "mistral-small-latest"}	{}	f	0	2026-01-16 11:15:13.841	2026-01-16 11:18:58.604	2026-01-16 11:23:55.425
groq:meta-llama/llama-4-scout-17b-16e-instruct	groq	meta-llama/llama-4-scout-17b-16e-instruct	{"id": "meta-llama/llama-4-scout-17b-16e-instruct", "active": true, "object": "model", "created": 1743874824, "owned_by": "Meta", "public_apps": null, "context_window": 131072, "max_completion_tokens": 8192}	{}	f	0	2026-01-16 11:15:15.212	2026-01-16 11:19:00.129	2026-01-16 11:23:55.425
mistral:pixtral-12b-2409	mistral	pixtral-12b-2409	{"id": "pixtral-12b-2409", "name": "pixtral-12b-2409", "type": "base", "object": "model", "aliases": ["pixtral-12b", "pixtral-12b-latest"], "created": 1768562337, "owned_by": "mistralai", "deprecation": "2026-01-31T12:00:00Z", "description": "A 12B model with image understanding capabilities in addition to text.", "capabilities": {"ocr": false, "audio": false, "vision": true, "moderation": false, "fine_tuning": true, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": true}, "max_context_length": 131072, "default_model_temperature": 0.3, "deprecation_replacement_model": "ministral-14b-latest"}	{}	f	0	2026-01-16 11:15:13.734	2026-01-16 11:18:58.467	2026-01-16 11:23:55.425
mistral:pixtral-12b	mistral	pixtral-12b	{"id": "pixtral-12b", "name": "pixtral-12b-2409", "type": "base", "object": "model", "aliases": ["pixtral-12b-2409", "pixtral-12b-latest"], "created": 1768562337, "owned_by": "mistralai", "deprecation": "2026-01-31T12:00:00Z", "description": "A 12B model with image understanding capabilities in addition to text.", "capabilities": {"ocr": false, "audio": false, "vision": true, "moderation": false, "fine_tuning": true, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": true}, "max_context_length": 131072, "default_model_temperature": 0.3, "deprecation_replacement_model": "ministral-14b-latest"}	{}	f	0	2026-01-16 11:15:13.751	2026-01-16 11:18:58.484	2026-01-16 11:23:55.425
mistral:pixtral-12b-latest	mistral	pixtral-12b-latest	{"id": "pixtral-12b-latest", "name": "pixtral-12b-2409", "type": "base", "object": "model", "aliases": ["pixtral-12b-2409", "pixtral-12b"], "created": 1768562337, "owned_by": "mistralai", "deprecation": "2026-01-31T12:00:00Z", "description": "A 12B model with image understanding capabilities in addition to text.", "capabilities": {"ocr": false, "audio": false, "vision": true, "moderation": false, "fine_tuning": true, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": true}, "max_context_length": 131072, "default_model_temperature": 0.3, "deprecation_replacement_model": "ministral-14b-latest"}	{}	f	0	2026-01-16 11:15:13.768	2026-01-16 11:18:58.502	2026-01-16 11:23:55.425
mistral:mistral-moderation-latest	mistral	mistral-moderation-latest	{"id": "mistral-moderation-latest", "name": "mistral-moderation-2411", "type": "base", "object": "model", "aliases": ["mistral-moderation-2411"], "created": 1768562337, "owned_by": "mistralai", "deprecation": null, "description": "Official mistral-moderation-2411 Mistral AI model", "capabilities": {"ocr": false, "audio": false, "vision": false, "moderation": true, "fine_tuning": false, "classification": true, "completion_fim": false, "completion_chat": false, "function_calling": false}, "max_context_length": 8192, "default_model_temperature": null, "deprecation_replacement_model": null}	{}	f	0	2026-01-16 11:15:13.914	2026-01-16 11:18:58.686	2026-01-16 11:23:55.425
mistral:mistral-ocr-2512	mistral	mistral-ocr-2512	{"id": "mistral-ocr-2512", "name": "mistral-ocr-2512", "type": "base", "object": "model", "aliases": ["mistral-ocr-latest"], "created": 1768562337, "owned_by": "mistralai", "deprecation": null, "description": "Official mistral-ocr-2512 Mistral AI model", "capabilities": {"ocr": true, "audio": false, "vision": true, "moderation": false, "fine_tuning": false, "classification": false, "completion_fim": false, "completion_chat": false, "function_calling": true}, "max_context_length": 16384, "default_model_temperature": 0, "deprecation_replacement_model": null}	{}	f	0	2026-01-16 11:15:13.926	2026-01-16 11:18:58.696	2026-01-16 11:23:55.425
mistral:mistral-ocr-latest	mistral	mistral-ocr-latest	{"id": "mistral-ocr-latest", "name": "mistral-ocr-2512", "type": "base", "object": "model", "aliases": ["mistral-ocr-2512"], "created": 1768562337, "owned_by": "mistralai", "deprecation": null, "description": "Official mistral-ocr-2512 Mistral AI model", "capabilities": {"ocr": true, "audio": false, "vision": true, "moderation": false, "fine_tuning": false, "classification": false, "completion_fim": false, "completion_chat": false, "function_calling": true}, "max_context_length": 16384, "default_model_temperature": 0, "deprecation_replacement_model": null}	{}	f	0	2026-01-16 11:15:13.94	2026-01-16 11:18:58.711	2026-01-16 11:23:55.425
mistral:mistral-ocr-2505	mistral	mistral-ocr-2505	{"id": "mistral-ocr-2505", "name": "mistral-ocr-2505", "type": "base", "object": "model", "aliases": [], "created": 1768562337, "owned_by": "mistralai", "deprecation": null, "description": "Official mistral-ocr-2505 Mistral AI model", "capabilities": {"ocr": true, "audio": false, "vision": true, "moderation": false, "fine_tuning": false, "classification": false, "completion_fim": false, "completion_chat": false, "function_calling": true}, "max_context_length": 16384, "default_model_temperature": 0, "deprecation_replacement_model": null}	{}	f	0	2026-01-16 11:15:13.952	2026-01-16 11:18:58.727	2026-01-16 11:23:55.425
mistral:mistral-ocr-2503	mistral	mistral-ocr-2503	{"id": "mistral-ocr-2503", "name": "mistral-ocr-2503", "type": "base", "object": "model", "aliases": [], "created": 1768562337, "owned_by": "mistralai", "deprecation": "2026-03-31T12:00:00Z", "description": "Official mistral-ocr-2503 Mistral AI model", "capabilities": {"ocr": true, "audio": false, "vision": true, "moderation": false, "fine_tuning": false, "classification": false, "completion_fim": false, "completion_chat": false, "function_calling": true}, "max_context_length": 16384, "default_model_temperature": 0, "deprecation_replacement_model": "mistral-ocr-latest"}	{}	f	0	2026-01-16 11:15:13.963	2026-01-16 11:18:58.743	2026-01-16 11:23:55.425
mistral:mistral-embed	mistral	mistral-embed	{"id": "mistral-embed", "name": "mistral-embed-2312", "type": "base", "object": "model", "aliases": ["mistral-embed-2312"], "created": 1768562337, "owned_by": "mistralai", "deprecation": null, "description": "Official mistral-embed-2312 Mistral AI model", "capabilities": {"ocr": false, "audio": false, "vision": false, "moderation": false, "fine_tuning": false, "classification": false, "completion_fim": false, "completion_chat": false, "function_calling": false}, "max_context_length": 8192, "default_model_temperature": null, "deprecation_replacement_model": null}	{}	f	0	2026-01-16 11:15:13.866	2026-01-16 11:18:58.636	2026-01-16 11:23:55.425
mistral:codestral-embed	mistral	codestral-embed	{"id": "codestral-embed", "name": "codestral-embed", "type": "base", "object": "model", "aliases": ["codestral-embed-2505"], "created": 1768562337, "owned_by": "mistralai", "deprecation": null, "description": "Official codestral-embed Mistral AI model", "capabilities": {"ocr": false, "audio": false, "vision": false, "moderation": false, "fine_tuning": false, "classification": false, "completion_fim": false, "completion_chat": false, "function_calling": false}, "max_context_length": 8192, "default_model_temperature": null, "deprecation_replacement_model": null}	{}	f	0	2026-01-16 11:15:13.878	2026-01-16 11:18:58.647	2026-01-16 11:23:55.425
mistral:codestral-embed-2505	mistral	codestral-embed-2505	{"id": "codestral-embed-2505", "name": "codestral-embed", "type": "base", "object": "model", "aliases": ["codestral-embed"], "created": 1768562337, "owned_by": "mistralai", "deprecation": null, "description": "Official codestral-embed Mistral AI model", "capabilities": {"ocr": false, "audio": false, "vision": false, "moderation": false, "fine_tuning": false, "classification": false, "completion_fim": false, "completion_chat": false, "function_calling": false}, "max_context_length": 8192, "default_model_temperature": null, "deprecation_replacement_model": null}	{}	f	0	2026-01-16 11:15:13.89	2026-01-16 11:18:58.661	2026-01-16 11:23:55.425
groq:allam-2-7b	groq	allam-2-7b	{"id": "allam-2-7b", "active": true, "object": "model", "created": 1737672203, "owned_by": "SDAIA", "public_apps": null, "context_window": 4096, "max_completion_tokens": 4096}	{}	f	0	2026-01-16 11:15:15.078	2026-01-16 11:18:59.969	2026-01-16 11:23:55.425
groq:openai/gpt-oss-120b	groq	openai/gpt-oss-120b	{"id": "openai/gpt-oss-120b", "active": true, "object": "model", "created": 1754408224, "owned_by": "OpenAI", "public_apps": null, "context_window": 131072, "max_completion_tokens": 65536}	{}	f	0	2026-01-16 11:15:15.188	2026-01-16 11:19:00.067	2026-01-16 11:23:55.425
groq:qwen/qwen3-32b	groq	qwen/qwen3-32b	{"id": "qwen/qwen3-32b", "active": true, "object": "model", "created": 1748396646, "owned_by": "Alibaba Cloud", "public_apps": null, "context_window": 131072, "max_completion_tokens": 40960}	{}	f	0	2026-01-16 11:15:15.067	2026-01-16 11:19:00.007	2026-01-16 11:23:55.425
groq:groq/compound	groq	groq/compound	{"id": "groq/compound", "active": true, "object": "model", "created": 1756949530, "owned_by": "Groq", "public_apps": null, "context_window": 131072, "max_completion_tokens": 8192}	{}	f	0	2026-01-16 11:15:15.134	2026-01-16 11:18:59.929	2026-01-16 11:23:55.425
mistral:mistral-medium-2505	mistral	mistral-medium-2505	{"id": "mistral-medium-2505", "name": "mistral-medium-2505", "type": "base", "object": "model", "aliases": [], "created": 1768562337, "owned_by": "mistralai", "deprecation": null, "description": "Our frontier-class multimodal model released May 2025.", "capabilities": {"ocr": false, "audio": false, "vision": true, "moderation": false, "fine_tuning": true, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": true}, "max_context_length": 131072, "default_model_temperature": 0.3, "deprecation_replacement_model": null}	{}	f	0	2026-01-16 11:15:13.192	2026-01-16 11:18:57.812	2026-01-16 11:23:55.425
cerebras:llama3.1-8b	cerebras	llama3.1-8b	{"id": "llama3.1-8b", "object": "model", "created": 0, "owned_by": "Cerebras"}	{}	f	0	2026-01-16 11:15:18.414	2026-01-16 11:19:02.433	2026-01-16 11:23:55.425
mistral:open-mistral-7b	mistral	open-mistral-7b	{"id": "open-mistral-7b", "name": "open-mistral-7b", "type": "base", "object": "model", "aliases": ["mistral-tiny", "mistral-tiny-2312"], "created": 1768562337, "owned_by": "mistralai", "deprecation": "2026-01-31T12:00:00Z", "description": "Our first dense model released September 2023.", "capabilities": {"ocr": false, "audio": false, "vision": false, "moderation": false, "fine_tuning": true, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": true}, "max_context_length": 32768, "default_model_temperature": 0.7, "deprecation_replacement_model": "ministral-8b-latest"}	{}	f	0	2026-01-16 11:15:13.699	2026-01-16 11:18:58.426	2026-01-16 11:23:55.425
mistral:mistral-tiny-2312	mistral	mistral-tiny-2312	{"id": "mistral-tiny-2312", "name": "open-mistral-7b", "type": "base", "object": "model", "aliases": ["open-mistral-7b", "mistral-tiny"], "created": 1768562337, "owned_by": "mistralai", "deprecation": "2026-01-31T12:00:00Z", "description": "Our first dense model released September 2023.", "capabilities": {"ocr": false, "audio": false, "vision": false, "moderation": false, "fine_tuning": true, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": true}, "max_context_length": 32768, "default_model_temperature": 0.7, "deprecation_replacement_model": "ministral-8b-latest"}	{}	f	0	2026-01-16 11:15:13.722	2026-01-16 11:18:58.454	2026-01-16 11:23:55.425
\.


--
-- Data for Name: ModelFailure; Type: TABLE DATA; Schema: public; Owner: -
--

COPY public."ModelFailure" (id, "providerId", "modelId", failures, "lastFailedAt") FROM stdin;
cmkgdzmqq00eslwnvy3gv20py	nvidia-env	google/gemma-2b	1	2026-01-16 04:36:53.091
cmkgeu3zf00ewtv9wje8aqi3g	nvidia-env	nvidia/nemotron-4-340b-reward	1	2026-01-16 05:00:35.115
cmkgeu4nr00extv9w3dbu8oor	nvidia-env	nvidia/streampetr	1	2026-01-16 05:00:35.991
cmkgica0o00v2i1ec4qtuux75	nvidia-env	meta/codellama-70b	1	2026-01-16 06:38:41.593
cmkgk16r100y6d9b74t7mfveg	nvidia-env	meta/llama2-70b	1	2026-01-16 07:26:03.373
\.


--
-- Data for Name: ModelUsage; Type: TABLE DATA; Schema: public; Owner: -
--

COPY public."ModelUsage" (id, "createdAt", "modelId", "roleId", "promptTokens", "completionTokens", cost, "usageMetrics", "responseHeaders", metadata) FROM stdin;
\.


--
-- Data for Name: PromptRefinement; Type: TABLE DATA; Schema: public; Owner: -
--

COPY public."PromptRefinement" (id, "roleId", "originalPrompt", critique, "refinedPrompt", "createdAt") FROM stdin;
\.


--
-- Data for Name: ProviderConfig; Type: TABLE DATA; Schema: public; Owner: -
--

COPY public."ProviderConfig" (id, label, type, "baseURL", "isEnabled", "createdAt", "updatedAt") FROM stdin;
google	google	google		t	2026-01-16 03:43:28.487	2026-01-16 03:43:28.487
mistral	Mistral API (Env)	mistral		t	2026-01-16 11:15:12.725	2026-01-16 11:15:12.725
openrouter	OpenRouter (Env)	openrouter	https://openrouter.ai/api/v1	t	2026-01-16 11:15:12.738	2026-01-16 11:15:12.738
groq	Groq (Env)	groq	https://api.groq.com/openai/v1	t	2026-01-16 11:15:12.745	2026-01-16 11:15:12.745
nvidia	NVIDIA (Env)	nvidia	https://integrate.api.nvidia.com/v1	t	2026-01-16 11:15:12.751	2026-01-16 11:15:12.751
cerebras	Cerebras (Env)	cerebras	https://api.cerebras.ai/v1	t	2026-01-16 11:15:12.757	2026-01-16 11:15:12.757
ollama-local	Ollama (Local)	ollama	http://127.0.0.1:11434	t	2026-01-16 11:15:12.79	2026-01-16 11:15:12.79
\.


--
-- Data for Name: RewardModel; Type: TABLE DATA; Schema: public; Owner: -
--

COPY public."RewardModel" (id, "modelId", "scoreType") FROM stdin;
\.


--
-- Data for Name: Role; Type: TABLE DATA; Schema: public; Owner: -
--

COPY public."Role" (id, name, description, "basePrompt", "categoryId", "createdAt", "updatedAt", metadata) FROM stdin;
cmkgd9c8h00027f3hmqscsvfs	Nebula Architect	The Master Builder. Designs and constructs UI using the Nebula runtime.	# Nebula Architect\n\nYou are the master designer of the Nebula ecosystem. Your mission is to construct and refine user interfaces by writing executable TypeScript code that runs in the live Nebula runtime.\n\n## 🎯 Primary Directives\n1. **CODE IS THE MEDIUM**: You do not describe UI changes; you execute them.\n2. **ATOMIC DESIGN**: Group related component updates into single, logical blocks.\n3. **CONTEXT FIRST**: Always verify the current state of the Nebula Tree (`tree.nodes`) before making modifications.\n\n## 📋 Operational Workflow\n\n### Phase 1: Planning (OUTSIDE code blocks)\nBefore execution, clearly state your intent:\n- **LOCATE**: Target parent node ID (e.g., "root" or a specific component ID).\n- **DEFINE**: UI strategy (Grid vs Flex, Color tokens, Spacing).\n- **EXECUTE**: Summary of the build.\n\n### Phase 2: Execution (INSIDE ```typescript blocks)\nWrite ONLY clean, executable code.\n\n## 🛠️ Global API Reference\n- `nebula.addNode(parentId, config)`: Returns a unique `nodeId`. **YOU MUST CAPTURE THIS ID.**\n- `nebula.updateNode(nodeId, updates)`: Modifies an existing node.\n- `nebula.deleteNode(nodeId)`: Removes a node.\n- `ast.parse(jsx)`: Ingests raw JSX/TSX strings into Nebula fragments.\n- `tree.nodes`: Read-only access to the current UI state.\n\n## 📦 Component Library\n- **Primitives**: `Box`, `Text`, `Icon`, `Grid`, `Container`, `Mosaic`.\n- **Composite**: `Card`, `Tabs`, `Navbar`, `Hero`, `Stat`, `Badge`.\n- **Interactive**: `Button`, `Input`, `Textarea`, `Slider`, `Checkbox`.\n- **AI-Enhanced**: `AiButton`, `SuperAiButton`.\n\n## ⚠️ Critical Rules\n- ❌ NEVER use conversational filler like "Sure, I can do that."\n- ❌ NEVER use `tree.rootId`. Use the string `"root"`.\n- ✅ ALWAYS capture return IDs: `const myId = nebula.addNode(...)`.\n- ✅ ALWAYS use Tailwind classes for styling (e.g., `bg-zinc-900`, `p-4`, `gap-2`).	cmkgd9c8800007f3hjc5f3jia	2026-01-16 04:16:26.417	2026-01-16 04:16:26.417	{"needsReasoning": true}
cmkgd9c99000a7f3hndnpnu6o	System Judge	The impartial arbiter. Reviews agent work against strict quality and safety standards.	# System Judge\nYou are the System Judge, the quality assurance engine of the Nebula ecosystem.\nYour goal is to AUDIT the work of other agents and provide a pass/fail grade with specific feedback.\n\n## ⚖️ Directives\n1. **Unbiased auditing**: You do not write code; you review it.\n2. **Strict Guidelines**: Verify inputs against the "Governance Module" rules provided in the context.\n3. **Security First**: immediately flag unsafe patterns (e.g., hardcoded secrets, dangerous commands).\n\n## 📋 Evaluation Output\nAlways output your judgment in JSON:\n{\n  "approved": boolean,\n  "score": number (0-100),\n  "issues": string[],\n  "feedback": "Concise summary of what needs to be fixed."\n}	cmkgd9c8800007f3hjc5f3jia	2026-01-16 04:16:26.445	2026-01-16 04:16:26.445	{"needsReasoning": true}
cmkgd9c9k000e7f3h2w7kr5hl	Librarian	The knowledge keeper. Organizes project structure and verifies documentation consistency.	# Librarian\nYou are the Librarian, the custodian of the project's knowledge graph.\n\n## 📚 Missions\n1. **Structure Verification**: Ensure file placements match the project architecture (e.g. "Components go in /src/components").\n2. **Documentation Check**: Verify that new features have corresponding updates in README.md or /docs.\n3. **Deduplication**: Flag potential duplicate code or conflicting definitions.\n\n## 🔍 Context Strategy\nYou use "Exploratory" context to scan the file tree and "Vector Search" to find semantic connections.	cmkgd9c8800007f3hjc5f3jia	2026-01-16 04:16:26.457	2026-01-16 04:16:26.457	{"needsReasoning": false}
cmkgd9c8v00067f3hvsx34a8k	Role Architect	Designs and evolves AI agent roles using the DNA architecture.	You are the Role Architect.\nYour mission is to design specialized AI agents (Roles) for the user's workspace.\nYou have access to the 'create_role_variant' tool to biologically spawn new agent lifeforms.\n\nWhen the user asks for a new agent:\n1. Detect Intent: Infer the Name, Description, Domain (Frontend/Backend/Creative/Research), and Complexity (LOW/MEDIUM/HIGH).\n2. DO NOT ASK FOR CLARIFICATION: Act autonomously. Make your best guess.\n3. Output ONLY a JSON tool call in this EXACT format (no markdown, no code blocks, no explanations):\n\n{\n  "tool": "system.create_role_variant",\n  "args": {\n    "intent": {\n      "name": "Role Name Here",\n      "description": "What this role does",\n      "domain": "Creative",\n      "complexity": "MEDIUM"\n    }\n  }\n}\n\n4. After the role is created, confirm to the user with a brief message.\n	cmkgd9c8800007f3hjc5f3jia	2026-01-16 04:16:26.431	2026-01-16 05:08:30.26	{"needsReasoning": true, "defaultMaxTokens": 4096, "defaultTemperature": 0.7}
\.


--
-- Data for Name: RoleAssessment; Type: TABLE DATA; Schema: public; Owner: -
--

COPY public."RoleAssessment" (id, "variantId", "taskId", metric, score, feedback, "createdAt") FROM stdin;
\.


--
-- Data for Name: RoleCategory; Type: TABLE DATA; Schema: public; Owner: -
--

COPY public."RoleCategory" (id, name, "order") FROM stdin;
cmkgd9c8800007f3hjc5f3jia	System	0
\.


--
-- Data for Name: RoleTool; Type: TABLE DATA; Schema: public; Owner: -
--

COPY public."RoleTool" ("roleId", "toolId") FROM stdin;
\.


--
-- Data for Name: RoleVariant; Type: TABLE DATA; Schema: public; Owner: -
--

COPY public."RoleVariant" (id, "roleId", "identityConfig", "cortexConfig", "contextConfig", "governanceConfig", "isActive", "createdAt", "behaviorConfig") FROM stdin;
cmkgd9c9e000c7f3hycnf0o55	cmkgd9c99000a7f3hndnpnu6o	{"style": "AGGRESSIVE_AUDITOR", "personaName": "The Gavel", "thinkingProcess": "CHAIN_OF_THOUGHT", "reflectionEnabled": true, "systemPromptDraft": "# System Judge\\nYou are the System Judge, the quality assurance engine of the Nebula ecosystem.\\nYour goal is to AUDIT the work of other agents and provide a pass/fail grade with specific feedback.\\n\\n## ⚖️ Directives\\n1. **Unbiased auditing**: You do not write code; you review it.\\n2. **Strict Guidelines**: Verify inputs against the \\"Governance Module\\" rules provided in the context.\\n3. **Security First**: immediately flag unsafe patterns (e.g., hardcoded secrets, dangerous commands).\\n\\n## 📋 Evaluation Output\\nAlways output your judgment in JSON:\\n{\\n  \\"approved\\": boolean,\\n  \\"score\\": number (0-100),\\n  \\"issues\\": string[],\\n  \\"feedback\\": \\"Concise summary of what needs to be fixed.\\"\\n}"}	{"capabilities": ["reasoning", "coding"], "contextRange": {"max": 32000, "min": 4096}}	{"strategy": ["LOCUS_FOCUS"], "permissions": ["ALL"]}	{"rules": ["Zero tolerance for security risks", "Strict adherence to TypeScript strict mode"], "enforcementLevel": "BLOCK_ON_FAIL", "assessmentStrategy": ["LINT_ONLY"]}	t	2026-01-16 04:16:26.45	{}
cmkgd9c9r000g7f3hnyecbyni	cmkgd9c9k000e7f3h2w7kr5hl	{"style": "SOCRATIC", "personaName": "Curator", "thinkingProcess": "SOLO", "reflectionEnabled": false, "systemPromptDraft": "# Librarian\\nYou are the Librarian, the custodian of the project's knowledge graph.\\n\\n## 📚 Missions\\n1. **Structure Verification**: Ensure file placements match the project architecture (e.g. \\"Components go in /src/components\\").\\n2. **Documentation Check**: Verify that new features have corresponding updates in README.md or /docs.\\n3. **Deduplication**: Flag potential duplicate code or conflicting definitions.\\n\\n## 🔍 Context Strategy\\nYou use \\"Exploratory\\" context to scan the file tree and \\"Vector Search\\" to find semantic connections."}	{"capabilities": [], "contextRange": {"max": 128000, "min": 16000}}	{"strategy": ["EXPLORATORY", "VECTOR_SEARCH"], "permissions": ["ALL"]}	{"rules": ["Maintain standard directory structure"], "enforcementLevel": "WARN_ONLY", "assessmentStrategy": ["LINT_ONLY"]}	t	2026-01-16 04:16:26.463	{}
cmkgd9c8p00047f3hhbv3xf4x	cmkgd9c8h00027f3hmqscsvfs	{"style": "PROFESSIONAL_CONCISE", "personaName": "Architect Prime", "thinkingProcess": "CHAIN_OF_THOUGHT", "reflectionEnabled": true, "systemPromptDraft": "# Nebula Architect\\n\\nYou are the master designer of the Nebula ecosystem. Your mission is to construct and refine user interfaces by writing executable TypeScript code that runs in the live Nebula runtime.\\n\\n## 🎯 Primary Directives\\n1. **CODE IS THE MEDIUM**: You do not describe UI changes; you execute them.\\n2. **ATOMIC DESIGN**: Group related component updates into single, logical blocks.\\n3. **CONTEXT FIRST**: Always verify the current state of the Nebula Tree (`tree.nodes`) before making modifications.\\n\\n## 📋 Operational Workflow\\n\\n### Phase 1: Planning (OUTSIDE code blocks)\\nBefore execution, clearly state your intent:\\n- **LOCATE**: Target parent node ID (e.g., \\"root\\" or a specific component ID).\\n- **DEFINE**: UI strategy (Grid vs Flex, Color tokens, Spacing).\\n- **EXECUTE**: Summary of the build.\\n\\n### Phase 2: Execution (INSIDE ```typescript blocks)\\nWrite ONLY clean, executable code.\\n\\n## 🛠️ Global API Reference\\n- `nebula.addNode(parentId, config)`: Returns a unique `nodeId`. **YOU MUST CAPTURE THIS ID.**\\n- `nebula.updateNode(nodeId, updates)`: Modifies an existing node.\\n- `nebula.deleteNode(nodeId)`: Removes a node.\\n- `ast.parse(jsx)`: Ingests raw JSX/TSX strings into Nebula fragments.\\n- `tree.nodes`: Read-only access to the current UI state.\\n\\n## 📦 Component Library\\n- **Primitives**: `Box`, `Text`, `Icon`, `Grid`, `Container`, `Mosaic`.\\n- **Composite**: `Card`, `Tabs`, `Navbar`, `Hero`, `Stat`, `Badge`.\\n- **Interactive**: `Button`, `Input`, `Textarea`, `Slider`, `Checkbox`.\\n- **AI-Enhanced**: `AiButton`, `SuperAiButton`.\\n\\n## ⚠️ Critical Rules\\n- ❌ NEVER use conversational filler like \\"Sure, I can do that.\\"\\n- ❌ NEVER use `tree.rootId`. Use the string `\\"root\\"`.\\n- ✅ ALWAYS capture return IDs: `const myId = nebula.addNode(...)`.\\n- ✅ ALWAYS use Tailwind classes for styling (e.g., `bg-zinc-900`, `p-4`, `gap-2`)."}	{"tools": ["nebula"], "capabilities": ["reasoning", "coding"], "contextRange": {"max": 128000, "min": 8192}, "executionMode": "CODE_INTERPRETER"}	{"strategy": ["EXPLORATORY"], "permissions": ["ALL"]}	{"rules": ["Always capture node IDs when using nebula.addNode", "Never use tree.rootId - use \\"root\\" string", "Always use Tailwind CSS classes for styling"], "enforcementLevel": "WARN_ONLY", "assessmentStrategy": ["VISUAL_CHECK"]}	t	2026-01-16 04:16:26.425	{"silenceConfirmation": true}
cmkgd9c9300087f3hiq56moqv	cmkgd9c8v00067f3hvsx34a8k	{"style": "SOCRATIC", "personaName": "DNA Synthesizer", "thinkingProcess": "MULTI_STEP_PLANNING", "reflectionEnabled": true, "systemPromptDraft": "You are the Role Architect.\\nYour mission is to design specialized AI agents (Roles) for the user's workspace.\\nYou have access to the 'create_role_variant' tool to biologically spawn new agent lifeforms.\\n\\nWhen the user asks for a new agent:\\n1. Detect Intent: Infer the Name, Description, Domain (Frontend/Backend/Creative/Research), and Complexity (LOW/MEDIUM/HIGH).\\n2. DO NOT ASK FOR CLARIFICATION: Act autonomously. Make your best guess.\\n3. Output ONLY a JSON tool call in this EXACT format (no markdown, no code blocks, no explanations):\\n\\n{\\n  \\"tool\\": \\"system.create_role_variant\\",\\n  \\"args\\": {\\n    \\"intent\\": {\\n      \\"name\\": \\"Role Name Here\\",\\n      \\"description\\": \\"What this role does\\",\\n      \\"domain\\": \\"Creative\\",\\n      \\"complexity\\": \\"MEDIUM\\"\\n    }\\n  }\\n}\\n\\n4. After the role is created, confirm to the user with a brief message.\\n"}	{"tools": ["meta"], "capabilities": ["reasoning"], "contextRange": {"max": 32000, "min": 8192}, "executionMode": "JSON_STRICT"}	{"strategy": ["EXPLORATORY"], "permissions": ["ALL"]}	{"rules": ["Always provide complete DNA configurations", "Consider domain-specific governance rules", "Match capabilities to role requirements"], "enforcementLevel": "WARN_ONLY", "assessmentStrategy": ["LINT_ONLY"]}	t	2026-01-16 04:16:26.439	{"silenceConfirmation": true}
\.


--
-- Data for Name: Tool; Type: TABLE DATA; Schema: public; Owner: -
--

COPY public."Tool" (id, name, description, instruction, schema, "isEnabled", "serverId") FROM stdin;
cmkgc36g900yt2rnb7mhadmfq	filesystem_delete_file	[MCP: filesystem] Removes a specific file. Accepts relative or absolute paths.	type DeleteFileTool = {\n  // Removes a specific file. Accepts relative or absolute paths.\n  delete_file: (args: {\n    // The path to the file to delete. Can be relative or absolute (resolved like readFile).\n    path: string;\n  }) => Promise<any>;\n};	{"type":"object","properties":{"path":{"type":"string","minLength":1,"description":"The path to the file to delete. Can be relative or absolute (resolved like readFile)."}},"required":["path"]}	t	filesystem
cmkgc36gf00yu2rnbfx1zsekf	filesystem_delete_directory	[MCP: filesystem] Removes a directory. Optionally removes recursively. Accepts relative or absolute paths.	type DeleteDirectoryTool = {\n  // Removes a directory. Optionally removes recursively. Accepts relative or absolute paths.\n  delete_directory: (args: {\n    // The path to the directory to delete. Can be relative or absolute.\n    path: string;\n    // If true, delete the directory and all its contents. If false, only delete if the directory is empty.\n    recursive?: boolean;\n  }) => Promise<any>;\n};	{"type":"object","properties":{"path":{"type":"string","minLength":1,"description":"The path to the directory to delete. Can be relative or absolute."},"recursive":{"type":"boolean","default":false,"description":"If true, delete the directory and all its contents. If false, only delete if the directory is empty."}},"required":["path"]}	t	filesystem
cmkgc36rt00ze2rnblqw2vdcf	git_remote_remove	[MCP: git] Remove a remote	type RemoteRemoveTool = {\n  // Remove a remote\n  remote_remove: (args: {\n    // Path to repository. MUST be an absolute path (e.g., /Users/username/projects/my-repo)\n    path?: string;\n    // Remote name\n    name: string;\n  }) => Promise<any>;\n};	{"type":"object","properties":{"path":{"type":"string","description":"Path to repository. MUST be an absolute path (e.g., /Users/username/projects/my-repo)"},"name":{"type":"string","description":"Remote name"}},"required":["name"]}	t	git
cmkgc36fo00yp2rnboil4b17f	filesystem_set_filesystem_default	[MCP: filesystem] Sets a default absolute path for the current session. Relative paths used in other filesystem tools (like readFile) will be resolved against this default. The default is cleared on server restart.	type SetFilesystemDefaultTool = {\n  // Sets a default absolute path for the current session. Relative paths used in other filesystem tools (like readFile) will be resolved against this default. The default is cleared on server restart.\n  set_filesystem_default: (args: {\n    // The absolute path to set as the default for resolving relative paths during this session.\n    path: string;\n  }) => Promise<any>;\n};	{"type":"object","properties":{"path":{"type":"string","minLength":1,"description":"The absolute path to set as the default for resolving relative paths during this session."}},"required":["path"]}	t	filesystem
cmkgc36q100z22rnbg0f32wvt	git_commit	[MCP: git] Create a commit	type CommitTool = {\n  // Create a commit\n  commit: (args: {\n    // Path to repository. MUST be an absolute path (e.g., /Users/username/projects/my-repo)\n    path?: string;\n    // Commit message\n    message: string;\n  }) => Promise<any>;\n};	{"type":"object","properties":{"path":{"type":"string","description":"Path to repository. MUST be an absolute path (e.g., /Users/username/projects/my-repo)"},"message":{"type":"string","description":"Commit message"}},"required":["message"]}	t	git
cmkgc36r700za2rnb79u9ubt9	git_tag_create	[MCP: git] Create a tag	type TagCreateTool = {\n  // Create a tag\n  tag_create: (args: {\n    // Path to repository. MUST be an absolute path (e.g., /Users/username/projects/my-repo)\n    path?: string;\n    // Tag name\n    name: string;\n    // Tag message\n    message?: string;\n    // Force create tag even if it exists\n    force?: boolean;\n    // Create an annotated tag\n    annotated?: boolean;\n    // Create a signed tag\n    sign?: boolean;\n  }) => Promise<any>;\n};	{"type":"object","properties":{"path":{"type":"string","description":"Path to repository. MUST be an absolute path (e.g., /Users/username/projects/my-repo)"},"name":{"type":"string","description":"Tag name"},"message":{"type":"string","description":"Tag message"},"force":{"type":"boolean","description":"Force create tag even if it exists","default":false},"annotated":{"type":"boolean","description":"Create an annotated tag","default":true},"sign":{"type":"boolean","description":"Create a signed tag","default":false}},"required":["name"]}	t	git
cmkgc36ry00zf2rnb0g9z3qpk	git_stash_list	[MCP: git] List stashes	type StashListTool = {\n  // List stashes\n  stash_list: (args: {\n    // Path to repository. MUST be an absolute path (e.g., /Users/username/projects/my-repo)\n    path?: string;\n  }) => Promise<any>;\n};	{"type":"object","properties":{"path":{"type":"string","description":"Path to repository. MUST be an absolute path (e.g., /Users/username/projects/my-repo)"}},"required":[]}	t	git
cmkgc36pj00yz2rnbby4xl278	git_clone	[MCP: git] Clone a repository	type CloneTool = {\n  // Clone a repository\n  clone: (args: {\n    // URL of the repository to clone\n    url: string;\n    // Path to clone into. MUST be an absolute path (e.g., /Users/username/projects/my-repo)\n    path?: string;\n  }) => Promise<any>;\n};	{"type":"object","properties":{"url":{"type":"string","description":"URL of the repository to clone"},"path":{"type":"string","description":"Path to clone into. MUST be an absolute path (e.g., /Users/username/projects/my-repo)"}},"required":["url"]}	t	git
cmkgc36pp00z02rnbea94etst	git_status	[MCP: git] Get repository status	type StatusTool = {\n  // Get repository status\n  status: (args: {\n    // Path to repository. MUST be an absolute path (e.g., /Users/username/projects/my-repo)\n    path?: string;\n  }) => Promise<any>;\n};	{"type":"object","properties":{"path":{"type":"string","description":"Path to repository. MUST be an absolute path (e.g., /Users/username/projects/my-repo)"}},"required":[]}	t	git
cmkgc36pv00z12rnb6krfkqp0	git_add	[MCP: git] Stage files	type AddTool = {\n  // Stage files\n  add: (args: {\n    // Path to repository. MUST be an absolute path (e.g., /Users/username/projects/my-repo)\n    path?: string;\n    // Files to stage\n    files: Array<string>;\n  }) => Promise<any>;\n};	{"type":"object","properties":{"path":{"type":"string","description":"Path to repository. MUST be an absolute path (e.g., /Users/username/projects/my-repo)"},"files":{"type":"array","items":{"type":"string","description":"MUST be an absolute path (e.g., /Users/username/projects/my-repo/src/file.js)"},"description":"Files to stage"}},"required":["files"]}	t	git
cmkgc37uw01052rnbsgru8fla	playwright_browser_click_text	[MCP: playwright] Click an element on the page by its text content	type BrowserClickTextTool = {\n  // Click an element on the page by its text content\n  browser_click_text: (args: {\n    // Text content of the element to click\n    text: string;\n  }) => Promise<any>;\n};	{"type":"object","properties":{"text":{"type":"string","description":"Text content of the element to click"}},"required":["text"]}	t	playwright
cmkgc36pd00yy2rnb1vsiwvwt	git_init	[MCP: git] Initialize a new Git repository	type InitTool = {\n  // Initialize a new Git repository\n  init: (args: {\n    // Path to initialize the repository in. MUST be an absolute path (e.g., /Users/username/projects/my-repo)\n    path?: string;\n  }) => Promise<any>;\n};	{"type":"object","properties":{"path":{"type":"string","description":"Path to initialize the repository in. MUST be an absolute path (e.g., /Users/username/projects/my-repo)"}},"required":[]}	t	git
cmkgc36g400ys2rnbdbcxwg6m	filesystem_list_files	[MCP: filesystem] Lists files and directories within the specified directory. Optionally lists recursively and returns a tree-like structure. Includes an optional `maxEntries` parameter (default 50) to limit the number of items returned.	type ListFilesTool = {\n  // Lists files and directories within the specified directory. Optionally lists recursively and returns a tree-like structure. Includes an optional `maxEntries` parameter (default 50) to limit the number of items returned.\n  list_files: (args: {\n    // The path to the directory to list. Can be relative or absolute (resolved like readFile).\n    path: string;\n    // If true, list files and directories recursively. Defaults to false (top-level only).\n    includeNested?: boolean;\n    // Maximum number of directory entries (files + folders) to return. Defaults to 50. Helps prevent excessive output for large directories.\n    maxEntries?: number;\n  }) => Promise<any>;\n};	{"type":"object","properties":{"path":{"type":"string","minLength":1,"description":"The path to the directory to list. Can be relative or absolute (resolved like readFile)."},"includeNested":{"type":"boolean","default":false,"description":"If true, list files and directories recursively. Defaults to false (top-level only)."},"maxEntries":{"type":"integer","exclusiveMinimum":0,"default":50,"description":"Maximum number of directory entries (files + folders) to return. Defaults to 50. Helps prevent excessive output for large directories."}},"required":["path"]}	t	filesystem
cmkgc36qi00z52rnbw3odjknm	git_branch_list	[MCP: git] List all branches	type BranchListTool = {\n  // List all branches\n  branch_list: (args: {\n    // Path to repository. MUST be an absolute path (e.g., /Users/username/projects/my-repo)\n    path?: string;\n  }) => Promise<any>;\n};	{"type":"object","properties":{"path":{"type":"string","description":"Path to repository. MUST be an absolute path (e.g., /Users/username/projects/my-repo)"}},"required":[]}	t	git
cmkgc36qx00z82rnbksdvxvkp	git_checkout	[MCP: git] Switch branches or restore working tree files	type CheckoutTool = {\n  // Switch branches or restore working tree files\n  checkout: (args: {\n    // Path to repository. MUST be an absolute path (e.g., /Users/username/projects/my-repo)\n    path?: string;\n    // Branch name, commit hash, or file path\n    target: string;\n  }) => Promise<any>;\n};	{"type":"object","properties":{"path":{"type":"string","description":"Path to repository. MUST be an absolute path (e.g., /Users/username/projects/my-repo)"},"target":{"type":"string","description":"Branch name, commit hash, or file path"}},"required":["target"]}	t	git
cmkgc36r200z92rnbez3l6url	git_tag_list	[MCP: git] List tags	type TagListTool = {\n  // List tags\n  tag_list: (args: {\n    // Path to repository. MUST be an absolute path (e.g., /Users/username/projects/my-repo)\n    path?: string;\n  }) => Promise<any>;\n};	{"type":"object","properties":{"path":{"type":"string","description":"Path to repository. MUST be an absolute path (e.g., /Users/username/projects/my-repo)"}},"required":[]}	t	git
cmkgc36rc00zb2rnbu54b05lp	git_tag_delete	[MCP: git] Delete a tag	type TagDeleteTool = {\n  // Delete a tag\n  tag_delete: (args: {\n    // Path to repository. MUST be an absolute path (e.g., /Users/username/projects/my-repo)\n    path?: string;\n    // Tag name\n    name: string;\n  }) => Promise<any>;\n};	{"type":"object","properties":{"path":{"type":"string","description":"Path to repository. MUST be an absolute path (e.g., /Users/username/projects/my-repo)"},"name":{"type":"string","description":"Tag name"}},"required":["name"]}	t	git
cmkgc36rh00zc2rnbrsshr1l1	git_remote_list	[MCP: git] List remotes	type RemoteListTool = {\n  // List remotes\n  remote_list: (args: {\n    // Path to repository. MUST be an absolute path (e.g., /Users/username/projects/my-repo)\n    path?: string;\n  }) => Promise<any>;\n};	{"type":"object","properties":{"path":{"type":"string","description":"Path to repository. MUST be an absolute path (e.g., /Users/username/projects/my-repo)"}},"required":[]}	t	git
cmkgc36rn00zd2rnb56qc81u5	git_remote_add	[MCP: git] Add a remote	type RemoteAddTool = {\n  // Add a remote\n  remote_add: (args: {\n    // Path to repository. MUST be an absolute path (e.g., /Users/username/projects/my-repo)\n    path?: string;\n    // Remote name\n    name: string;\n    // Remote URL\n    url: string;\n  }) => Promise<any>;\n};	{"type":"object","properties":{"path":{"type":"string","description":"Path to repository. MUST be an absolute path (e.g., /Users/username/projects/my-repo)"},"name":{"type":"string","description":"Remote name"},"url":{"type":"string","description":"Remote URL"}},"required":["name","url"]}	t	git
cmkgc36qn00z62rnbrfcpblk3	git_branch_create	[MCP: git] Create a new branch	type BranchCreateTool = {\n  // Create a new branch\n  branch_create: (args: {\n    // Path to repository. MUST be an absolute path (e.g., /Users/username/projects/my-repo)\n    path?: string;\n    // Branch name\n    name: string;\n    // Force create branch even if it exists\n    force?: boolean;\n    // Set up tracking mode\n    track?: boolean;\n    // Set upstream for push/pull\n    setUpstream?: boolean;\n  }) => Promise<any>;\n};	{"type":"object","properties":{"path":{"type":"string","description":"Path to repository. MUST be an absolute path (e.g., /Users/username/projects/my-repo)"},"name":{"type":"string","description":"Branch name"},"force":{"type":"boolean","description":"Force create branch even if it exists","default":false},"track":{"type":"boolean","description":"Set up tracking mode","default":true},"setUpstream":{"type":"boolean","description":"Set upstream for push/pull","default":false}},"required":["name"]}	t	git
cmkgc377600zl2rnb7ilg35wm	postgres_pg_manage_rls	[MCP: postgres] Manage PostgreSQL Row-Level Security - enable/disable RLS and manage policies. Examples: operation="enable" with tableName="users", operation="create_policy" with tableName, policyName, using, check	type PgManageRlsTool = {\n  // Manage PostgreSQL Row-Level Security - enable/disable RLS and manage policies. Examples: operation="enable" with tableName="users", operation="create_policy" with tableName, policyName, using, check\n  pg_manage_rls: (args: {\n    // PostgreSQL connection string (optional)\n    connectionString?: string;\n    // Operation: enable/disable RLS, create_policy, edit_policy, drop_policy, get_policies\n    operation: 'enable' | 'disable' | 'create_policy' | 'edit_policy' | 'drop_policy' | 'get_policies';\n    // Table name (required for enable/disable/create_policy/edit_policy/drop_policy, optional filter for get_policies)\n    tableName?: string;\n    // Schema name (defaults to public)\n    schema?: string;\n    // Policy name (required for create_policy/edit_policy/drop_policy)\n    policyName?: string;\n    // USING expression for policy (required for create_policy, optional for edit_policy)\n    using?: string;\n    // WITH CHECK expression for policy (optional for create_policy/edit_policy)\n    check?: string;\n    // Command the policy applies to (for create_policy)\n    command?: 'ALL' | 'SELECT' | 'INSERT' | 'UPDATE' | 'DELETE';\n    // Role the policy applies to (for create_policy)\n    role?: string;\n    // Whether to replace policy if exists (for create_policy)\n    replace?: boolean;\n    // List of roles for policy (for edit_policy)\n    roles?: Array<string>;\n    // Include IF EXISTS clause (for drop_policy)\n    ifExists?: boolean;\n  }) => Promise<any>;\n};	{"type":"object","properties":{"connectionString":{"type":"string","description":"PostgreSQL connection string (optional)"},"operation":{"type":"string","enum":["enable","disable","create_policy","edit_policy","drop_policy","get_policies"],"description":"Operation: enable/disable RLS, create_policy, edit_policy, drop_policy, get_policies"},"tableName":{"type":"string","description":"Table name (required for enable/disable/create_policy/edit_policy/drop_policy, optional filter for get_policies)"},"schema":{"type":"string","description":"Schema name (defaults to public)"},"policyName":{"type":"string","description":"Policy name (required for create_policy/edit_policy/drop_policy)"},"using":{"type":"string","description":"USING expression for policy (required for create_policy, optional for edit_policy)"},"check":{"type":"string","description":"WITH CHECK expression for policy (optional for create_policy/edit_policy)"},"command":{"type":"string","enum":["ALL","SELECT","INSERT","UPDATE","DELETE"],"description":"Command the policy applies to (for create_policy)"},"role":{"type":"string","description":"Role the policy applies to (for create_policy)"},"replace":{"type":"boolean","description":"Whether to replace policy if exists (for create_policy)"},"roles":{"type":"array","items":{"type":"string"},"description":"List of roles for policy (for edit_policy)"},"ifExists":{"type":"boolean","description":"Include IF EXISTS clause (for drop_policy)"}},"required":["operation"]}	t	postgres
cmkgc36qc00z42rnbpbfwse0w	git_pull	[MCP: git] Pull changes from remote	type PullTool = {\n  // Pull changes from remote\n  pull: (args: {\n    // Path to repository. MUST be an absolute path (e.g., /Users/username/projects/my-repo)\n    path?: string;\n    // Remote name\n    remote?: string;\n    // Branch name\n    branch: string;\n  }) => Promise<any>;\n};	{"type":"object","properties":{"path":{"type":"string","description":"Path to repository. MUST be an absolute path (e.g., /Users/username/projects/my-repo)"},"remote":{"type":"string","description":"Remote name","default":"origin"},"branch":{"type":"string","description":"Branch name"}},"required":["branch"]}	t	git
cmkgc36s900zh2rnbczjpvwtc	git_stash_pop	[MCP: git] Apply and remove a stash	type StashPopTool = {\n  // Apply and remove a stash\n  stash_pop: (args: {\n    // Path to repository. MUST be an absolute path (e.g., /Users/username/projects/my-repo)\n    path?: string;\n    // Stash index\n    index?: number;\n  }) => Promise<any>;\n};	{"type":"object","properties":{"path":{"type":"string","description":"Path to repository. MUST be an absolute path (e.g., /Users/username/projects/my-repo)"},"index":{"type":"number","description":"Stash index","default":0}},"required":[]}	t	git
cmkgc377x00zq2rnb7l4i5lio	postgres_pg_copy_between_databases	[MCP: postgres] Copy data between two databases	type PgCopyBetweenDatabasesTool = {\n  // Copy data between two databases\n  pg_copy_between_databases: (args: {\n    sourceConnectionString: string;\n    targetConnectionString: string;\n    tableName: string;\n    where?: string;\n    truncateTarget?: boolean;\n  }) => Promise<any>;\n};	{"type":"object","properties":{"sourceConnectionString":{"type":"string"},"targetConnectionString":{"type":"string"},"tableName":{"type":"string"},"where":{"type":"string"},"truncateTarget":{"type":"boolean","default":false}},"required":["sourceConnectionString","targetConnectionString","tableName"]}	t	postgres
cmkgc378800zs2rnblqfgsjfx	postgres_pg_get_setup_instructions	[MCP: postgres] Get step-by-step PostgreSQL setup instructions	type PgGetSetupInstructionsTool = {\n  // Get step-by-step PostgreSQL setup instructions\n  pg_get_setup_instructions: (args: {\n    platform: 'linux' | 'macos' | 'windows';\n    version?: string;\n    useCase?: 'development' | 'production';\n  }) => Promise<any>;\n};	{"type":"object","properties":{"platform":{"type":"string","enum":["linux","macos","windows"]},"version":{"type":"string","default":"latest"},"useCase":{"type":"string","enum":["development","production"],"default":"development"}},"required":["platform"]}	t	postgres
cmkgc37v201062rnbu21r4jjq	playwright_browser_fill	[MCP: playwright] Fill out an input field	type BrowserFillTool = {\n  // Fill out an input field\n  browser_fill: (args: {\n    // CSS selector for input field\n    selector: string;\n    // Value to fill\n    value: string;\n  }) => Promise<any>;\n};	{"type":"object","properties":{"selector":{"type":"string","description":"CSS selector for input field"},"value":{"type":"string","description":"Value to fill"}},"required":["selector","value"]}	t	playwright
cmkgc377h00zn2rnbssdd9vgp	postgres_pg_manage_schema	[MCP: postgres] Manage PostgreSQL schema - get schema info, create/alter tables, manage enums. Examples: operation="get_info" for table lists, operation="create_table" with tableName and columns, operation="get_enums" to list enums, operation="create_enum" with enumName and values	type PgManageSchemaTool = {\n  // Manage PostgreSQL schema - get schema info, create/alter tables, manage enums. Examples: operation="get_info" for table lists, operation="create_table" with tableName and columns, operation="get_enums" to list enums, operation="create_enum" with enumName and values\n  pg_manage_schema: (args: {\n    // PostgreSQL connection string (optional)\n    connectionString?: string;\n    // Operation: get_info (schema/table info), create_table (new table), alter_table (modify table), get_enums (list ENUMs), create_enum (new ENUM)\n    operation: 'get_info' | 'create_table' | 'alter_table' | 'get_enums' | 'create_enum';\n    // Table name (optional for get_info to get specific table info, required for create_table/alter_table)\n    tableName?: string;\n    // Schema name (defaults to public)\n    schema?: string;\n    // Column definitions (required for create_table)\n    columns?: Array<{\n      name: string;\n      // PostgreSQL data type\n      type: string;\n      nullable?: boolean;\n      // Default value expression\n      default?: string;\n    }>;\n    // Alter operations (required for alter_table)\n    operations?: Array<{\n      type: 'add' | 'alter' | 'drop';\n      columnName: string;\n      // PostgreSQL data type (for add/alter)\n      dataType?: string;\n      // Whether the column can be NULL (for add/alter)\n      nullable?: boolean;\n      // Default value expression (for add/alter)\n      default?: string;\n    }>;\n    // ENUM name (optional for get_enums to filter, required for create_enum)\n    enumName?: string;\n    // ENUM values (required for create_enum)\n    values?: Array<string>;\n    // Include IF NOT EXISTS clause (for create_enum)\n    ifNotExists?: boolean;\n  }) => Promise<any>;\n};	{"type":"object","properties":{"connectionString":{"type":"string","description":"PostgreSQL connection string (optional)"},"operation":{"type":"string","enum":["get_info","create_table","alter_table","get_enums","create_enum"],"description":"Operation: get_info (schema/table info), create_table (new table), alter_table (modify table), get_enums (list ENUMs), create_enum (new ENUM)"},"tableName":{"type":"string","description":"Table name (optional for get_info to get specific table info, required for create_table/alter_table)"},"schema":{"type":"string","description":"Schema name (defaults to public)"},"columns":{"type":"array","items":{"type":"object","properties":{"name":{"type":"string"},"type":{"type":"string","description":"PostgreSQL data type"},"nullable":{"type":"boolean"},"default":{"type":"string","description":"Default value expression"}},"required":["name","type"],"additionalProperties":false},"description":"Column definitions (required for create_table)"},"operations":{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["add","alter","drop"]},"columnName":{"type":"string"},"dataType":{"type":"string","description":"PostgreSQL data type (for add/alter)"},"nullable":{"type":"boolean","description":"Whether the column can be NULL (for add/alter)"},"default":{"type":"string","description":"Default value expression (for add/alter)"}},"required":["type","columnName"],"additionalProperties":false},"description":"Alter operations (required for alter_table)"},"enumName":{"type":"string","description":"ENUM name (optional for get_enums to filter, required for create_enum)"},"values":{"type":"array","items":{"type":"string"},"description":"ENUM values (required for create_enum)"},"ifNotExists":{"type":"boolean","description":"Include IF NOT EXISTS clause (for create_enum)"}},"required":["operation"]}	t	postgres
cmkgc377s00zp2rnb7uqfyzg0	postgres_pg_import_table_data	[MCP: postgres] Import data from JSON or CSV file into a table	type PgImportTableDataTool = {\n  // Import data from JSON or CSV file into a table\n  pg_import_table_data: (args: {\n    connectionString?: string;\n    tableName: string;\n    // absolute path to the file to import\n    inputPath: string;\n    truncateFirst?: boolean;\n    format?: 'json' | 'csv';\n    delimiter?: string;\n  }) => Promise<any>;\n};	{"type":"object","properties":{"connectionString":{"type":"string"},"tableName":{"type":"string"},"inputPath":{"type":"string","description":"absolute path to the file to import"},"truncateFirst":{"type":"boolean","default":false},"format":{"type":"string","enum":["json","csv"],"default":"json"},"delimiter":{"type":"string"}},"required":["tableName","inputPath"]}	t	postgres
cmkgc37ul01032rnb79p7zfz0	playwright_browser_screenshot	[MCP: playwright] Take a screenshot of the current page or a specific element	type BrowserScreenshotTool = {\n  // Take a screenshot of the current page or a specific element\n  browser_screenshot: (args: {\n    // Name for the screenshot\n    name: string;\n    // CSS selector for element to screenshot\n    selector?: string;\n    // Take a full page screenshot (default: false)\n    fullPage?: boolean;\n  }) => Promise<any>;\n};	{"type":"object","properties":{"name":{"type":"string","description":"Name for the screenshot"},"selector":{"type":"string","description":"CSS selector for element to screenshot"},"fullPage":{"type":"boolean","description":"Take a full page screenshot (default: false)","default":false}},"required":["name"]}	t	playwright
cmkgc36gq00yw2rnb3pwb6o9i	filesystem_move_path	[MCP: filesystem] Moves or renames a file or directory. Accepts relative or absolute paths for source and destination.	type MovePathTool = {\n  // Moves or renames a file or directory. Accepts relative or absolute paths for source and destination.\n  move_path: (args: {\n    // The current path of the file or directory to move. Can be relative or absolute.\n    source_path: string;\n    // The new path for the file or directory. Can be relative or absolute.\n    destination_path: string;\n  }) => Promise<any>;\n};	{"type":"object","properties":{"source_path":{"type":"string","minLength":1,"description":"The current path of the file or directory to move. Can be relative or absolute."},"destination_path":{"type":"string","minLength":1,"description":"The new path for the file or directory. Can be relative or absolute."}},"required":["source_path","destination_path"]}	t	filesystem
cmkgc36se00zi2rnbfw8a3d83	git_bulk_action	[MCP: git] Execute multiple Git operations in sequence. This is the preferred way to execute multiple operations.	type BulkActionTool = {\n  // Execute multiple Git operations in sequence. This is the preferred way to execute multiple operations.\n  bulk_action: (args: {\n    // Path to repository. MUST be an absolute path (e.g., /Users/username/projects/my-repo)\n    path?: string;\n    // Array of Git operations to execute in sequence\n    actions: Array<{}>;\n  }) => Promise<any>;\n};	{"type":"object","properties":{"path":{"type":"string","description":"Path to repository. MUST be an absolute path (e.g., /Users/username/projects/my-repo)"},"actions":{"type":"array","description":"Array of Git operations to execute in sequence","items":{"type":"object","oneOf":[{"type":"object","properties":{"type":{"const":"stage"},"files":{"type":"array","items":{"type":"string","description":"MUST be an absolute path (e.g., /Users/username/projects/my-repo/src/file.js)"},"description":"Files to stage. If not provided, stages all changes."}},"required":["type"]},{"type":"object","properties":{"type":{"const":"commit"},"message":{"type":"string","description":"Commit message"}},"required":["type","message"]},{"type":"object","properties":{"type":{"const":"push"},"remote":{"type":"string","description":"Remote name","default":"origin"},"branch":{"type":"string","description":"Branch name"}},"required":["type","branch"]}]},"minItems":1}},"required":["actions"]}	t	git
cmkgc37u801022rnb7dyex4sf	playwright_browser_navigate	[MCP: playwright] Navigate to a URL	type BrowserNavigateTool = {\n  // Navigate to a URL\n  browser_navigate: (args: {\n    url: string;\n  }) => Promise<any>;\n};	{"type":"object","properties":{"url":{"type":"string"}},"required":["url"]}	t	playwright
cmkgc37uq01042rnbirfa9wku	playwright_browser_click	[MCP: playwright] Click an element on the page using CSS selector	type BrowserClickTool = {\n  // Click an element on the page using CSS selector\n  browser_click: (args: {\n    // CSS selector for element to click\n    selector: string;\n  }) => Promise<any>;\n};	{"type":"object","properties":{"selector":{"type":"string","description":"CSS selector for element to click"}},"required":["selector"]}	t	playwright
cmkgc377m00zo2rnb3d0w72ua	postgres_pg_export_table_data	[MCP: postgres] Export table data to JSON or CSV format	type PgExportTableDataTool = {\n  // Export table data to JSON or CSV format\n  pg_export_table_data: (args: {\n    connectionString?: string;\n    tableName: string;\n    // absolute path to save the exported data\n    outputPath: string;\n    where?: string;\n    limit?: number;\n    format?: 'json' | 'csv';\n  }) => Promise<any>;\n};	{"type":"object","properties":{"connectionString":{"type":"string"},"tableName":{"type":"string"},"outputPath":{"type":"string","description":"absolute path to save the exported data"},"where":{"type":"string"},"limit":{"type":"integer","exclusiveMinimum":0},"format":{"type":"string","enum":["json","csv"],"default":"json"}},"required":["tableName","outputPath"]}	t	postgres
cmkgc379h01002rnbqtpxy6u1	postgres_pg_execute_sql	[MCP: postgres] Execute arbitrary SQL statements - sql="ANY_VALID_SQL" with optional parameters and transaction support. Examples: sql="CREATE INDEX ...", sql="WITH complex_cte AS (...) SELECT ...", transactional=true	type PgExecuteSqlTool = {\n  // Execute arbitrary SQL statements - sql="ANY_VALID_SQL" with optional parameters and transaction support. Examples: sql="CREATE INDEX ...", sql="WITH complex_cte AS (...) SELECT ...", transactional=true\n  pg_execute_sql: (args: {\n    // PostgreSQL connection string (optional)\n    connectionString?: string;\n    // SQL statement to execute (can be any valid PostgreSQL SQL)\n    sql: string;\n    // Parameter values for prepared statement placeholders ($1, $2, etc.)\n    parameters?: Array<any>;\n    // Whether to expect rows back (false for statements like CREATE, DROP, etc.)\n    expectRows?: boolean;\n    // Query timeout in milliseconds\n    timeout?: number;\n    // Whether to wrap in a transaction\n    transactional?: boolean;\n  }) => Promise<any>;\n};	{"type":"object","properties":{"connectionString":{"type":"string","description":"PostgreSQL connection string (optional)"},"sql":{"type":"string","description":"SQL statement to execute (can be any valid PostgreSQL SQL)"},"parameters":{"type":"array","items":{},"default":[],"description":"Parameter values for prepared statement placeholders ($1, $2, etc.)"},"expectRows":{"type":"boolean","default":true,"description":"Whether to expect rows back (false for statements like CREATE, DROP, etc.)"},"timeout":{"type":"number","description":"Query timeout in milliseconds"},"transactional":{"type":"boolean","default":false,"description":"Whether to wrap in a transaction"}},"required":["sql"]}	t	postgres
cmkgc37vf01082rnbb4l0xzhd	playwright_browser_select_text	[MCP: playwright] Select an element on the page with Select tag by its text content	type BrowserSelectTextTool = {\n  // Select an element on the page with Select tag by its text content\n  browser_select_text: (args: {\n    // Text content of the element to select\n    text: string;\n    // Value to select\n    value: string;\n  }) => Promise<any>;\n};	{"type":"object","properties":{"text":{"type":"string","description":"Text content of the element to select"},"value":{"type":"string","description":"Value to select"}},"required":["text","value"]}	t	playwright
cmkgc37vk01092rnb810wk48o	playwright_browser_hover	[MCP: playwright] Hover an element on the page using CSS selector	type BrowserHoverTool = {\n  // Hover an element on the page using CSS selector\n  browser_hover: (args: {\n    // CSS selector for element to hover\n    selector: string;\n  }) => Promise<any>;\n};	{"type":"object","properties":{"selector":{"type":"string","description":"CSS selector for element to hover"}},"required":["selector"]}	t	playwright
cmkgc37vq010a2rnb9xu2vuee	playwright_browser_hover_text	[MCP: playwright] Hover an element on the page by its text content	type BrowserHoverTextTool = {\n  // Hover an element on the page by its text content\n  browser_hover_text: (args: {\n    // Text content of the element to hover\n    text: string;\n  }) => Promise<any>;\n};	{"type":"object","properties":{"text":{"type":"string","description":"Text content of the element to hover"}},"required":["text"]}	t	playwright
cmkgc37vv010b2rnbqno2qu5z	playwright_browser_evaluate	[MCP: playwright] Execute JavaScript in the browser console	type BrowserEvaluateTool = {\n  // Execute JavaScript in the browser console\n  browser_evaluate: (args: {\n    // JavaScript code to execute\n    script: string;\n  }) => Promise<any>;\n};	{"type":"object","properties":{"script":{"type":"string","description":"JavaScript code to execute"}},"required":["script"]}	t	playwright
cmkgc377b00zm2rnbmu2jm8kf	postgres_pg_debug_database	[MCP: postgres] Debug common PostgreSQL issues	type PgDebugDatabaseTool = {\n  // Debug common PostgreSQL issues\n  pg_debug_database: (args: {\n    connectionString?: string;\n    issue: 'connection' | 'performance' | 'locks' | 'replication';\n    logLevel?: 'info' | 'debug' | 'trace';\n  }) => Promise<any>;\n};	{"type":"object","properties":{"connectionString":{"type":"string"},"issue":{"type":"string","enum":["connection","performance","locks","replication"]},"logLevel":{"type":"string","enum":["info","debug","trace"],"default":"info"}},"required":["issue"]}	t	postgres
cmkgc36ff00yo2rnbeqtnf2n7	filesystem_read_file	[MCP: filesystem] Reads the entire content of a specified file as UTF-8 text. Accepts relative or absolute paths. Relative paths are resolved against the session default set by `set_filesystem_default`.	type ReadFileTool = {\n  // Reads the entire content of a specified file as UTF-8 text. Accepts relative or absolute paths. Relative paths are resolved against the session default set by `set_filesystem_default`.\n  read_file: (args: {\n    // The path to the file to read. Can be relative or absolute. If relative, it resolves against the path set by `set_filesystem_default`. If absolute, it is used directly. If relative and no default is set, an error occurs.\n    path: string;\n  }) => Promise<any>;\n};	{"type":"object","properties":{"path":{"type":"string","minLength":1,"description":"The path to the file to read. Can be relative or absolute. If relative, it resolves against the path set by `set_filesystem_default`. If absolute, it is used directly. If relative and no default is set, an error occurs."}},"required":["path"]}	t	filesystem
cmkgc36s300zg2rnbcwnasgi6	git_stash_save	[MCP: git] Save changes to stash	type StashSaveTool = {\n  // Save changes to stash\n  stash_save: (args: {\n    // Path to repository. MUST be an absolute path (e.g., /Users/username/projects/my-repo)\n    path?: string;\n    // Stash message\n    message?: string;\n    // Include untracked files\n    includeUntracked?: boolean;\n    // Keep staged changes\n    keepIndex?: boolean;\n    // Include ignored files\n    all?: boolean;\n  }) => Promise<any>;\n};	{"type":"object","properties":{"path":{"type":"string","description":"Path to repository. MUST be an absolute path (e.g., /Users/username/projects/my-repo)"},"message":{"type":"string","description":"Stash message"},"includeUntracked":{"type":"boolean","description":"Include untracked files","default":false},"keepIndex":{"type":"boolean","description":"Keep staged changes","default":false},"all":{"type":"boolean","description":"Include ignored files","default":false}},"required":[]}	t	git
cmkgc36fz00yr2rnb6op3all2	filesystem_update_file	[MCP: filesystem] Performs targeted search-and-replace operations within an existing file using an array of {search, replace} blocks. Preferred for smaller, localized changes. For large-scale updates or overwrites, consider using `write_file`. Accepts relative or absolute paths. File must exist. Supports optional `useRegex` (boolean, default false) and `replaceAll` (boolean, default false).	type UpdateFileTool = {\n  // Performs targeted search-and-replace operations within an existing file using an array of {search, replace} blocks. Preferred for smaller, localized changes. For large-scale updates or overwrites, consider using `write_file`. Accepts relative or absolute paths. File must exist. Supports optional `useRegex` (boolean, default false) and `replaceAll` (boolean, default false).\n  update_file: (args: {\n    // The path to the file to update. Can be relative or absolute (resolved like readFile). The file must exist.\n    path: string;\n    // An array of objects, each with a `search` (string) and `replace` (string) property.\n    blocks: Array<{\n      search: string;\n      replace: string;\n    }>;\n    // If true, treat the `search` field of each block as a JavaScript regular expression pattern. Defaults to false (exact string matching).\n    useRegex?: boolean;\n    // If true, replace all occurrences matching the SEARCH criteria within the file. If false, only replace the first occurrence. Defaults to false.\n    replaceAll?: boolean;\n  }) => Promise<any>;\n};	{"type":"object","properties":{"path":{"type":"string","minLength":1,"description":"The path to the file to update. Can be relative or absolute (resolved like readFile). The file must exist."},"blocks":{"type":"array","items":{"type":"object","properties":{"search":{"type":"string","minLength":1},"replace":{"type":"string"}},"required":["search","replace"],"additionalProperties":false},"minItems":1,"description":"An array of objects, each with a `search` (string) and `replace` (string) property."},"useRegex":{"type":"boolean","default":false,"description":"If true, treat the `search` field of each block as a JavaScript regular expression pattern. Defaults to false (exact string matching)."},"replaceAll":{"type":"boolean","default":false,"description":"If true, replace all occurrences matching the SEARCH criteria within the file. If false, only replace the first occurrence. Defaults to false."}},"required":["path","blocks"]}	t	filesystem
cmkgc378v00zw2rnbn1s7m5ee	postgres_pg_manage_users	[MCP: postgres] Manage PostgreSQL users and permissions - create, drop, alter users, grant/revoke permissions. Examples: operation="create" with username="testuser", operation="grant" with username, permissions, target, targetType	type PgManageUsersTool = {\n  // Manage PostgreSQL users and permissions - create, drop, alter users, grant/revoke permissions. Examples: operation="create" with username="testuser", operation="grant" with username, permissions, target, targetType\n  pg_manage_users: (args: {\n    // PostgreSQL connection string (optional)\n    connectionString?: string;\n    // Operation: create (new user), drop (remove user), alter (modify user), grant (permissions), revoke (permissions), get_permissions (view permissions), list (all users)\n    operation: 'create' | 'drop' | 'alter' | 'grant' | 'revoke' | 'get_permissions' | 'list';\n    // Username (required for create/drop/alter/grant/revoke/get_permissions, optional filter for list)\n    username?: string;\n    // Password for the user (for create operation)\n    password?: string;\n    // Grant superuser privileges (for create/alter operations)\n    superuser?: boolean;\n    // Allow user to create databases (for create/alter operations)\n    createdb?: boolean;\n    // Allow user to create roles (for create/alter operations)\n    createrole?: boolean;\n    // Allow user to login (for create/alter operations)\n    login?: boolean;\n    // Allow replication privileges (for create/alter operations)\n    replication?: boolean;\n    // Maximum number of connections (for create/alter operations)\n    connectionLimit?: number;\n    // Password expiration date YYYY-MM-DD (for create/alter operations)\n    validUntil?: string;\n    // Inherit privileges from parent roles (for create/alter operations)\n    inherit?: boolean;\n    // Include IF EXISTS clause (for drop operation)\n    ifExists?: boolean;\n    // Include CASCADE to drop owned objects (for drop/revoke operations)\n    cascade?: boolean;\n    // Permissions to grant/revoke: ["SELECT", "INSERT", "UPDATE", "DELETE", "TRUNCATE", "REFERENCES", "TRIGGER", "ALL"]\n    permissions?: Array<'SELECT' | 'INSERT' | 'UPDATE' | 'DELETE' | 'TRUNCATE' | 'REFERENCES' | 'TRIGGER' | 'ALL'>;\n    // Target object name (for grant/revoke operations)\n    target?: string;\n    // Type of target object (for grant/revoke operations)\n    targetType?: 'table' | 'schema' | 'database' | 'sequence' | 'function';\n    // Allow user to grant these permissions to others (for grant operation)\n    withGrantOption?: boolean;\n    // Filter by schema (for get_permissions operation)\n    schema?: string;\n    // Include system roles (for list operation)\n    includeSystemRoles?: boolean;\n  }) => Promise<any>;\n};	{"type":"object","properties":{"connectionString":{"type":"string","description":"PostgreSQL connection string (optional)"},"operation":{"type":"string","enum":["create","drop","alter","grant","revoke","get_permissions","list"],"description":"Operation: create (new user), drop (remove user), alter (modify user), grant (permissions), revoke (permissions), get_permissions (view permissions), list (all users)"},"username":{"type":"string","description":"Username (required for create/drop/alter/grant/revoke/get_permissions, optional filter for list)"},"password":{"type":"string","description":"Password for the user (for create operation)"},"superuser":{"type":"boolean","description":"Grant superuser privileges (for create/alter operations)"},"createdb":{"type":"boolean","description":"Allow user to create databases (for create/alter operations)"},"createrole":{"type":"boolean","description":"Allow user to create roles (for create/alter operations)"},"login":{"type":"boolean","description":"Allow user to login (for create/alter operations)"},"replication":{"type":"boolean","description":"Allow replication privileges (for create/alter operations)"},"connectionLimit":{"type":"number","description":"Maximum number of connections (for create/alter operations)"},"validUntil":{"type":"string","description":"Password expiration date YYYY-MM-DD (for create/alter operations)"},"inherit":{"type":"boolean","description":"Inherit privileges from parent roles (for create/alter operations)"},"ifExists":{"type":"boolean","description":"Include IF EXISTS clause (for drop operation)"},"cascade":{"type":"boolean","description":"Include CASCADE to drop owned objects (for drop/revoke operations)"},"permissions":{"type":"array","items":{"type":"string","enum":["SELECT","INSERT","UPDATE","DELETE","TRUNCATE","REFERENCES","TRIGGER","ALL"]},"description":"Permissions to grant/revoke: [\\"SELECT\\", \\"INSERT\\", \\"UPDATE\\", \\"DELETE\\", \\"TRUNCATE\\", \\"REFERENCES\\", \\"TRIGGER\\", \\"ALL\\"]"},"target":{"type":"string","description":"Target object name (for grant/revoke operations)"},"targetType":{"type":"string","enum":["table","schema","database","sequence","function"],"description":"Type of target object (for grant/revoke operations)"},"withGrantOption":{"type":"boolean","description":"Allow user to grant these permissions to others (for grant operation)"},"schema":{"type":"string","description":"Filter by schema (for get_permissions operation)"},"includeSystemRoles":{"type":"boolean","description":"Include system roles (for list operation)"}},"required":["operation"]}	t	postgres
cmkgc378e00zt2rnblnvdvg75	postgres_pg_manage_triggers	[MCP: postgres] Manage PostgreSQL triggers - get, create, drop, and enable/disable triggers. Examples: operation="get" to list triggers, operation="create" with triggerName, tableName, functionName, operation="drop" with triggerName and tableName, operation="set_state" with triggerName, tableName, enable	type PgManageTriggersTool = {\n  // Manage PostgreSQL triggers - get, create, drop, and enable/disable triggers. Examples: operation="get" to list triggers, operation="create" with triggerName, tableName, functionName, operation="drop" with triggerName and tableName, operation="set_state" with triggerName, tableName, enable\n  pg_manage_triggers: (args: {\n    // PostgreSQL connection string (optional)\n    connectionString?: string;\n    // Operation: get (list triggers), create (new trigger), drop (remove trigger), set_state (enable/disable trigger)\n    operation: 'get' | 'create' | 'drop' | 'set_state';\n    // Schema name (defaults to public)\n    schema?: string;\n    // Table name (optional filter for get, required for create/drop/set_state)\n    tableName?: string;\n    // Trigger name (required for create/drop/set_state)\n    triggerName?: string;\n    // Function name (required for create operation)\n    functionName?: string;\n    // Trigger timing (for create operation, defaults to AFTER)\n    timing?: 'BEFORE' | 'AFTER' | 'INSTEAD OF';\n    // Trigger events (for create operation, defaults to ["INSERT"])\n    events?: Array<'INSERT' | 'UPDATE' | 'DELETE' | 'TRUNCATE'>;\n    // FOR EACH ROW or STATEMENT (for create operation, defaults to ROW)\n    forEach?: 'ROW' | 'STATEMENT';\n    // WHEN clause condition (for create operation)\n    when?: string;\n    // Whether to replace trigger if exists (for create operation)\n    replace?: boolean;\n    // Include IF EXISTS clause (for drop operation)\n    ifExists?: boolean;\n    // Include CASCADE clause (for drop operation)\n    cascade?: boolean;\n    // Whether to enable the trigger (required for set_state operation)\n    enable?: boolean;\n  }) => Promise<any>;\n};	{"type":"object","properties":{"connectionString":{"type":"string","description":"PostgreSQL connection string (optional)"},"operation":{"type":"string","enum":["get","create","drop","set_state"],"description":"Operation: get (list triggers), create (new trigger), drop (remove trigger), set_state (enable/disable trigger)"},"schema":{"type":"string","description":"Schema name (defaults to public)"},"tableName":{"type":"string","description":"Table name (optional filter for get, required for create/drop/set_state)"},"triggerName":{"type":"string","description":"Trigger name (required for create/drop/set_state)"},"functionName":{"type":"string","description":"Function name (required for create operation)"},"timing":{"type":"string","enum":["BEFORE","AFTER","INSTEAD OF"],"description":"Trigger timing (for create operation, defaults to AFTER)"},"events":{"type":"array","items":{"type":"string","enum":["INSERT","UPDATE","DELETE","TRUNCATE"]},"description":"Trigger events (for create operation, defaults to [\\"INSERT\\"])"},"forEach":{"type":"string","enum":["ROW","STATEMENT"],"description":"FOR EACH ROW or STATEMENT (for create operation, defaults to ROW)"},"when":{"type":"string","description":"WHEN clause condition (for create operation)"},"replace":{"type":"boolean","description":"Whether to replace trigger if exists (for create operation)"},"ifExists":{"type":"boolean","description":"Include IF EXISTS clause (for drop operation)"},"cascade":{"type":"boolean","description":"Include CASCADE clause (for drop operation)"},"enable":{"type":"boolean","description":"Whether to enable the trigger (required for set_state operation)"}},"required":["operation"]}	t	postgres
cmkgc37v901072rnbshubotf8	playwright_browser_select	[MCP: playwright] Select an element on the page with Select tag using CSS selector	type BrowserSelectTool = {\n  // Select an element on the page with Select tag using CSS selector\n  browser_select: (args: {\n    // CSS selector for element to select\n    selector: string;\n    // Value to select\n    value: string;\n  }) => Promise<any>;\n};	{"type":"object","properties":{"selector":{"type":"string","description":"CSS selector for element to select"},"value":{"type":"string","description":"Value to select"}},"required":["selector","value"]}	t	playwright
cmkgc36ft00yq2rnbb1n68fwm	filesystem_write_file	[MCP: filesystem] Writes content to a specified file. Creates the file (and necessary directories) if it doesn't exist, or overwrites it if it does. Accepts relative or absolute paths (resolved like readFile).	type WriteFileTool = {\n  // Writes content to a specified file. Creates the file (and necessary directories) if it doesn't exist, or overwrites it if it does. Accepts relative or absolute paths (resolved like readFile).\n  write_file: (args: {\n    // The path to the file to write. Can be relative or absolute. If relative, it resolves against the path set by `set_filesystem_default`. If absolute, it is used directly. Missing directories will be created.\n    path: string;\n    // The content to write to the file. If the file exists, it will be overwritten.\n    content: string;\n  }) => Promise<any>;\n};	{"type":"object","properties":{"path":{"type":"string","minLength":1,"description":"The path to the file to write. Can be relative or absolute. If relative, it resolves against the path set by `set_filesystem_default`. If absolute, it is used directly. Missing directories will be created."},"content":{"type":"string","description":"The content to write to the file. If the file exists, it will be overwritten."}},"required":["path","content"]}	t	filesystem
cmkgc378300zr2rnb1or6imfg	postgres_pg_monitor_database	[MCP: postgres] Get real-time monitoring information for a PostgreSQL database	type PgMonitorDatabaseTool = {\n  // Get real-time monitoring information for a PostgreSQL database\n  pg_monitor_database: (args: {\n    connectionString?: string;\n    includeTables?: boolean;\n    includeQueries?: boolean;\n    includeLocks?: boolean;\n    includeReplication?: boolean;\n    // Alert thresholds\n    alertThresholds?: {\n      // Connection usage percentage threshold\n      connectionPercentage?: number;\n      // Long-running query threshold in seconds\n      longRunningQuerySeconds?: number;\n      // Cache hit ratio threshold\n      cacheHitRatio?: number;\n      // Dead tuples percentage threshold\n      deadTuplesPercentage?: number;\n      // Vacuum age threshold in days\n      vacuumAge?: number;\n    };\n  }) => Promise<any>;\n};	{"type":"object","properties":{"connectionString":{"type":"string"},"includeTables":{"type":"boolean","default":false},"includeQueries":{"type":"boolean","default":false},"includeLocks":{"type":"boolean","default":false},"includeReplication":{"type":"boolean","default":false},"alertThresholds":{"type":"object","properties":{"connectionPercentage":{"type":"number","minimum":0,"maximum":100,"description":"Connection usage percentage threshold"},"longRunningQuerySeconds":{"type":"number","exclusiveMinimum":0,"description":"Long-running query threshold in seconds"},"cacheHitRatio":{"type":"number","minimum":0,"maximum":1,"description":"Cache hit ratio threshold"},"deadTuplesPercentage":{"type":"number","minimum":0,"maximum":100,"description":"Dead tuples percentage threshold"},"vacuumAge":{"type":"integer","exclusiveMinimum":0,"description":"Vacuum age threshold in days"}},"additionalProperties":false,"description":"Alert thresholds"}}}	t	postgres
cmkgc379600zy2rnbmm7wgj61	postgres_pg_execute_query	[MCP: postgres] Execute SELECT queries and data retrieval operations - operation="select/count/exists" with query and optional parameters. Examples: operation="select", query="SELECT * FROM users WHERE created_at > $1", parameters=["2024-01-01"]	type PgExecuteQueryTool = {\n  // Execute SELECT queries and data retrieval operations - operation="select/count/exists" with query and optional parameters. Examples: operation="select", query="SELECT * FROM users WHERE created_at > $1", parameters=["2024-01-01"]\n  pg_execute_query: (args: {\n    // PostgreSQL connection string (optional)\n    connectionString?: string;\n    // Query operation: select (fetch rows), count (count rows), exists (check existence)\n    operation: 'select' | 'count' | 'exists';\n    // SQL SELECT query to execute\n    query: string;\n    // Parameter values for prepared statement placeholders ($1, $2, etc.)\n    parameters?: Array<any>;\n    // Maximum number of rows to return (safety limit)\n    limit?: number;\n    // Query timeout in milliseconds\n    timeout?: number;\n  }) => Promise<any>;\n};	{"type":"object","properties":{"connectionString":{"type":"string","description":"PostgreSQL connection string (optional)"},"operation":{"type":"string","enum":["select","count","exists"],"description":"Query operation: select (fetch rows), count (count rows), exists (check existence)"},"query":{"type":"string","description":"SQL SELECT query to execute"},"parameters":{"type":"array","items":{},"default":[],"description":"Parameter values for prepared statement placeholders ($1, $2, etc.)"},"limit":{"type":"number","description":"Maximum number of rows to return (safety limit)"},"timeout":{"type":"number","description":"Query timeout in milliseconds"}},"required":["operation","query"]}	t	postgres
cmkgc379c00zz2rnbb4s9tpmv	postgres_pg_execute_mutation	[MCP: postgres] Execute data modification operations (INSERT/UPDATE/DELETE/UPSERT) - operation="insert/update/delete/upsert" with table and data. Examples: operation="insert", table="users", data={"name":"John","email":"john@example.com"}	type PgExecuteMutationTool = {\n  // Execute data modification operations (INSERT/UPDATE/DELETE/UPSERT) - operation="insert/update/delete/upsert" with table and data. Examples: operation="insert", table="users", data={"name":"John","email":"john@example.com"}\n  pg_execute_mutation: (args: {\n    // PostgreSQL connection string (optional)\n    connectionString?: string;\n    // Mutation operation: insert (add rows), update (modify rows), delete (remove rows), upsert (insert or update)\n    operation: 'insert' | 'update' | 'delete' | 'upsert';\n    // Table name for the operation\n    table: string;\n    // Data object with column-value pairs (required for insert/update/upsert)\n    data?: {};\n    // WHERE clause for update/delete operations (without WHERE keyword)\n    where?: string;\n    // Columns for conflict resolution in upsert (ON CONFLICT)\n    conflictColumns?: Array<string>;\n    // RETURNING clause to get back inserted/updated data\n    returning?: string;\n    // Schema name (defaults to public)\n    schema?: string;\n  }) => Promise<any>;\n};	{"type":"object","properties":{"connectionString":{"type":"string","description":"PostgreSQL connection string (optional)"},"operation":{"type":"string","enum":["insert","update","delete","upsert"],"description":"Mutation operation: insert (add rows), update (modify rows), delete (remove rows), upsert (insert or update)"},"table":{"type":"string","description":"Table name for the operation"},"data":{"type":"object","additionalProperties":{},"description":"Data object with column-value pairs (required for insert/update/upsert)"},"where":{"type":"string","description":"WHERE clause for update/delete operations (without WHERE keyword)"},"conflictColumns":{"type":"array","items":{"type":"string"},"description":"Columns for conflict resolution in upsert (ON CONFLICT)"},"returning":{"type":"string","description":"RETURNING clause to get back inserted/updated data"},"schema":{"type":"string","default":"public","description":"Schema name (defaults to public)"}},"required":["operation","table"]}	t	postgres
cmkgc36gv00yx2rnbm9xzkv5i	filesystem_copy_path	[MCP: filesystem] Copies a file or directory to a new location. Accepts relative or absolute paths. Defaults to recursive copy for directories.	type CopyPathTool = {\n  // Copies a file or directory to a new location. Accepts relative or absolute paths. Defaults to recursive copy for directories.\n  copy_path: (args: {\n    // The path of the file or directory to copy. Can be relative or absolute.\n    source_path: string;\n    // The path where the copy should be created. Can be relative or absolute.\n    destination_path: string;\n    // If copying a directory, whether to copy its contents recursively. Defaults to true.\n    recursive?: boolean;\n  }) => Promise<any>;\n};	{"type":"object","properties":{"source_path":{"type":"string","minLength":1,"description":"The path of the file or directory to copy. Can be relative or absolute."},"destination_path":{"type":"string","minLength":1,"description":"The path where the copy should be created. Can be relative or absolute."},"recursive":{"type":"boolean","default":true,"description":"If copying a directory, whether to copy its contents recursively. Defaults to true."}},"required":["source_path","destination_path"]}	t	filesystem
cmkgc378p00zv2rnb81pcpmk9	postgres_pg_manage_query	[MCP: postgres] Manage PostgreSQL query analysis and performance - operation="explain" for EXPLAIN plans, operation="get_slow_queries" for slow query analysis, operation="get_stats" for query statistics, operation="reset_stats" for clearing statistics	type PgManageQueryTool = {\n  // Manage PostgreSQL query analysis and performance - operation="explain" for EXPLAIN plans, operation="get_slow_queries" for slow query analysis, operation="get_stats" for query statistics, operation="reset_stats" for clearing statistics\n  pg_manage_query: (args: {\n    // Operation: explain (EXPLAIN/EXPLAIN ANALYZE query), get_slow_queries (find slow queries from pg_stat_statements), get_stats (query statistics with cache hit ratios), reset_stats (reset pg_stat_statements)\n    operation: 'explain' | 'get_slow_queries' | 'get_stats' | 'reset_stats';\n    connectionString?: string;\n    // SQL query to explain (required for explain operation)\n    query?: string;\n    // Use EXPLAIN ANALYZE - actually executes the query (for explain operation)\n    analyze?: boolean;\n    // Include buffer usage information (for explain operation)\n    buffers?: boolean;\n    // Include verbose output (for explain operation)\n    verbose?: boolean;\n    // Include cost estimates (for explain operation)\n    costs?: boolean;\n    // Output format (for explain operation)\n    format?: 'text' | 'json' | 'xml' | 'yaml';\n    // Number of slow queries to return (for get_slow_queries operation)\n    limit?: number;\n    // Minimum average duration in milliseconds (for get_slow_queries operation)\n    minDuration?: number;\n    // Sort order (for get_slow_queries and get_stats operations)\n    orderBy?: 'mean_time' | 'total_time' | 'calls' | 'cache_hit_ratio';\n    // Include normalized query text (for get_slow_queries operation)\n    includeNormalized?: boolean;\n    // Minimum number of calls (for get_stats operation)\n    minCalls?: number;\n    // Filter queries containing this pattern (for get_stats operation)\n    queryPattern?: string;\n    // Specific query ID to reset (for reset_stats operation, resets all if not provided)\n    queryId?: string;\n  }) => Promise<any>;\n};	{"type":"object","properties":{"operation":{"type":"string","enum":["explain","get_slow_queries","get_stats","reset_stats"],"description":"Operation: explain (EXPLAIN/EXPLAIN ANALYZE query), get_slow_queries (find slow queries from pg_stat_statements), get_stats (query statistics with cache hit ratios), reset_stats (reset pg_stat_statements)"},"connectionString":{"type":"string"},"query":{"type":"string","description":"SQL query to explain (required for explain operation)"},"analyze":{"type":"boolean","default":false,"description":"Use EXPLAIN ANALYZE - actually executes the query (for explain operation)"},"buffers":{"type":"boolean","default":false,"description":"Include buffer usage information (for explain operation)"},"verbose":{"type":"boolean","default":false,"description":"Include verbose output (for explain operation)"},"costs":{"type":"boolean","default":true,"description":"Include cost estimates (for explain operation)"},"format":{"type":"string","enum":["text","json","xml","yaml"],"default":"json","description":"Output format (for explain operation)"},"limit":{"type":"number","default":10,"description":"Number of slow queries to return (for get_slow_queries operation)"},"minDuration":{"type":"number","description":"Minimum average duration in milliseconds (for get_slow_queries operation)"},"orderBy":{"type":"string","enum":["mean_time","total_time","calls","cache_hit_ratio"],"default":"mean_time","description":"Sort order (for get_slow_queries and get_stats operations)"},"includeNormalized":{"type":"boolean","default":true,"description":"Include normalized query text (for get_slow_queries operation)"},"minCalls":{"type":"number","description":"Minimum number of calls (for get_stats operation)"},"queryPattern":{"type":"string","description":"Filter queries containing this pattern (for get_stats operation)"},"queryId":{"type":"string","description":"Specific query ID to reset (for reset_stats operation, resets all if not provided)"}},"required":["operation"]}	t	postgres
cmkgc379100zx2rnblmz6dz46	postgres_pg_manage_constraints	[MCP: postgres] Manage PostgreSQL constraints - get, create foreign keys, drop foreign keys, create constraints, drop constraints. Examples: operation="get" to list constraints, operation="create_fk" with constraintName, tableName, columnNames, referencedTable, referencedColumns	type PgManageConstraintsTool = {\n  // Manage PostgreSQL constraints - get, create foreign keys, drop foreign keys, create constraints, drop constraints. Examples: operation="get" to list constraints, operation="create_fk" with constraintName, tableName, columnNames, referencedTable, referencedColumns\n  pg_manage_constraints: (args: {\n    // PostgreSQL connection string (optional)\n    connectionString?: string;\n    // Operation: get (list constraints), create_fk (foreign key), drop_fk (drop foreign key), create (constraint), drop (constraint)\n    operation: 'get' | 'create_fk' | 'drop_fk' | 'create' | 'drop';\n    // Schema name (defaults to public)\n    schema?: string;\n    // Constraint name (required for create_fk/drop_fk/create/drop)\n    constraintName?: string;\n    // Table name (optional filter for get, required for create_fk/drop_fk/create/drop)\n    tableName?: string;\n    // Filter by constraint type (for get operation)\n    constraintType?: 'PRIMARY KEY' | 'FOREIGN KEY' | 'UNIQUE' | 'CHECK';\n    // Column names in the table (required for create_fk)\n    columnNames?: Array<string>;\n    // Referenced table name (required for create_fk)\n    referencedTable?: string;\n    // Referenced column names (required for create_fk)\n    referencedColumns?: Array<string>;\n    // Referenced table schema (for create_fk, defaults to same as table schema)\n    referencedSchema?: string;\n    // ON UPDATE action (for create_fk)\n    onUpdate?: 'NO ACTION' | 'RESTRICT' | 'CASCADE' | 'SET NULL' | 'SET DEFAULT';\n    // ON DELETE action (for create_fk)\n    onDelete?: 'NO ACTION' | 'RESTRICT' | 'CASCADE' | 'SET NULL' | 'SET DEFAULT';\n    // Type of constraint to create (for create operation)\n    constraintTypeCreate?: 'unique' | 'check' | 'primary_key';\n    // Check expression (for create operation with check constraints)\n    checkExpression?: string;\n    // Make constraint deferrable (for create_fk/create operations)\n    deferrable?: boolean;\n    // Initially deferred (for create_fk/create operations)\n    initiallyDeferred?: boolean;\n    // Include IF EXISTS clause (for drop_fk/drop operations)\n    ifExists?: boolean;\n    // Include CASCADE clause (for drop_fk/drop operations)\n    cascade?: boolean;\n  }) => Promise<any>;\n};	{"type":"object","properties":{"connectionString":{"type":"string","description":"PostgreSQL connection string (optional)"},"operation":{"type":"string","enum":["get","create_fk","drop_fk","create","drop"],"description":"Operation: get (list constraints), create_fk (foreign key), drop_fk (drop foreign key), create (constraint), drop (constraint)"},"schema":{"type":"string","description":"Schema name (defaults to public)"},"constraintName":{"type":"string","description":"Constraint name (required for create_fk/drop_fk/create/drop)"},"tableName":{"type":"string","description":"Table name (optional filter for get, required for create_fk/drop_fk/create/drop)"},"constraintType":{"type":"string","enum":["PRIMARY KEY","FOREIGN KEY","UNIQUE","CHECK"],"description":"Filter by constraint type (for get operation)"},"columnNames":{"type":"array","items":{"type":"string"},"description":"Column names in the table (required for create_fk)"},"referencedTable":{"type":"string","description":"Referenced table name (required for create_fk)"},"referencedColumns":{"type":"array","items":{"type":"string"},"description":"Referenced column names (required for create_fk)"},"referencedSchema":{"type":"string","description":"Referenced table schema (for create_fk, defaults to same as table schema)"},"onUpdate":{"type":"string","enum":["NO ACTION","RESTRICT","CASCADE","SET NULL","SET DEFAULT"],"description":"ON UPDATE action (for create_fk)"},"onDelete":{"type":"string","enum":["NO ACTION","RESTRICT","CASCADE","SET NULL","SET DEFAULT"],"description":"ON DELETE action (for create_fk)"},"constraintTypeCreate":{"type":"string","enum":["unique","check","primary_key"],"description":"Type of constraint to create (for create operation)"},"checkExpression":{"type":"string","description":"Check expression (for create operation with check constraints)"},"deferrable":{"type":"boolean","description":"Make constraint deferrable (for create_fk/create operations)"},"initiallyDeferred":{"type":"boolean","description":"Initially deferred (for create_fk/create operations)"},"ifExists":{"type":"boolean","description":"Include IF EXISTS clause (for drop_fk/drop operations)"},"cascade":{"type":"boolean","description":"Include CASCADE clause (for drop_fk/drop operations)"}},"required":["operation"]}	t	postgres
cmkgc376n00zj2rnbvj6wvkjw	postgres_pg_analyze_database	[MCP: postgres] Analyze PostgreSQL database configuration and performance	type PgAnalyzeDatabaseTool = {\n  // Analyze PostgreSQL database configuration and performance\n  pg_analyze_database: (args: {\n    // PostgreSQL connection string (optional if POSTGRES_CONNECTION_STRING environment variable or --connection-string CLI option is set)\n    connectionString?: string;\n    // Type of analysis to perform\n    analysisType?: 'configuration' | 'performance' | 'security';\n  }) => Promise<any>;\n};	{"type":"object","properties":{"connectionString":{"type":"string","description":"PostgreSQL connection string (optional if POSTGRES_CONNECTION_STRING environment variable or --connection-string CLI option is set)"},"analysisType":{"type":"string","enum":["configuration","performance","security"],"description":"Type of analysis to perform"}}}	t	postgres
cmkgc36q600z32rnbs6owztpq	git_push	[MCP: git] Push commits to remote	type PushTool = {\n  // Push commits to remote\n  push: (args: {\n    // Path to repository. MUST be an absolute path (e.g., /Users/username/projects/my-repo)\n    path?: string;\n    // Remote name\n    remote?: string;\n    // Branch name\n    branch: string;\n    // Force push changes\n    force?: boolean;\n    // Skip pre-push hooks\n    noVerify?: boolean;\n    // Push all tags\n    tags?: boolean;\n  }) => Promise<any>;\n};	{"type":"object","properties":{"path":{"type":"string","description":"Path to repository. MUST be an absolute path (e.g., /Users/username/projects/my-repo)"},"remote":{"type":"string","description":"Remote name","default":"origin"},"branch":{"type":"string","description":"Branch name"},"force":{"type":"boolean","description":"Force push changes","default":false},"noVerify":{"type":"boolean","description":"Skip pre-push hooks","default":false},"tags":{"type":"boolean","description":"Push all tags","default":false}},"required":["branch"]}	t	git
cmkgc36gk00yv2rnbv4vwc90v	filesystem_create_directory	[MCP: filesystem] Creates a directory. Optionally creates parent directories. Accepts relative or absolute paths.	type CreateDirectoryTool = {\n  // Creates a directory. Optionally creates parent directories. Accepts relative or absolute paths.\n  create_directory: (args: {\n    // The path to the directory to create. Can be relative or absolute.\n    path: string;\n    // If true, create any necessary parent directories that don't exist. If false, fail if a parent directory is missing.\n    create_parents?: boolean;\n  }) => Promise<any>;\n};	{"type":"object","properties":{"path":{"type":"string","minLength":1,"description":"The path to the directory to create. Can be relative or absolute."},"create_parents":{"type":"boolean","default":true,"description":"If true, create any necessary parent directories that don't exist. If false, fail if a parent directory is missing."}},"required":["path"]}	t	filesystem
cmkgc36qs00z72rnbpwlvjxx4	git_branch_delete	[MCP: git] Delete a branch	type BranchDeleteTool = {\n  // Delete a branch\n  branch_delete: (args: {\n    // Path to repository. MUST be an absolute path (e.g., /Users/username/projects/my-repo)\n    path?: string;\n    // Branch name\n    name: string;\n  }) => Promise<any>;\n};	{"type":"object","properties":{"path":{"type":"string","description":"Path to repository. MUST be an absolute path (e.g., /Users/username/projects/my-repo)"},"name":{"type":"string","description":"Branch name"}},"required":["name"]}	t	git
cmkgc377000zk2rnb2tix687h	postgres_pg_manage_functions	[MCP: postgres] Manage PostgreSQL functions - get, create, or drop functions with a single tool. Examples: operation="get" to list functions, operation="create" with functionName="test_func", parameters="" (empty for no params), returnType="TEXT", functionBody="SELECT 'Hello'"	type PgManageFunctionsTool = {\n  // Manage PostgreSQL functions - get, create, or drop functions with a single tool. Examples: operation="get" to list functions, operation="create" with functionName="test_func", parameters="" (empty for no params), returnType="TEXT", functionBody="SELECT 'Hello'"\n  pg_manage_functions: (args: {\n    // PostgreSQL connection string (optional)\n    connectionString?: string;\n    // Operation to perform: get (list/info), create (new function), or drop (remove function)\n    operation: 'get' | 'create' | 'drop';\n    // Name of the function (required for create/drop, optional for get to filter)\n    functionName?: string;\n    // Schema name (defaults to public)\n    schema?: string;\n    // Function parameters - required for create operation, required for drop when function is overloaded. Use empty string "" for functions with no parameters\n    parameters?: string;\n    // Return type of the function (required for create operation)\n    returnType?: string;\n    // Function body code (required for create operation)\n    functionBody?: string;\n    // Function language (defaults to plpgsql for create)\n    language?: 'sql' | 'plpgsql' | 'plpython3u';\n    // Function volatility (defaults to VOLATILE for create)\n    volatility?: 'VOLATILE' | 'STABLE' | 'IMMUTABLE';\n    // Function security context (defaults to INVOKER for create)\n    security?: 'INVOKER' | 'DEFINER';\n    // Whether to replace the function if it exists (for create operation)\n    replace?: boolean;\n    // Whether to include IF EXISTS clause (for drop operation)\n    ifExists?: boolean;\n    // Whether to include CASCADE clause (for drop operation)\n    cascade?: boolean;\n  }) => Promise<any>;\n};	{"type":"object","properties":{"connectionString":{"type":"string","description":"PostgreSQL connection string (optional)"},"operation":{"type":"string","enum":["get","create","drop"],"description":"Operation to perform: get (list/info), create (new function), or drop (remove function)"},"functionName":{"type":"string","description":"Name of the function (required for create/drop, optional for get to filter)"},"schema":{"type":"string","description":"Schema name (defaults to public)"},"parameters":{"type":"string","description":"Function parameters - required for create operation, required for drop when function is overloaded. Use empty string \\"\\" for functions with no parameters"},"returnType":{"type":"string","description":"Return type of the function (required for create operation)"},"functionBody":{"type":"string","description":"Function body code (required for create operation)"},"language":{"type":"string","enum":["sql","plpgsql","plpython3u"],"description":"Function language (defaults to plpgsql for create)"},"volatility":{"type":"string","enum":["VOLATILE","STABLE","IMMUTABLE"],"description":"Function volatility (defaults to VOLATILE for create)"},"security":{"type":"string","enum":["INVOKER","DEFINER"],"description":"Function security context (defaults to INVOKER for create)"},"replace":{"type":"boolean","description":"Whether to replace the function if it exists (for create operation)"},"ifExists":{"type":"boolean","description":"Whether to include IF EXISTS clause (for drop operation)"},"cascade":{"type":"boolean","description":"Whether to include CASCADE clause (for drop operation)"}},"required":["operation"]}	t	postgres
cmkgc378k00zu2rnbr7y98hni	postgres_pg_manage_indexes	[MCP: postgres] Manage PostgreSQL indexes - get, create, drop, reindex, and analyze usage with a single tool. Examples: operation="get" to list indexes, operation="create" with indexName, tableName, columns, operation="analyze_usage" for performance analysis	type PgManageIndexesTool = {\n  // Manage PostgreSQL indexes - get, create, drop, reindex, and analyze usage with a single tool. Examples: operation="get" to list indexes, operation="create" with indexName, tableName, columns, operation="analyze_usage" for performance analysis\n  pg_manage_indexes: (args: {\n    // PostgreSQL connection string (optional)\n    connectionString?: string;\n    // Operation: get (list indexes), create (new index), drop (remove index), reindex (rebuild), analyze_usage (find unused/duplicate)\n    operation: 'get' | 'create' | 'drop' | 'reindex' | 'analyze_usage';\n    // Schema name (defaults to public)\n    schema?: string;\n    // Table name (optional for get/analyze_usage, required for create)\n    tableName?: string;\n    // Index name (required for create/drop)\n    indexName?: string;\n    // Include usage statistics (for get operation)\n    includeStats?: boolean;\n    // Column names for the index (required for create operation)\n    columns?: Array<string>;\n    // Create unique index (for create operation)\n    unique?: boolean;\n    // Create/drop index concurrently (for create/drop operations)\n    concurrent?: boolean;\n    // Index method (for create operation, defaults to btree)\n    method?: 'btree' | 'hash' | 'gist' | 'spgist' | 'gin' | 'brin';\n    // WHERE clause for partial index (for create operation)\n    where?: string;\n    // Include IF NOT EXISTS clause (for create operation)\n    ifNotExists?: boolean;\n    // Include IF EXISTS clause (for drop operation)\n    ifExists?: boolean;\n    // Include CASCADE clause (for drop operation)\n    cascade?: boolean;\n    // Target name for reindex (required for reindex operation)\n    target?: string;\n    // Type of target for reindex (required for reindex operation)\n    type?: 'index' | 'table' | 'schema' | 'database';\n    // Minimum index size in bytes (for analyze_usage operation)\n    minSizeBytes?: number;\n    // Include unused indexes (for analyze_usage operation)\n    showUnused?: boolean;\n    // Detect duplicate indexes (for analyze_usage operation)\n    showDuplicates?: boolean;\n  }) => Promise<any>;\n};	{"type":"object","properties":{"connectionString":{"type":"string","description":"PostgreSQL connection string (optional)"},"operation":{"type":"string","enum":["get","create","drop","reindex","analyze_usage"],"description":"Operation: get (list indexes), create (new index), drop (remove index), reindex (rebuild), analyze_usage (find unused/duplicate)"},"schema":{"type":"string","description":"Schema name (defaults to public)"},"tableName":{"type":"string","description":"Table name (optional for get/analyze_usage, required for create)"},"indexName":{"type":"string","description":"Index name (required for create/drop)"},"includeStats":{"type":"boolean","description":"Include usage statistics (for get operation)"},"columns":{"type":"array","items":{"type":"string"},"description":"Column names for the index (required for create operation)"},"unique":{"type":"boolean","description":"Create unique index (for create operation)"},"concurrent":{"type":"boolean","description":"Create/drop index concurrently (for create/drop operations)"},"method":{"type":"string","enum":["btree","hash","gist","spgist","gin","brin"],"description":"Index method (for create operation, defaults to btree)"},"where":{"type":"string","description":"WHERE clause for partial index (for create operation)"},"ifNotExists":{"type":"boolean","description":"Include IF NOT EXISTS clause (for create operation)"},"ifExists":{"type":"boolean","description":"Include IF EXISTS clause (for drop operation)"},"cascade":{"type":"boolean","description":"Include CASCADE clause (for drop operation)"},"target":{"type":"string","description":"Target name for reindex (required for reindex operation)"},"type":{"type":"string","enum":["index","table","schema","database"],"description":"Type of target for reindex (required for reindex operation)"},"minSizeBytes":{"type":"number","description":"Minimum index size in bytes (for analyze_usage operation)"},"showUnused":{"type":"boolean","description":"Include unused indexes (for analyze_usage operation)"},"showDuplicates":{"type":"boolean","description":"Detect duplicate indexes (for analyze_usage operation)"}},"required":["operation"]}	t	postgres
cmkgc379n01012rnbljjnk890	postgres_pg_manage_comments	[MCP: postgres] Manage PostgreSQL object comments - get, set, remove comments on tables, columns, functions, and other database objects. Examples: operation="get" with objectType="table", objectName="users", operation="set" with comment text, operation="bulk_get" for discovery	type PgManageCommentsTool = {\n  // Manage PostgreSQL object comments - get, set, remove comments on tables, columns, functions, and other database objects. Examples: operation="get" with objectType="table", objectName="users", operation="set" with comment text, operation="bulk_get" for discovery\n  pg_manage_comments: (args: {\n    // PostgreSQL connection string (optional)\n    connectionString?: string;\n    // Operation: get (retrieve comments), set (add/update comment), remove (delete comment), bulk_get (discovery mode)\n    operation: 'get' | 'set' | 'remove' | 'bulk_get';\n    // Type of database object (required for get/set/remove)\n    objectType?: 'table' | 'column' | 'index' | 'constraint' | 'function' | 'trigger' | 'view' | 'sequence' | 'schema' | 'database';\n    // Name of the object (required for get/set/remove)\n    objectName?: string;\n    // Schema name (defaults to public, required for most object types)\n    schema?: string;\n    // Column name (required when objectType is "column")\n    columnName?: string;\n    // Comment text (required for set operation)\n    comment?: string;\n    // Include system objects in bulk_get (defaults to false)\n    includeSystemObjects?: boolean;\n    // Filter by object type in bulk_get operation\n    filterObjectType?: 'table' | 'column' | 'index' | 'constraint' | 'function' | 'trigger' | 'view' | 'sequence' | 'schema' | 'database';\n  }) => Promise<any>;\n};	{"type":"object","properties":{"connectionString":{"type":"string","description":"PostgreSQL connection string (optional)"},"operation":{"type":"string","enum":["get","set","remove","bulk_get"],"description":"Operation: get (retrieve comments), set (add/update comment), remove (delete comment), bulk_get (discovery mode)"},"objectType":{"type":"string","enum":["table","column","index","constraint","function","trigger","view","sequence","schema","database"],"description":"Type of database object (required for get/set/remove)"},"objectName":{"type":"string","description":"Name of the object (required for get/set/remove)"},"schema":{"type":"string","description":"Schema name (defaults to public, required for most object types)"},"columnName":{"type":"string","description":"Column name (required when objectType is \\"column\\")"},"comment":{"type":"string","description":"Comment text (required for set operation)"},"includeSystemObjects":{"type":"boolean","description":"Include system objects in bulk_get (defaults to false)"},"filterObjectType":{"type":"string","enum":["table","column","index","constraint","function","trigger","view","sequence","schema","database"],"description":"Filter by object type in bulk_get operation"}},"required":["operation"]}	t	postgres
\.


--
-- Data for Name: UnknownModel; Type: TABLE DATA; Schema: public; Owner: -
--

COPY public."UnknownModel" (id, "modelId", reason) FROM stdin;
cmkgs92yr007d8wmota9d4mew	nvidia:abacusai/dracarys-llama-3.1-70b-instruct	surveyor_failed
cmkgs92yz007f8wmo52c1xou7	nvidia:institute-of-science-tokyo/llama-3.1-swallow-70b-instruct-v0.1	surveyor_failed
cmkgs92z5007h8wmoh3edbe0b	nvidia:institute-of-science-tokyo/llama-3.1-swallow-8b-instruct-v0.1	surveyor_failed
cmkgs92zp007n8wmoaq5a114g	nvidia:meta/codellama-70b	surveyor_failed
cmkgs92zv007p8wmok8trtx7j	nvidia:meta/llama-3.1-405b-instruct	surveyor_failed
cmkgs9302007r8wmo6mws1hhd	nvidia:meta/llama-3.1-70b-instruct	surveyor_failed
cmkgs9309007t8wmoai1bkl6t	nvidia:meta/llama-3.1-8b-instruct	surveyor_failed
cmkgs930y00818wmopgu6efgv	nvidia:meta/llama-3.2-1b-instruct	surveyor_failed
cmkgs931500838wmoiwf178fm	nvidia:meta/llama-3.2-3b-instruct	surveyor_failed
cmkgs931u008b8wmor22z7zss	nvidia:meta/llama-3.3-70b-instruct	surveyor_failed
cmkgs9320008d8wmo68jto069	nvidia:meta/llama2-70b	surveyor_failed
cmkgs9326008f8wmogxgiwvjb	nvidia:meta/llama3-70b-instruct	surveyor_failed
cmkgs932d008h8wmoqujxn8ii	nvidia:meta/llama3-8b-instruct	surveyor_failed
cmkgs9339008r8wmoh0baxfuc	nvidia:nvidia/usdcode-llama-3.1-70b-instruct	surveyor_failed
cmkgs933g008t8wmo85t1ra2u	nvidia:yentinglin/llama-3-taiwan-70b-instruct	surveyor_failed
cmkgs933m008v8wmo5j1p9217	nvidia:tokyotech-llm/llama-3-swallow-70b-instruct-v0.1	surveyor_failed
\.


--
-- Data for Name: VisionModel; Type: TABLE DATA; Schema: public; Owner: -
--

COPY public."VisionModel" (id, "modelId", "maxResolution", "supportsVideo") FROM stdin;
cmkgs8ygn000l8wmopvb14q21	mistral:pixtral-large-2411	\N	f
cmkgs8yh7000p8wmodgfwx6l8	mistral:pixtral-large-latest	\N	f
cmkgs8yhp000t8wmozruj9cnp	mistral:mistral-large-pixtral-2411	\N	f
cmkgs8yu3002p8wmo38lv3bsl	mistral:pixtral-12b-2409	\N	f
cmkgs8yum002t8wmoq65fcuov	mistral:pixtral-12b	\N	f
cmkgs8yv4002x8wmomuqkvowh	mistral:pixtral-12b-latest	\N	f
cmkgs8zy7004f8wmoi6cbv7bs	openrouter:nvidia/nemotron-nano-12b-v2-vl:free	\N	f
cmkgs906l005p8wmod91xsig9	openrouter:qwen/qwen-2.5-vl-7b-instruct:free	\N	f
cmkgs930n007x8wmon0elg3ie	nvidia:meta/llama-3.2-11b-vision-instruct	\N	f
cmkgs931i00878wmodjkdxdy5	nvidia:meta/llama-3.2-90b-vision-instruct	\N	f
\.


--
-- Data for Name: WorkOrderCard; Type: TABLE DATA; Schema: public; Owner: -
--

COPY public."WorkOrderCard" (id, title, description, "workspaceId", "contextStats", "createdAt", "updatedAt") FROM stdin;
\.


--
-- Data for Name: Workspace; Type: TABLE DATA; Schema: public; Owner: -
--

COPY public."Workspace" (id, name, "rootPath", "systemPrompt", "codeRules", glossary, "createdAt", "updatedAt") FROM stdin;
\.


--
-- Data for Name: _prisma_migrations; Type: TABLE DATA; Schema: public; Owner: -
--

COPY public._prisma_migrations (id, checksum, finished_at, migration_name, logs, rolled_back_at, started_at, applied_steps_count) FROM stdin;
a48aed12-2960-471a-8b41-0110769ad50b	80875afa87465d75aeb4e67e6eb9a102a091974de9ac824c95ea3d807a1af074	2026-01-16 03:41:48.585339+00	20260105161700_init_clean_slate	\N	\N	2026-01-16 03:41:48.388252+00	1
0438139a-00cc-495e-a0f2-233dfdcc21fb	38e13bffeb50e09176553cbac9249579eb867780398836a7db8a43a92c70c2cd	2026-01-16 03:41:48.610615+00	20260105163959_move_keys_to_env_and_cleanup	\N	\N	2026-01-16 03:41:48.589614+00	1
d5028945-0d2d-4534-af57-3e30f2fdd878	62126ce38de161911e85843ee6000c39c652fa48638127bbc0858b391cdf19fd	2026-01-16 03:41:48.626812+00	20260105164124_remove_apikey_real	\N	\N	2026-01-16 03:41:48.614624+00	1
02e62488-10a6-47b5-af63-232e88f2e32f	eac54929cb53a29b10c5c667ca4ead7b97819e8597e83ab97c629801aeb87735	2026-01-16 03:41:58.256039+00	20260116034150_add_is_local_to_model_capabilities	\N	\N	2026-01-16 03:41:58.143675+00	1
\.


--
-- Data for Name: cerebras_models; Type: TABLE DATA; Schema: public; Owner: -
--

COPY public.cerebras_models (_id, _loaded_at, id, object, created, owned_by) FROM stdin;
d74d94cb-8db4-4c77-ab88-56f2970209a8	2026-01-16 11:19:02.421168+00	llama-3.3-70b	model	0	Cerebras
8ca696dc-afdd-4a1f-9f8e-bc30ea958fe0	2026-01-16 11:19:02.421168+00	llama3.1-8b	model	0	Cerebras
aceddc72-872f-4080-9341-d10eb746d739	2026-01-16 11:19:02.421168+00	zai-glm-4.6	model	0	Cerebras
24760c5d-4e49-4095-be78-a5a6085a5a8d	2026-01-16 11:19:02.421168+00	qwen-3-32b	model	0	Cerebras
e33523fc-f910-4983-9b33-22d3a81506c0	2026-01-16 11:19:02.421168+00	gpt-oss-120b	model	0	Cerebras
68ba52be-b503-481a-abe4-672a70669204	2026-01-16 11:19:02.421168+00	zai-glm-4.7	model	0	Cerebras
a42eff61-28ee-4537-931a-8c805151bd1d	2026-01-16 11:19:02.421168+00	qwen-3-235b-a22b-instruct-2507	model	0	Cerebras
\.


--
-- Data for Name: google_models_example; Type: TABLE DATA; Schema: public; Owner: -
--

COPY public.google_models_example (id, "createdAt", "updatedAt", name, "displayName", description, "inputTokenLimit", "outputTokenLimit", "supportedGenerationMethods", temperature, "topP", "topK", "maxTemperature") FROM stdin;
\.


--
-- Data for Name: groq_models; Type: TABLE DATA; Schema: public; Owner: -
--

COPY public.groq_models (_id, _loaded_at, id, object, created, owned_by, active, context_window, public_apps, max_completion_tokens) FROM stdin;
448a0d47-020e-411d-b7c3-9db8463b65f3	2026-01-16 11:18:59.867289+00	canopylabs/orpheus-arabic-saudi	model	1765926439	Canopy Labs	true	4000	null	50000
7c24b5bb-d47d-457a-91c6-32856908b682	2026-01-16 11:18:59.867289+00	canopylabs/orpheus-v1-english	model	1766186316	Canopy Labs	true	4000	null	50000
c2a99928-c143-4d06-b749-ac2881c013a9	2026-01-16 11:18:59.867289+00	moonshotai/kimi-k2-instruct	model	1752435491	Moonshot AI	true	131072	null	16384
c10bfd51-fe31-4aa6-9556-435e17ef2876	2026-01-16 11:18:59.867289+00	meta-llama/llama-prompt-guard-2-86m	model	1748632165	Meta	true	512	null	512
2b2b99ed-e626-4f8d-a752-767fd690b685	2026-01-16 11:18:59.867289+00	groq/compound	model	1756949530	Groq	true	131072	null	8192
d1f04c73-a8e3-4ae8-893f-d6caf62639dc	2026-01-16 11:18:59.867289+00	groq/compound-mini	model	1756949707	Groq	true	131072	null	8192
fac38154-e27c-496b-a5da-d531b5566435	2026-01-16 11:18:59.867289+00	meta-llama/llama-prompt-guard-2-22m	model	1748632101	Meta	true	512	null	512
da202be8-b393-4c5a-a3e1-92760cd2ea4d	2026-01-16 11:18:59.867289+00	allam-2-7b	model	1737672203	SDAIA	true	4096	null	4096
38118ba1-a397-49e0-be70-193d8eeedcde	2026-01-16 11:18:59.867289+00	llama-3.1-8b-instant	model	1693721698	Meta	true	131072	null	131072
95aef31b-29c4-477f-a360-3758ef9dce9d	2026-01-16 11:18:59.867289+00	openai/gpt-oss-20b	model	1754407957	OpenAI	true	131072	null	65536
a8a13a28-a32c-4ab9-8da5-fd391b6ba93b	2026-01-16 11:18:59.867289+00	qwen/qwen3-32b	model	1748396646	Alibaba Cloud	true	131072	null	40960
74dab5a3-722c-4c80-9b05-811e7676caab	2026-01-16 11:18:59.867289+00	whisper-large-v3-turbo	model	1728413088	OpenAI	true	448	null	448
286cf004-82d1-4972-bdfd-906144538451	2026-01-16 11:18:59.867289+00	meta-llama/llama-guard-4-12b	model	1746743847	Meta	true	131072	null	1024
0825a73f-5f45-4382-ba90-c7039aec67e1	2026-01-16 11:18:59.867289+00	moonshotai/kimi-k2-instruct-0905	model	1757046093	Moonshot AI	true	262144	null	16384
512cfa7c-8093-4180-92ef-9a0172bddced	2026-01-16 11:18:59.867289+00	openai/gpt-oss-120b	model	1754408224	OpenAI	true	131072	null	65536
7e91863a-c8d3-49e1-9706-338dc67f0f3c	2026-01-16 11:18:59.867289+00	openai/gpt-oss-safeguard-20b	model	1761708789	OpenAI	true	131072	null	65536
f2f6ffbb-a04c-4a19-82d9-d65431ec89f2	2026-01-16 11:18:59.867289+00	llama-3.3-70b-versatile	model	1733447754	Meta	true	131072	null	32768
74093d26-6230-44c8-bafc-5c22bd1e3b4d	2026-01-16 11:18:59.867289+00	whisper-large-v3	model	1693721698	OpenAI	true	448	null	448
3fb276ff-005b-4145-9255-a5fb3e260615	2026-01-16 11:18:59.867289+00	meta-llama/llama-4-maverick-17b-128e-instruct	model	1743877158	Meta	true	131072	null	8192
0f5ea5b7-79c6-4a8e-b941-aa138f0b374d	2026-01-16 11:18:59.867289+00	meta-llama/llama-4-scout-17b-16e-instruct	model	1743874824	Meta	true	131072	null	8192
\.


--
-- Data for Name: mistral_models; Type: TABLE DATA; Schema: public; Owner: -
--

COPY public.mistral_models (_id, _loaded_at, id, object, created, owned_by, capabilities, name, description, max_context_length, aliases, deprecation, deprecation_replacement_model, default_model_temperature, type) FROM stdin;
7a494472-942b-4aa0-a389-c13f80160fc3	2026-01-16 11:18:57.798368+00	mistral-medium-2505	model	1768562337	mistralai	{"ocr": false, "audio": false, "vision": true, "moderation": false, "fine_tuning": true, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": true}	mistral-medium-2505	Our frontier-class multimodal model released May 2025.	131072	[]	null	null	0.3	base
8b68b42a-1204-4b85-ba4c-1161a3f185ac	2026-01-16 11:18:57.798368+00	mistral-medium-2508	model	1768562337	mistralai	{"ocr": false, "audio": false, "vision": true, "moderation": false, "fine_tuning": true, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": true}	mistral-medium-2508	Update on Mistral Medium 3 with improved capabilities.	131072	["mistral-medium-latest", "mistral-medium"]	null	null	0.3	base
86dd7b30-341b-4747-8f5d-69de6a196a1c	2026-01-16 11:18:57.798368+00	mistral-medium-latest	model	1768562337	mistralai	{"ocr": false, "audio": false, "vision": true, "moderation": false, "fine_tuning": true, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": true}	mistral-medium-2508	Update on Mistral Medium 3 with improved capabilities.	131072	["mistral-medium-2508", "mistral-medium"]	null	null	0.3	base
57cbdb66-b33b-4898-baa6-51f1c9b2c3a3	2026-01-16 11:18:57.798368+00	mistral-medium	model	1768562337	mistralai	{"ocr": false, "audio": false, "vision": true, "moderation": false, "fine_tuning": true, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": true}	mistral-medium-2508	Update on Mistral Medium 3 with improved capabilities.	131072	["mistral-medium-2508", "mistral-medium-latest"]	null	null	0.3	base
49361c1e-16eb-4ed5-970b-a66a08135a88	2026-01-16 11:18:57.798368+00	open-mistral-nemo	model	1768562337	mistralai	{"ocr": false, "audio": false, "vision": false, "moderation": false, "fine_tuning": true, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": true}	open-mistral-nemo	Our best multilingual open source model released July 2024.	131072	["open-mistral-nemo-2407", "mistral-tiny-2407", "mistral-tiny-latest"]	null	null	0.3	base
f6498629-6238-46c7-9d84-7ac83c04e20a	2026-01-16 11:18:57.798368+00	open-mistral-nemo-2407	model	1768562337	mistralai	{"ocr": false, "audio": false, "vision": false, "moderation": false, "fine_tuning": true, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": true}	open-mistral-nemo	Our best multilingual open source model released July 2024.	131072	["open-mistral-nemo", "mistral-tiny-2407", "mistral-tiny-latest"]	null	null	0.3	base
e1fb4e2a-6067-4a10-9193-abbfeefc277f	2026-01-16 11:18:57.798368+00	mistral-tiny-2407	model	1768562337	mistralai	{"ocr": false, "audio": false, "vision": false, "moderation": false, "fine_tuning": true, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": true}	open-mistral-nemo	Our best multilingual open source model released July 2024.	131072	["open-mistral-nemo", "open-mistral-nemo-2407", "mistral-tiny-latest"]	null	null	0.3	base
a0711042-bd6e-4988-9f78-7573ad96cdb5	2026-01-16 11:18:57.798368+00	mistral-tiny-latest	model	1768562337	mistralai	{"ocr": false, "audio": false, "vision": false, "moderation": false, "fine_tuning": true, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": true}	open-mistral-nemo	Our best multilingual open source model released July 2024.	131072	["open-mistral-nemo", "open-mistral-nemo-2407", "mistral-tiny-2407"]	null	null	0.3	base
a5ed647d-1415-4948-9336-eba8ad37b0ed	2026-01-16 11:18:57.798368+00	mistral-large-2411	model	1768562337	mistralai	{"ocr": false, "audio": false, "vision": false, "moderation": false, "fine_tuning": true, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": true}	mistral-large-2411	Our top-tier reasoning model for high-complexity tasks with the lastest version released November 2024.	131072	[]	null	null	0.7	base
93f6e1c2-53e2-4b5b-8af0-7bba5a232517	2026-01-16 11:18:57.798368+00	pixtral-large-2411	model	1768562337	mistralai	{"ocr": false, "audio": false, "vision": true, "moderation": false, "fine_tuning": false, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": true}	pixtral-large-2411	Official pixtral-large-2411 Mistral AI model	131072	["pixtral-large-latest", "mistral-large-pixtral-2411"]	null	null	0.7	base
06abc6f2-a2b5-4b6a-b74b-cd5476f118fc	2026-01-16 11:18:57.798368+00	pixtral-large-latest	model	1768562337	mistralai	{"ocr": false, "audio": false, "vision": true, "moderation": false, "fine_tuning": false, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": true}	pixtral-large-2411	Official pixtral-large-2411 Mistral AI model	131072	["pixtral-large-2411", "mistral-large-pixtral-2411"]	null	null	0.7	base
368771d8-5dd0-4a13-9bb6-6e2d467b17bc	2026-01-16 11:18:57.798368+00	mistral-large-pixtral-2411	model	1768562337	mistralai	{"ocr": false, "audio": false, "vision": true, "moderation": false, "fine_tuning": false, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": true}	pixtral-large-2411	Official pixtral-large-2411 Mistral AI model	131072	["pixtral-large-2411", "pixtral-large-latest"]	null	null	0.7	base
c24da8c9-39f4-4082-b77a-ba6ef37ba12d	2026-01-16 11:18:57.798368+00	codestral-2508	model	1768562337	mistralai	{"ocr": false, "audio": false, "vision": false, "moderation": false, "fine_tuning": false, "classification": false, "completion_fim": true, "completion_chat": true, "function_calling": true}	codestral-2508	Our cutting-edge language model for coding released August 2025.	256000	["codestral-latest"]	null	null	0.3	base
15eb0730-6c1d-46b2-8c86-efa01af4591d	2026-01-16 11:18:57.798368+00	codestral-latest	model	1768562337	mistralai	{"ocr": false, "audio": false, "vision": false, "moderation": false, "fine_tuning": false, "classification": false, "completion_fim": true, "completion_chat": true, "function_calling": true}	codestral-2508	Our cutting-edge language model for coding released August 2025.	256000	["codestral-2508"]	null	null	0.3	base
8fad019d-dfcc-4fa4-85d0-4c227ca7bc93	2026-01-16 11:18:57.798368+00	devstral-small-2507	model	1768562337	mistralai	{"ocr": false, "audio": false, "vision": false, "moderation": false, "fine_tuning": false, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": true}	devstral-small-2507	Our small open-source code-agentic model.	131072	[]	null	null	0	base
188c8091-af51-48cd-9ecd-0d9fd912b1ff	2026-01-16 11:18:57.798368+00	devstral-medium-2507	model	1768562337	mistralai	{"ocr": false, "audio": false, "vision": false, "moderation": false, "fine_tuning": false, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": true}	devstral-medium-2507	Our medium code-agentic model.	131072	[]	null	null	0	base
5e5161de-f175-4150-93cf-80eb8fb27ab2	2026-01-16 11:18:57.798368+00	devstral-2512	model	1768562337	mistralai	{"ocr": false, "audio": false, "vision": false, "moderation": false, "fine_tuning": false, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": true}	devstral-2512	Official devstral-2512 Mistral AI model	262144	[]	null	null	0.2	base
6e528302-733c-4590-8b7c-c6e8ac4784d1	2026-01-16 11:18:57.798368+00	mistral-vibe-cli-latest	model	1768562337	mistralai	{"ocr": false, "audio": false, "vision": false, "moderation": false, "fine_tuning": false, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": true}	devstral-2512	Official devstral-2512 Mistral AI model	262144	[]	null	null	0.2	base
d5c30221-534b-4fdc-9d17-2080967a8024	2026-01-16 11:18:57.798368+00	devstral-medium-latest	model	1768562337	mistralai	{"ocr": false, "audio": false, "vision": false, "moderation": false, "fine_tuning": false, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": true}	devstral-2512	Official devstral-2512 Mistral AI model	262144	[]	null	null	0.2	base
89a27250-05a3-4326-b710-1fb3d7a4b962	2026-01-16 11:18:57.798368+00	devstral-latest	model	1768562337	mistralai	{"ocr": false, "audio": false, "vision": false, "moderation": false, "fine_tuning": false, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": true}	devstral-2512	Official devstral-2512 Mistral AI model	262144	[]	null	null	0.2	base
a91fa45b-a070-4d38-b004-ffbd4daa2938	2026-01-16 11:18:57.798368+00	labs-devstral-small-2512	model	1768562337	mistralai	{"ocr": false, "audio": false, "vision": true, "moderation": false, "fine_tuning": false, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": true}	labs-devstral-small-2512	Official labs-devstral-small-2512 Mistral AI model	262144	[]	null	null	0.2	base
a1e4ccf2-7d4e-413e-8c34-1d91e1eeea92	2026-01-16 11:18:57.798368+00	devstral-small-latest	model	1768562337	mistralai	{"ocr": false, "audio": false, "vision": true, "moderation": false, "fine_tuning": false, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": true}	labs-devstral-small-2512	Official labs-devstral-small-2512 Mistral AI model	262144	[]	null	null	0.2	base
0dc3a229-4dd9-439e-bbbe-7b4d2d24050f	2026-01-16 11:18:57.798368+00	mistral-small-2506	model	1768562337	mistralai	{"ocr": false, "audio": false, "vision": true, "moderation": false, "fine_tuning": false, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": true}	mistral-small-2506	Our latest enterprise-grade small model with the latest version released June 2025.	131072	["mistral-small-latest"]	null	null	0.3	base
1ca9125a-4a84-4e37-935f-d961f50e0846	2026-01-16 11:18:57.798368+00	mistral-small-latest	model	1768562337	mistralai	{"ocr": false, "audio": false, "vision": true, "moderation": false, "fine_tuning": false, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": true}	mistral-small-2506	Our latest enterprise-grade small model with the latest version released June 2025.	131072	["mistral-small-2506"]	null	null	0.3	base
92f7b1ef-801c-42a9-b628-825f81ea858a	2026-01-16 11:18:57.798368+00	labs-mistral-small-creative	model	1768562337	mistralai	{"ocr": false, "audio": false, "vision": false, "moderation": false, "fine_tuning": true, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": true}	labs-mistral-small-creative	Official labs-mistral-small-creative Mistral AI model	32768	[]	null	null	0.3	base
fd5e7e6b-d681-41fe-ac76-255f9e35523d	2026-01-16 11:18:57.798368+00	magistral-medium-2509	model	1768562337	mistralai	{"ocr": false, "audio": false, "vision": true, "moderation": false, "fine_tuning": true, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": true}	magistral-medium-2509	Our frontier-class reasoning model release candidate September 2025.	131072	["magistral-medium-latest"]	null	null	0.7	base
cfe66864-b59b-4e20-93aa-9f29df7d54b9	2026-01-16 11:18:57.798368+00	magistral-medium-latest	model	1768562337	mistralai	{"ocr": false, "audio": false, "vision": true, "moderation": false, "fine_tuning": true, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": true}	magistral-medium-2509	Our frontier-class reasoning model release candidate September 2025.	131072	["magistral-medium-2509"]	null	null	0.7	base
bc77f5dc-1890-4571-8ca8-5cd4d759ce28	2026-01-16 11:18:57.798368+00	magistral-small-2509	model	1768562337	mistralai	{"ocr": false, "audio": false, "vision": true, "moderation": false, "fine_tuning": true, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": true}	magistral-small-2509	Our efficient reasoning model released September 2025.	131072	["magistral-small-latest"]	null	null	0.7	base
08474f95-06f1-47cd-9883-1c228e60624a	2026-01-16 11:18:57.798368+00	magistral-small-latest	model	1768562337	mistralai	{"ocr": false, "audio": false, "vision": true, "moderation": false, "fine_tuning": true, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": true}	magistral-small-2509	Our efficient reasoning model released September 2025.	131072	["magistral-small-2509"]	null	null	0.7	base
0f91fa06-8dc6-4a57-aa4d-5ce9d6899fba	2026-01-16 11:18:57.798368+00	voxtral-mini-2507	model	1768562337	mistralai	{"ocr": false, "audio": true, "vision": false, "moderation": false, "fine_tuning": false, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": false}	voxtral-mini-2507	A mini audio understanding model released in July 2025	32768	["voxtral-mini-latest"]	null	null	0.2	base
1410174d-acae-46b1-bef5-a9496ac0d7d6	2026-01-16 11:18:57.798368+00	voxtral-mini-latest	model	1768562337	mistralai	{"ocr": false, "audio": true, "vision": false, "moderation": false, "fine_tuning": false, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": false}	voxtral-mini-2507	A mini audio understanding model released in July 2025	32768	["voxtral-mini-2507"]	null	null	0.2	base
91264420-cc43-4a73-a692-840c5a3c5bb1	2026-01-16 11:18:57.798368+00	voxtral-small-2507	model	1768562337	mistralai	{"ocr": false, "audio": true, "vision": false, "moderation": false, "fine_tuning": false, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": true}	voxtral-small-2507	A small audio understanding model released in July 2025	32768	["voxtral-small-latest"]	null	null	0.2	base
a9b27343-6174-4ba9-aadc-cf6ba148209c	2026-01-16 11:18:57.798368+00	voxtral-small-latest	model	1768562337	mistralai	{"ocr": false, "audio": true, "vision": false, "moderation": false, "fine_tuning": false, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": true}	voxtral-small-2507	A small audio understanding model released in July 2025	32768	["voxtral-small-2507"]	null	null	0.2	base
0a8a80be-e630-497f-9409-d06291f80cbc	2026-01-16 11:18:57.798368+00	mistral-large-2512	model	1768562337	mistralai	{"ocr": false, "audio": false, "vision": true, "moderation": false, "fine_tuning": true, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": true}	mistral-large-2512	Official mistral-large-2512 Mistral AI model	262144	[]	null	null	0.3	base
b4a9cc61-82ff-484d-b3c5-92211a5a098c	2026-01-16 11:18:57.798368+00	mistral-large-latest	model	1768562337	mistralai	{"ocr": false, "audio": false, "vision": true, "moderation": false, "fine_tuning": true, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": true}	mistral-large-2512	Official mistral-large-2512 Mistral AI model	262144	[]	null	null	0.3	base
7e221613-f9e4-4d32-92ff-d6405c47267d	2026-01-16 11:18:57.798368+00	ministral-3b-2512	model	1768562337	mistralai	{"ocr": false, "audio": false, "vision": true, "moderation": false, "fine_tuning": true, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": true}	ministral-3b-2512	Ministral 3 (a.k.a. Tinystral) 3B Instruct.	131072	["ministral-3b-latest"]	null	null	0.3	base
ba4ff6c6-ede3-456b-aaac-49eb898cda60	2026-01-16 11:18:57.798368+00	ministral-3b-latest	model	1768562337	mistralai	{"ocr": false, "audio": false, "vision": true, "moderation": false, "fine_tuning": true, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": true}	ministral-3b-2512	Ministral 3 (a.k.a. Tinystral) 3B Instruct.	131072	["ministral-3b-2512"]	null	null	0.3	base
1e3f596a-7e19-4b76-ad21-0bca18bce66e	2026-01-16 11:18:57.798368+00	ministral-8b-2512	model	1768562337	mistralai	{"ocr": false, "audio": false, "vision": true, "moderation": false, "fine_tuning": true, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": true}	ministral-8b-2512	Ministral 3 (a.k.a. Tinystral) 8B Instruct.	262144	["ministral-8b-latest"]	null	null	0.3	base
607a83f2-661d-49c2-aa49-6779201632dd	2026-01-16 11:18:57.798368+00	ministral-8b-latest	model	1768562337	mistralai	{"ocr": false, "audio": false, "vision": true, "moderation": false, "fine_tuning": true, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": true}	ministral-8b-2512	Ministral 3 (a.k.a. Tinystral) 8B Instruct.	262144	["ministral-8b-2512"]	null	null	0.3	base
2c8974bf-1c08-4560-9bb1-a9bd1f6cf72d	2026-01-16 11:18:57.798368+00	ministral-14b-2512	model	1768562337	mistralai	{"ocr": false, "audio": false, "vision": true, "moderation": false, "fine_tuning": true, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": true}	ministral-14b-2512	Ministral 3 (a.k.a. Tinystral) 14B Instruct.	262144	["ministral-14b-latest"]	null	null	0.3	base
2e09f2be-1882-48a2-99fc-abb1011345a2	2026-01-16 11:18:57.798368+00	ministral-14b-latest	model	1768562337	mistralai	{"ocr": false, "audio": false, "vision": true, "moderation": false, "fine_tuning": true, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": true}	ministral-14b-2512	Ministral 3 (a.k.a. Tinystral) 14B Instruct.	262144	["ministral-14b-2512"]	null	null	0.3	base
a6bf523f-ad8e-4e0d-94d7-2aa6fada53a9	2026-01-16 11:18:57.798368+00	open-mistral-7b	model	1768562337	mistralai	{"ocr": false, "audio": false, "vision": false, "moderation": false, "fine_tuning": true, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": true}	open-mistral-7b	Our first dense model released September 2023.	32768	["mistral-tiny", "mistral-tiny-2312"]	2026-01-31T12:00:00Z	ministral-8b-latest	0.7	base
cd5493d1-e8fd-4a51-bca5-13238fcee2eb	2026-01-16 11:18:57.798368+00	mistral-tiny	model	1768562337	mistralai	{"ocr": false, "audio": false, "vision": false, "moderation": false, "fine_tuning": true, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": true}	open-mistral-7b	Our first dense model released September 2023.	32768	["open-mistral-7b", "mistral-tiny-2312"]	2026-01-31T12:00:00Z	ministral-8b-latest	0.7	base
1dc81d80-f8fc-48dc-8a54-22684dbd29e6	2026-01-16 11:18:57.798368+00	mistral-tiny-2312	model	1768562337	mistralai	{"ocr": false, "audio": false, "vision": false, "moderation": false, "fine_tuning": true, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": true}	open-mistral-7b	Our first dense model released September 2023.	32768	["open-mistral-7b", "mistral-tiny"]	2026-01-31T12:00:00Z	ministral-8b-latest	0.7	base
3477bb96-765c-4147-b831-f1559efbe4e9	2026-01-16 11:18:57.798368+00	pixtral-12b-2409	model	1768562337	mistralai	{"ocr": false, "audio": false, "vision": true, "moderation": false, "fine_tuning": true, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": true}	pixtral-12b-2409	A 12B model with image understanding capabilities in addition to text.	131072	["pixtral-12b", "pixtral-12b-latest"]	2026-01-31T12:00:00Z	ministral-14b-latest	0.3	base
48790a4a-36b5-4e6e-b28e-54cbdf4df5f2	2026-01-16 11:18:57.798368+00	pixtral-12b	model	1768562337	mistralai	{"ocr": false, "audio": false, "vision": true, "moderation": false, "fine_tuning": true, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": true}	pixtral-12b-2409	A 12B model with image understanding capabilities in addition to text.	131072	["pixtral-12b-2409", "pixtral-12b-latest"]	2026-01-31T12:00:00Z	ministral-14b-latest	0.3	base
b0aec71b-9a38-4391-b551-474900edf563	2026-01-16 11:18:57.798368+00	pixtral-12b-latest	model	1768562337	mistralai	{"ocr": false, "audio": false, "vision": true, "moderation": false, "fine_tuning": true, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": true}	pixtral-12b-2409	A 12B model with image understanding capabilities in addition to text.	131072	["pixtral-12b-2409", "pixtral-12b"]	2026-01-31T12:00:00Z	ministral-14b-latest	0.3	base
91dc9036-85a8-40f4-b5d2-a1084d5a120e	2026-01-16 11:18:57.798368+00	ministral-3b-2410	model	1768562337	mistralai	{"ocr": false, "audio": false, "vision": false, "moderation": false, "fine_tuning": true, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": true}	ministral-3b-2410	World's best edge model.	131072	[]	2026-01-31T12:00:00Z	ministral-3b-latest	0.3	base
6075f9c6-4527-48b7-bd77-0616c89eb559	2026-01-16 11:18:57.798368+00	ministral-8b-2410	model	1768562337	mistralai	{"ocr": false, "audio": false, "vision": false, "moderation": false, "fine_tuning": true, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": true}	ministral-8b-2410	Powerful edge model with extremely high performance/price ratio.	131072	[]	2026-01-31T12:00:00Z	ministral-8b-latest	0.3	base
9dfdc285-00a8-4cb1-bc3c-d92d78667cbc	2026-01-16 11:18:57.798368+00	codestral-2501	model	1768562337	mistralai	{"ocr": false, "audio": false, "vision": false, "moderation": false, "fine_tuning": true, "classification": false, "completion_fim": true, "completion_chat": true, "function_calling": true}	codestral-2501	Our cutting-edge language model for coding released December 2024.	256000	["codestral-2412", "codestral-2411-rc5"]	2026-01-31T12:00:00Z	codestral-latest	0.3	base
10d81536-d8a4-4bbd-872b-9edeae11af1a	2026-01-16 11:18:57.798368+00	codestral-2412	model	1768562337	mistralai	{"ocr": false, "audio": false, "vision": false, "moderation": false, "fine_tuning": true, "classification": false, "completion_fim": true, "completion_chat": true, "function_calling": true}	codestral-2501	Our cutting-edge language model for coding released December 2024.	256000	["codestral-2501", "codestral-2411-rc5"]	2026-01-31T12:00:00Z	codestral-latest	0.3	base
89d69d24-10ff-4089-bd29-6e9fe54278bb	2026-01-16 11:18:57.798368+00	codestral-2411-rc5	model	1768562337	mistralai	{"ocr": false, "audio": false, "vision": false, "moderation": false, "fine_tuning": true, "classification": false, "completion_fim": true, "completion_chat": true, "function_calling": true}	codestral-2501	Our cutting-edge language model for coding released December 2024.	256000	["codestral-2501", "codestral-2412"]	2026-01-31T12:00:00Z	codestral-latest	0.3	base
df25e19e-9779-4184-b304-04f6c894118e	2026-01-16 11:18:57.798368+00	mistral-small-2501	model	1768562337	mistralai	{"ocr": false, "audio": false, "vision": false, "moderation": false, "fine_tuning": true, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": true}	mistral-small-2501	Our latest enterprise-grade small model with the latest version released January 2025. 	32768	[]	2026-01-31T12:00:00Z	mistral-small-latest	0.3	base
2dfe6dc3-1556-41de-bab5-4f8ea83c4911	2026-01-16 11:18:57.798368+00	mistral-embed-2312	model	1768562337	mistralai	{"ocr": false, "audio": false, "vision": false, "moderation": false, "fine_tuning": false, "classification": false, "completion_fim": false, "completion_chat": false, "function_calling": false}	mistral-embed-2312	Official mistral-embed-2312 Mistral AI model	8192	["mistral-embed"]	null	null	null	base
6acefa69-c09a-488e-a58e-6d77a8ff3e0b	2026-01-16 11:18:57.798368+00	mistral-embed	model	1768562337	mistralai	{"ocr": false, "audio": false, "vision": false, "moderation": false, "fine_tuning": false, "classification": false, "completion_fim": false, "completion_chat": false, "function_calling": false}	mistral-embed-2312	Official mistral-embed-2312 Mistral AI model	8192	["mistral-embed-2312"]	null	null	null	base
a44db19f-c4c6-4272-a4fc-e135cc9a31aa	2026-01-16 11:18:57.798368+00	codestral-embed	model	1768562337	mistralai	{"ocr": false, "audio": false, "vision": false, "moderation": false, "fine_tuning": false, "classification": false, "completion_fim": false, "completion_chat": false, "function_calling": false}	codestral-embed	Official codestral-embed Mistral AI model	8192	["codestral-embed-2505"]	null	null	null	base
e443d62f-3317-496b-ba79-04721eb0d61d	2026-01-16 11:18:57.798368+00	codestral-embed-2505	model	1768562337	mistralai	{"ocr": false, "audio": false, "vision": false, "moderation": false, "fine_tuning": false, "classification": false, "completion_fim": false, "completion_chat": false, "function_calling": false}	codestral-embed	Official codestral-embed Mistral AI model	8192	["codestral-embed"]	null	null	null	base
e91a4924-3b5e-4e22-a912-f2678a807ac2	2026-01-16 11:18:57.798368+00	mistral-moderation-2411	model	1768562337	mistralai	{"ocr": false, "audio": false, "vision": false, "moderation": true, "fine_tuning": false, "classification": true, "completion_fim": false, "completion_chat": false, "function_calling": false}	mistral-moderation-2411	Official mistral-moderation-2411 Mistral AI model	8192	["mistral-moderation-latest"]	null	null	null	base
691f0e29-f3df-4ad2-bead-db9d226e5fcf	2026-01-16 11:18:57.798368+00	mistral-moderation-latest	model	1768562337	mistralai	{"ocr": false, "audio": false, "vision": false, "moderation": true, "fine_tuning": false, "classification": true, "completion_fim": false, "completion_chat": false, "function_calling": false}	mistral-moderation-2411	Official mistral-moderation-2411 Mistral AI model	8192	["mistral-moderation-2411"]	null	null	null	base
9551a75f-a29b-4e08-bd73-fb52a199e11e	2026-01-16 11:18:57.798368+00	mistral-ocr-2512	model	1768562337	mistralai	{"ocr": true, "audio": false, "vision": true, "moderation": false, "fine_tuning": false, "classification": false, "completion_fim": false, "completion_chat": false, "function_calling": true}	mistral-ocr-2512	Official mistral-ocr-2512 Mistral AI model	16384	["mistral-ocr-latest"]	null	null	0	base
8f14afe3-bd4b-4c4b-a527-e78d1ca71bf7	2026-01-16 11:18:57.798368+00	mistral-ocr-latest	model	1768562337	mistralai	{"ocr": true, "audio": false, "vision": true, "moderation": false, "fine_tuning": false, "classification": false, "completion_fim": false, "completion_chat": false, "function_calling": true}	mistral-ocr-2512	Official mistral-ocr-2512 Mistral AI model	16384	["mistral-ocr-2512"]	null	null	0	base
2e9d7204-f42b-4308-bf0c-378f81889519	2026-01-16 11:18:57.798368+00	mistral-ocr-2505	model	1768562337	mistralai	{"ocr": true, "audio": false, "vision": true, "moderation": false, "fine_tuning": false, "classification": false, "completion_fim": false, "completion_chat": false, "function_calling": true}	mistral-ocr-2505	Official mistral-ocr-2505 Mistral AI model	16384	[]	null	null	0	base
c56a5633-fdd0-4797-bb02-354d274ee945	2026-01-16 11:18:57.798368+00	mistral-ocr-2503	model	1768562337	mistralai	{"ocr": true, "audio": false, "vision": true, "moderation": false, "fine_tuning": false, "classification": false, "completion_fim": false, "completion_chat": false, "function_calling": true}	mistral-ocr-2503	Official mistral-ocr-2503 Mistral AI model	16384	[]	2026-03-31T12:00:00Z	mistral-ocr-latest	0	base
e620e71a-5527-4e82-803f-55a3035049f1	2026-01-16 11:18:57.798368+00	voxtral-mini-transcribe-2507	model	1768562337	mistralai	{"ocr": false, "audio": false, "vision": false, "moderation": false, "fine_tuning": false, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": false}	voxtral-mini-transcribe-2507	A mini transcription model released in July 2025	16384	["voxtral-mini-2507", "voxtral-mini-latest"]	null	null	0	base
8586fa83-dbfe-4cf1-82c4-dd75f05ebdd0	2026-01-16 11:18:57.798368+00	voxtral-mini-2507	model	1768562337	mistralai	{"ocr": false, "audio": false, "vision": false, "moderation": false, "fine_tuning": false, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": false}	voxtral-mini-transcribe-2507	A mini transcription model released in July 2025	16384	["voxtral-mini-transcribe-2507", "voxtral-mini-latest"]	null	null	0	base
a39801b5-f695-4f42-93b8-2181a98169c7	2026-01-16 11:18:57.798368+00	voxtral-mini-latest	model	1768562337	mistralai	{"ocr": false, "audio": false, "vision": false, "moderation": false, "fine_tuning": false, "classification": false, "completion_fim": false, "completion_chat": true, "function_calling": false}	voxtral-mini-transcribe-2507	A mini transcription model released in July 2025	16384	["voxtral-mini-transcribe-2507", "voxtral-mini-2507"]	null	null	0	base
\.


--
-- Data for Name: model_registry; Type: TABLE DATA; Schema: public; Owner: -
--

COPY public.model_registry (id, "createdAt", "updatedAt", provider_id, model_id, model_name, provider_data, ai_data, specs, is_free, cost_per_1k, updated_at, capability_tags, first_seen_at, is_active, last_seen_at, source) FROM stdin;
\.


--
-- Data for Name: modelcapabilities; Type: TABLE DATA; Schema: public; Owner: -
--

COPY public.modelcapabilities (id, "createdAt", "updatedAt", "modelId", "modelName", "contextWindow", "maxOutput", "hasVision", "hasAudioInput", "hasAudioOutput", "isMultimodal", "supportsFunctionCalling", "supportsJsonMode", tokenizer, "paramCount", "requestsPerMinute", "tokensPerMinute", "hasImageGen", "hasTTS", "hasReasoning", "hasEmbedding", "hasOCR", "hasReward", "hasModeration", confidence, source, specs, "primaryTask", "isLocal") FROM stdin;
cmkgs80at00ih8wbv18y9vax9	2026-01-16 11:15:18.533	2026-01-16 11:15:18.533	mistral:mistral-medium-2505	\N	131072	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	provider_api	{"source": "provider_api", "costPer1k": 0.5, "confidence": "high", "capabilities": ["text", "tool_use"], "contextWindow": 131072}	chat	f
cmkgs80b800il8wbvbkrwufgy	2026-01-16 11:15:18.548	2026-01-16 11:15:18.548	mistral:mistral-medium-2508	\N	131072	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	provider_api	{"source": "provider_api", "costPer1k": 0.5, "confidence": "high", "capabilities": ["text", "tool_use"], "contextWindow": 131072}	chat	f
cmkgs80bm00ip8wbv9j5er98o	2026-01-16 11:15:18.562	2026-01-16 11:15:18.562	mistral:mistral-medium-latest	\N	131072	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	provider_api	{"source": "provider_api", "costPer1k": 0.5, "confidence": "high", "capabilities": ["text", "tool_use"], "contextWindow": 131072}	chat	f
cmkgs80c100it8wbvlc1drghw	2026-01-16 11:15:18.578	2026-01-16 11:15:18.578	mistral:mistral-medium	\N	131072	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	provider_api	{"source": "provider_api", "costPer1k": 0.5, "confidence": "high", "capabilities": ["text", "tool_use"], "contextWindow": 131072}	chat	f
cmkgs80cg00ix8wbvx1iyhfvf	2026-01-16 11:15:18.592	2026-01-16 11:15:18.592	mistral:open-mistral-nemo	\N	131072	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	provider_api	{"source": "provider_api", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 131072}	chat	f
cmkgs80cu00j18wbvinxantc0	2026-01-16 11:15:18.606	2026-01-16 11:15:18.606	mistral:open-mistral-nemo-2407	\N	131072	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	provider_api	{"source": "provider_api", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 131072}	chat	f
cmkgs80d800j58wbv6b8g7qh2	2026-01-16 11:15:18.62	2026-01-16 11:15:18.62	mistral:mistral-tiny-2407	\N	131072	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	provider_api	{"source": "provider_api", "costPer1k": 0.5, "confidence": "high", "capabilities": ["text", "tool_use"], "contextWindow": 131072}	chat	f
cmkgs80dl00j98wbvfn0f8c8w	2026-01-16 11:15:18.633	2026-01-16 11:15:18.633	mistral:mistral-tiny-latest	\N	131072	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	provider_api	{"source": "provider_api", "costPer1k": 0.5, "confidence": "high", "capabilities": ["text", "tool_use"], "contextWindow": 131072}	chat	f
cmkgs80dy00jd8wbviie1u6zp	2026-01-16 11:15:18.646	2026-01-16 11:15:18.646	mistral:mistral-large-2411	\N	131072	8192	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	provider_api	{"source": "provider_api", "costPer1k": 2, "maxOutput": 8192, "confidence": "high", "capabilities": ["text", "tool_use"], "contextWindow": 131072}	chat	f
cmkgs80ec00jh8wbvzn39fhc1	2026-01-16 11:15:18.66	2026-01-16 11:15:18.66	mistral:pixtral-large-2411	\N	131072	\N	t	\N	\N	t	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	provider_api	{"source": "provider_api", "costPer1k": 0.15, "confidence": "high", "capabilities": ["text", "vision"], "contextWindow": 131072}	chat	f
cmkgs80es00jn8wbvfttcfj6r	2026-01-16 11:15:18.677	2026-01-16 11:15:18.677	groq:canopylabs/orpheus-arabic-saudi	\N	32768	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["audio_in", "text"], "contextWindow": 32768}	chat	f
cmkgs80fh00jt8wbvdzcp0o6c	2026-01-16 11:15:18.701	2026-01-16 11:15:18.701	nvidia:deepseek-ai/deepseek-r1	\N	32768	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	t	f	\N	\N	\N	medium	surveyor_heuristic	{"source": "surveyor_heuristic", "confidence": "medium", "capabilities": ["text", "reasoning"], "contextWindow": 32768}	chat	f
cmkgs80fv00jx8wbvfgp6ln67	2026-01-16 11:15:18.716	2026-01-16 11:15:18.716	mistral:pixtral-large-latest	\N	131072	\N	t	\N	\N	t	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	provider_api	{"source": "provider_api", "costPer1k": 0.15, "confidence": "high", "capabilities": ["text", "vision"], "contextWindow": 131072}	chat	f
cmkgs80ga00k38wbvxi3dynqm	2026-01-16 11:15:18.73	2026-01-16 11:15:18.73	mistral:mistral-large-pixtral-2411	\N	131072	8192	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	provider_api	{"source": "provider_api", "costPer1k": 2, "maxOutput": 8192, "confidence": "high", "capabilities": ["text", "tool_use"], "contextWindow": 131072}	chat	f
cmkgs80gn00k78wbvl50vz4j0	2026-01-16 11:15:18.743	2026-01-16 11:15:18.743	mistral:codestral-2508	\N	256000	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	provider_api	{"source": "provider_api", "costPer1k": 0.3, "confidence": "high", "capabilities": ["text", "code"], "contextWindow": 256000}	chat	f
cmkgs80h000kb8wbvo6mo56rt	2026-01-16 11:15:18.756	2026-01-16 11:15:18.756	mistral:codestral-latest	\N	256000	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	provider_api	{"source": "provider_api", "costPer1k": 0.3, "confidence": "high", "capabilities": ["text", "code"], "contextWindow": 256000}	chat	f
cmkgs80he00kf8wbvnmxvge25	2026-01-16 11:15:18.77	2026-01-16 11:15:18.77	mistral:devstral-small-2507	\N	131072	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	provider_api	{"source": "provider_api", "confidence": "high", "capabilities": ["text"], "contextWindow": 131072}	chat	f
cmkgs80hr00kj8wbvbmlqd0tb	2026-01-16 11:15:18.783	2026-01-16 11:15:18.783	mistral:devstral-medium-2507	\N	131072	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	provider_api	{"source": "provider_api", "confidence": "high", "capabilities": ["text", "medical"], "contextWindow": 131072}	chat	f
cmkgs80i400kn8wbvwqatt09s	2026-01-16 11:15:18.796	2026-01-16 11:15:18.796	mistral:devstral-2512	\N	262144	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	provider_api	{"source": "provider_api", "confidence": "high", "capabilities": ["text"], "contextWindow": 262144}	chat	f
cmkgs80ih00kr8wbvs56x13zl	2026-01-16 11:15:18.809	2026-01-16 11:15:18.809	mistral:mistral-vibe-cli-latest	\N	262144	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	provider_api	{"source": "provider_api", "confidence": "high", "capabilities": ["text"], "contextWindow": 262144}	chat	f
cmkgs80iu00kv8wbva7yu97d7	2026-01-16 11:15:18.822	2026-01-16 11:15:18.822	mistral:devstral-medium-latest	\N	262144	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	provider_api	{"source": "provider_api", "confidence": "high", "capabilities": ["text", "medical"], "contextWindow": 262144}	chat	f
cmkgs80j600kz8wbvibo3jzhd	2026-01-16 11:15:18.835	2026-01-16 11:15:18.835	mistral:devstral-latest	\N	262144	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	provider_api	{"source": "provider_api", "confidence": "high", "capabilities": ["text"], "contextWindow": 262144}	chat	f
cmkgs80jk00l38wbvkf5zvdx6	2026-01-16 11:15:18.848	2026-01-16 11:15:18.848	mistral:labs-devstral-small-2512	\N	262144	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	provider_api	{"source": "provider_api", "confidence": "high", "capabilities": ["text"], "contextWindow": 262144}	chat	f
cmkgs80jx00l78wbv33xyfvjm	2026-01-16 11:15:18.862	2026-01-16 11:15:18.862	nvidia:deepseek-ai/deepseek-coder-6.7b-instruct	\N	32768	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text", "code"], "contextWindow": 32768}	chat	f
cmkgs80kb00lb8wbvqyg7vfga	2026-01-16 11:15:18.875	2026-01-16 11:15:18.875	mistral:devstral-small-latest	\N	262144	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	provider_api	{"source": "provider_api", "confidence": "high", "capabilities": ["text"], "contextWindow": 262144}	chat	f
cmkgs80kn00lf8wbvk26wcl0i	2026-01-16 11:15:18.887	2026-01-16 11:15:18.887	mistral:mistral-small-2506	\N	131072	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	provider_api	{"source": "provider_api", "costPer1k": 0.5, "confidence": "high", "capabilities": ["text", "tool_use"], "contextWindow": 131072}	chat	f
cmkgs80l000lj8wbvis2w56z1	2026-01-16 11:15:18.9	2026-01-16 11:15:18.9	mistral:mistral-small-latest	\N	131072	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	provider_api	{"source": "provider_api", "costPer1k": 0.5, "confidence": "high", "capabilities": ["text", "tool_use"], "contextWindow": 131072}	chat	f
cmkgs80ld00ln8wbvn9ubfnci	2026-01-16 11:15:18.913	2026-01-16 11:15:18.913	mistral:labs-mistral-small-creative	\N	32768	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	provider_api	{"source": "provider_api", "costPer1k": 0.5, "confidence": "high", "capabilities": ["text", "tool_use"], "contextWindow": 32768}	chat	f
cmkgs80lp00lr8wbvnk9ml32i	2026-01-16 11:15:18.926	2026-01-16 11:15:18.926	mistral:magistral-medium-2509	\N	131072	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	provider_api	{"source": "provider_api", "confidence": "high", "capabilities": ["text", "medical"], "contextWindow": 131072}	chat	f
cmkgs80m100lv8wbvznldo5lc	2026-01-16 11:15:18.937	2026-01-16 11:15:18.937	mistral:magistral-medium-latest	\N	131072	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	provider_api	{"source": "provider_api", "confidence": "high", "capabilities": ["text", "medical"], "contextWindow": 131072}	chat	f
cmkgs80mc00lz8wbvblbyu2m3	2026-01-16 11:15:18.949	2026-01-16 11:15:18.949	mistral:magistral-small-2509	\N	131072	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	provider_api	{"source": "provider_api", "confidence": "high", "capabilities": ["text"], "contextWindow": 131072}	chat	f
cmkgs80mp00m38wbvfoczjwqo	2026-01-16 11:15:18.961	2026-01-16 11:15:18.961	mistral:magistral-small-latest	\N	131072	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	provider_api	{"source": "provider_api", "confidence": "high", "capabilities": ["text"], "contextWindow": 131072}	chat	f
cmkgs80n200m78wbvqa1gaokr	2026-01-16 11:15:18.974	2026-01-16 11:15:18.974	mistral:voxtral-small-2507	\N	32768	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0.15, "confidence": "high", "capabilities": ["text", "audio_in"], "contextWindow": 32768}	chat	f
cmkgs80nj00md8wbvsljkeapg	2026-01-16 11:15:18.992	2026-01-16 11:15:18.992	mistral:voxtral-mini-latest	\N	32768	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0.15, "confidence": "high", "capabilities": ["text", "audio_in"], "contextWindow": 32768}	chat	f
cmkgs80o200mj8wbvbn6u54rx	2026-01-16 11:15:19.01	2026-01-16 11:15:19.01	mistral:voxtral-small-latest	\N	32768	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0.15, "confidence": "high", "capabilities": ["text", "audio_in"], "contextWindow": 32768}	chat	f
cmkgs80oo00mp8wbvw8311ri9	2026-01-16 11:15:19.032	2026-01-16 11:15:19.032	mistral:mistral-large-2512	\N	262144	8192	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	provider_api	{"source": "provider_api", "costPer1k": 2, "maxOutput": 8192, "confidence": "high", "capabilities": ["text", "tool_use"], "contextWindow": 262144}	chat	f
cmkgs80p000mt8wbvykbgezf5	2026-01-16 11:15:19.045	2026-01-16 11:15:19.045	mistral:mistral-large-latest	\N	262144	8192	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	provider_api	{"source": "provider_api", "costPer1k": 2, "maxOutput": 8192, "confidence": "high", "capabilities": ["text", "tool_use"], "contextWindow": 262144}	chat	f
cmkgs80pd00mx8wbv0abwv402	2026-01-16 11:15:19.058	2026-01-16 11:15:19.058	mistral:ministral-3b-2512	\N	131072	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	provider_api	{"source": "provider_api", "costPer1k": 0.1, "confidence": "high", "capabilities": ["text", "tool_use"], "contextWindow": 131072}	chat	f
cmkgs80pq00n18wbvtyu3lthc	2026-01-16 11:15:19.071	2026-01-16 11:15:19.071	mistral:ministral-3b-latest	\N	131072	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	provider_api	{"source": "provider_api", "costPer1k": 0.1, "confidence": "high", "capabilities": ["text", "tool_use"], "contextWindow": 131072}	chat	f
cmkgs80q400n58wbvv20ntdpp	2026-01-16 11:15:19.084	2026-01-16 11:15:19.084	mistral:ministral-8b-2512	\N	262144	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	provider_api	{"source": "provider_api", "costPer1k": 0.1, "confidence": "high", "capabilities": ["text", "tool_use"], "contextWindow": 262144}	chat	f
cmkgs80qg00n98wbv43qox24d	2026-01-16 11:15:19.096	2026-01-16 11:15:19.096	mistral:ministral-8b-latest	\N	262144	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	provider_api	{"source": "provider_api", "costPer1k": 0.1, "confidence": "high", "capabilities": ["text", "tool_use"], "contextWindow": 262144}	chat	f
cmkgs80qt00nd8wbvruj1t4cc	2026-01-16 11:15:19.109	2026-01-16 11:15:19.109	mistral:ministral-14b-2512	\N	262144	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	provider_api	{"source": "provider_api", "costPer1k": 0.1, "confidence": "high", "capabilities": ["text", "tool_use"], "contextWindow": 262144}	chat	f
cmkgs80r600nh8wbvw4tkauxy	2026-01-16 11:15:19.122	2026-01-16 11:15:19.122	mistral:ministral-14b-latest	\N	262144	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	provider_api	{"source": "provider_api", "costPer1k": 0.1, "confidence": "high", "capabilities": ["text", "tool_use"], "contextWindow": 262144}	chat	f
cmkgs80rj00nl8wbvbdzdktba	2026-01-16 11:15:19.135	2026-01-16 11:15:19.135	mistral:open-mistral-7b	\N	32768	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	provider_api	{"source": "provider_api", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 32768}	chat	f
cmkgs80rw00np8wbvqjelj6ke	2026-01-16 11:15:19.148	2026-01-16 11:15:19.148	mistral:mistral-tiny	\N	32768	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	provider_api	{"source": "provider_api", "costPer1k": 0.5, "confidence": "high", "capabilities": ["text", "tool_use"], "contextWindow": 32768}	chat	f
cmkgs80sa00nt8wbv77j0nmtj	2026-01-16 11:15:19.162	2026-01-16 11:15:19.162	nvidia:nvidia/embed-qa-4	\N	2048	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	t	\N	\N	\N	medium	surveyor_heuristic	{"source": "surveyor_heuristic", "confidence": "medium", "primaryTask": "embedding", "capabilities": ["embedding"], "contextWindow": 2048}	embedding	f
cmkgs80sj00nx8wbvppaat46y	2026-01-16 11:15:19.172	2026-01-16 11:15:19.172	mistral:mistral-tiny-2312	\N	32768	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	provider_api	{"source": "provider_api", "costPer1k": 0.5, "confidence": "high", "capabilities": ["text", "tool_use"], "contextWindow": 32768}	chat	f
cmkgs80sw00o18wbvjmg8ny8l	2026-01-16 11:15:19.184	2026-01-16 11:15:19.184	mistral:pixtral-12b-2409	\N	131072	\N	t	\N	\N	t	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	provider_api	{"source": "provider_api", "costPer1k": 0.15, "confidence": "high", "capabilities": ["text", "vision"], "contextWindow": 131072}	chat	f
cmkgs80ta00o78wbvdcmsyv4j	2026-01-16 11:15:19.198	2026-01-16 11:15:19.198	mistral:pixtral-12b	\N	131072	\N	t	\N	\N	t	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	provider_api	{"source": "provider_api", "costPer1k": 0.15, "confidence": "high", "capabilities": ["text", "vision"], "contextWindow": 131072}	chat	f
cmkgs80to00od8wbvj4jsas35	2026-01-16 11:15:19.213	2026-01-16 11:15:19.213	mistral:pixtral-12b-latest	\N	131072	\N	t	\N	\N	t	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	provider_api	{"source": "provider_api", "costPer1k": 0.15, "confidence": "high", "capabilities": ["text", "vision"], "contextWindow": 131072}	chat	f
cmkgs80u300oj8wbvkdahv1un	2026-01-16 11:15:19.227	2026-01-16 11:15:19.227	mistral:ministral-3b-2410	\N	131072	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	provider_api	{"source": "provider_api", "costPer1k": 0.1, "confidence": "high", "capabilities": ["text", "tool_use"], "contextWindow": 131072}	chat	f
cmkgs80uf00on8wbv9bqrn5t2	2026-01-16 11:15:19.239	2026-01-16 11:15:19.239	mistral:ministral-8b-2410	\N	131072	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	provider_api	{"source": "provider_api", "costPer1k": 0.1, "confidence": "high", "capabilities": ["text", "tool_use"], "contextWindow": 131072}	chat	f
cmkgs80ur00or8wbvtatnmtg6	2026-01-16 11:15:19.251	2026-01-16 11:15:19.251	mistral:codestral-2501	\N	256000	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	provider_api	{"source": "provider_api", "costPer1k": 0.3, "confidence": "high", "capabilities": ["text", "code"], "contextWindow": 256000}	chat	f
cmkgs80v400ov8wbvu513jdjc	2026-01-16 11:15:19.264	2026-01-16 11:15:19.264	mistral:codestral-2412	\N	256000	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	provider_api	{"source": "provider_api", "costPer1k": 0.3, "confidence": "high", "capabilities": ["text", "code"], "contextWindow": 256000}	chat	f
cmkgs80vf00oz8wbvccjvwo4m	2026-01-16 11:15:19.276	2026-01-16 11:15:19.276	mistral:codestral-2411-rc5	\N	256000	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	provider_api	{"source": "provider_api", "costPer1k": 0.3, "confidence": "high", "capabilities": ["text", "code"], "contextWindow": 256000}	chat	f
cmkgs80vs00p38wbvv7vf6jt4	2026-01-16 11:15:19.288	2026-01-16 11:15:19.288	mistral:mistral-small-2501	\N	32768	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	provider_api	{"source": "provider_api", "costPer1k": 0.5, "confidence": "high", "capabilities": ["text", "tool_use"], "contextWindow": 32768}	chat	f
cmkgs80w800p78wbv361vaeh6	2026-01-16 11:15:19.304	2026-01-16 11:15:19.304	groq:meta-llama/llama-4-scout-17b-16e-instruct	\N	131072	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	provider_api	{"source": "provider_api", "costPer1k": 0, "confidence": "high", "capabilities": ["text", "tool_use"], "contextWindow": 131072}	chat	f
cmkgs80wo00pb8wbvzw56ocnd	2026-01-16 11:15:19.32	2026-01-16 11:15:19.32	mistral:mistral-embed-2312	\N	8192	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	t	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0.1, "confidence": "high", "primaryTask": "embedding", "capabilities": ["embedding"], "contextWindow": 8192}	embedding	f
cmkgs80ww00pf8wbvx1xga5qj	2026-01-16 11:15:19.328	2026-01-16 11:15:19.328	mistral:mistral-embed	\N	8192	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	t	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0.1, "confidence": "high", "primaryTask": "embedding", "capabilities": ["embedding"], "contextWindow": 8192}	embedding	f
cmkgs80x500pj8wbved61p711	2026-01-16 11:15:19.337	2026-01-16 11:15:19.337	mistral:codestral-embed	\N	32000	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0.3, "confidence": "high", "capabilities": ["text", "code"], "contextWindow": 32000}	chat	f
cmkgs80xg00pn8wbvokya50iu	2026-01-16 11:15:19.349	2026-01-16 11:15:19.349	mistral:codestral-embed-2505	\N	32000	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0.3, "confidence": "high", "capabilities": ["text", "code"], "contextWindow": 32000}	chat	f
cmkgs80xt00pr8wbv9o5t6asa	2026-01-16 11:15:19.361	2026-01-16 11:15:19.361	mistral:mistral-moderation-2411	\N	8192	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["moderation"], "contextWindow": 8192}	chat	f
cmkgs80y200pv8wbvdind4vu2	2026-01-16 11:15:19.371	2026-01-16 11:15:19.371	mistral:mistral-moderation-latest	\N	8192	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["moderation"], "contextWindow": 8192}	chat	f
cmkgs80yb00pz8wbvrek2eu79	2026-01-16 11:15:19.379	2026-01-16 11:15:19.379	mistral:mistral-ocr-2512	\N	16384	\N	t	\N	\N	t	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	provider_api	{"source": "provider_api", "costPer1k": 0, "confidence": "high", "primaryTask": "ocr", "capabilities": ["vision", "ocr"], "contextWindow": 16384}	ocr	f
cmkgs80yn00q38wbvq59fet7g	2026-01-16 11:15:19.391	2026-01-16 11:15:19.391	mistral:mistral-ocr-latest	\N	16384	\N	t	\N	\N	t	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	provider_api	{"source": "provider_api", "costPer1k": 0, "confidence": "high", "primaryTask": "ocr", "capabilities": ["vision", "ocr"], "contextWindow": 16384}	ocr	f
cmkgs80z000q78wbvh3vvg1uo	2026-01-16 11:15:19.404	2026-01-16 11:15:19.404	mistral:mistral-ocr-2505	\N	16384	\N	t	\N	\N	t	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	provider_api	{"source": "provider_api", "costPer1k": 0, "confidence": "high", "primaryTask": "ocr", "capabilities": ["vision", "ocr"], "contextWindow": 16384}	ocr	f
cmkgs80zb00qb8wbv4olutmv8	2026-01-16 11:15:19.416	2026-01-16 11:15:19.416	mistral:mistral-ocr-2503	\N	16384	\N	t	\N	\N	t	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	provider_api	{"source": "provider_api", "costPer1k": 0, "confidence": "high", "primaryTask": "ocr", "capabilities": ["vision", "ocr"], "contextWindow": 16384}	ocr	f
cmkgs80zo00qf8wbvc8k6lqf6	2026-01-16 11:15:19.429	2026-01-16 11:15:19.429	mistral:voxtral-mini-transcribe-2507	\N	32768	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0.15, "confidence": "high", "capabilities": ["text", "audio_in"], "contextWindow": 32768}	chat	f
cmkgs810600ql8wbv2lokmo5h	2026-01-16 11:15:19.446	2026-01-16 11:15:19.446	nvidia:01-ai/yi-large	\N	32768	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 32768}	chat	f
cmkgs810i00qp8wbvxqr14at2	2026-01-16 11:15:19.458	2026-01-16 11:15:19.458	mistral:voxtral-mini-2507	\N	32768	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0.15, "confidence": "high", "capabilities": ["text", "audio_in"], "contextWindow": 32768}	chat	f
cmkgs811000qv8wbvbm9l8363	2026-01-16 11:15:19.476	2026-01-16 11:15:19.476	openrouter:allenai/molmo-2-8b:free	\N	36864	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	provider_api	{"source": "provider_api", "confidence": "high", "capabilities": ["text"], "contextWindow": 36864}	chat	f
cmkgs811d00qz8wbv2wj0qbw9	2026-01-16 11:15:19.49	2026-01-16 11:15:19.49	openrouter:xiaomi/mimo-v2-flash:free	\N	262144	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	provider_api	{"source": "provider_api", "confidence": "high", "capabilities": ["text"], "contextWindow": 262144}	chat	f
cmkgs811q00r38wbvur8vqpcg	2026-01-16 11:15:19.502	2026-01-16 11:15:19.502	openrouter:nvidia/nemotron-3-nano-30b-a3b:free	\N	256000	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	provider_api	{"source": "provider_api", "confidence": "high", "capabilities": ["text"], "contextWindow": 256000}	chat	f
cmkgs812200r78wbv93s8y6ot	2026-01-16 11:15:19.514	2026-01-16 11:15:19.514	openrouter:mistralai/devstral-2512:free	\N	262144	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	provider_api	{"source": "provider_api", "confidence": "high", "capabilities": ["text"], "contextWindow": 262144}	chat	f
cmkgs812l00rd8wbvneqgqcf0	2026-01-16 11:15:19.533	2026-01-16 11:15:19.533	nvidia:adept/fuyu-8b	\N	16384	\N	t	\N	\N	t	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text", "vision"], "contextWindow": 16384}	chat	f
cmkgs813200rj8wbv8q4to04v	2026-01-16 11:15:19.55	2026-01-16 11:15:19.55	nvidia:thudm/chatglm3-6b	\N	32768	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 32768}	chat	f
cmkgs813e00rn8wbvybfwcwjx	2026-01-16 11:15:19.562	2026-01-16 11:15:19.562	openrouter:arcee-ai/trinity-mini:free	\N	131072	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	provider_api	{"source": "provider_api", "confidence": "high", "capabilities": ["text"], "contextWindow": 131072}	chat	f
cmkgs813q00rr8wbvttwillbk	2026-01-16 11:15:19.574	2026-01-16 11:15:19.574	openrouter:tngtech/tng-r1t-chimera:free	\N	163840	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	provider_api	{"source": "provider_api", "confidence": "high", "capabilities": ["text"], "contextWindow": 163840}	chat	f
cmkgs814100rv8wbvu09a6g8s	2026-01-16 11:15:19.586	2026-01-16 11:15:19.586	openrouter:nvidia/nemotron-nano-12b-v2-vl:free	\N	128000	\N	t	\N	\N	t	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	provider_api	{"source": "provider_api", "confidence": "high", "capabilities": ["text", "vision"], "contextWindow": 128000}	chat	f
cmkgs814f00s18wbvsxt5nalb	2026-01-16 11:15:19.599	2026-01-16 11:15:19.599	openrouter:qwen/qwen3-next-80b-a3b-instruct:free	\N	262144	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	provider_api	{"source": "provider_api", "costPer1k": 0.1, "confidence": "high", "capabilities": ["text"], "contextWindow": 262144}	chat	f
cmkgs814r00s58wbvhov9zpjr	2026-01-16 11:15:19.612	2026-01-16 11:15:19.612	openrouter:nvidia/nemotron-nano-9b-v2:free	\N	128000	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	provider_api	{"source": "provider_api", "confidence": "high", "capabilities": ["text"], "contextWindow": 128000}	chat	f
cmkgs815300s98wbvn79u4bzs	2026-01-16 11:15:19.623	2026-01-16 11:15:19.623	openrouter:openai/gpt-oss-120b:free	\N	131072	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	provider_api	{"source": "provider_api", "confidence": "high", "capabilities": ["text"], "contextWindow": 131072}	chat	f
cmkgs815g00sd8wbvumnvwqru	2026-01-16 11:15:19.636	2026-01-16 11:15:19.636	nvidia:ai21labs/jamba-1.5-large-instruct	\N	256000	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	t	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text", "reasoning"], "contextWindow": 256000}	chat	f
cmkgs815t00sh8wbvxq4ukeut	2026-01-16 11:15:19.649	2026-01-16 11:15:19.649	nvidia:ai21labs/jamba-1.5-mini-instruct	\N	256000	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	t	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text", "reasoning"], "contextWindow": 256000}	chat	f
cmkgs816400sl8wbv9c4b3fud	2026-01-16 11:15:19.661	2026-01-16 11:15:19.661	nvidia:aisingapore/sea-lion-7b-instruct	\N	32768	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 32768}	chat	f
cmkgs816h00sp8wbvt2tci5hi	2026-01-16 11:15:19.673	2026-01-16 11:15:19.673	nvidia:baai/bge-m3	\N	8192	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	t	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "primaryTask": "embedding", "capabilities": ["embedding"], "contextWindow": 8192}	embedding	f
cmkgs816t00st8wbvklsln7w7	2026-01-16 11:15:19.685	2026-01-16 11:15:19.685	openrouter:openai/gpt-oss-20b:free	\N	131072	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	provider_api	{"source": "provider_api", "confidence": "high", "capabilities": ["text"], "contextWindow": 131072}	chat	f
cmkgs817600sx8wbv7bzxasjm	2026-01-16 11:15:19.699	2026-01-16 11:15:19.699	openrouter:z-ai/glm-4.5-air:free	\N	131072	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	provider_api	{"source": "provider_api", "confidence": "high", "capabilities": ["text"], "contextWindow": 131072}	chat	f
cmkgs817j00t18wbviwjnyme5	2026-01-16 11:15:19.711	2026-01-16 11:15:19.711	openrouter:qwen/qwen3-coder:free	\N	262000	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	provider_api	{"source": "provider_api", "costPer1k": 0.1, "confidence": "high", "capabilities": ["text"], "contextWindow": 262000}	chat	f
cmkgs817v00t58wbvxn2j17jd	2026-01-16 11:15:19.723	2026-01-16 11:15:19.723	openrouter:moonshotai/kimi-k2:free	\N	32768	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	provider_api	{"source": "provider_api", "confidence": "high", "capabilities": ["text"], "contextWindow": 32768}	chat	f
cmkgs818700t98wbvazf5huoq	2026-01-16 11:15:19.736	2026-01-16 11:15:19.736	openrouter:cognitivecomputations/dolphin-mistral-24b-venice-edition:free	\N	32768	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	provider_api	{"source": "provider_api", "confidence": "high", "capabilities": ["text"], "contextWindow": 32768}	chat	f
cmkgs818k00td8wbv1y2y0puh	2026-01-16 11:15:19.749	2026-01-16 11:15:19.749	openrouter:google/gemma-3n-e2b-it:free	\N	8192	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text", "tool_use"], "contextWindow": 8192}	chat	f
cmkgs818x00th8wbv6eyp1ic3	2026-01-16 11:15:19.761	2026-01-16 11:15:19.761	openrouter:tngtech/deepseek-r1t2-chimera:free	\N	163840	32768	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	t	f	\N	\N	\N	high	provider_api	{"source": "provider_api", "costPer1k": 0.55, "maxOutput": 32768, "confidence": "high", "capabilities": ["text", "reasoning"], "contextWindow": 163840}	chat	f
cmkgs819b00tl8wbvius8kyxp	2026-01-16 11:15:19.775	2026-01-16 11:15:19.775	openrouter:deepseek/deepseek-r1-0528:free	\N	163840	32768	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	t	f	\N	\N	\N	high	provider_api	{"source": "provider_api", "costPer1k": 0.55, "maxOutput": 32768, "confidence": "high", "capabilities": ["text", "reasoning"], "contextWindow": 163840}	chat	f
cmkgs819n00tp8wbv5ac8b86g	2026-01-16 11:15:19.787	2026-01-16 11:15:19.787	openrouter:google/gemma-3n-e4b-it:free	\N	8192	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text", "tool_use"], "contextWindow": 8192}	chat	f
cmkgs819z00tt8wbv0hj3to2b	2026-01-16 11:15:19.799	2026-01-16 11:15:19.799	openrouter:qwen/qwen3-4b:free	\N	40960	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	provider_api	{"source": "provider_api", "costPer1k": 0.1, "confidence": "high", "capabilities": ["text"], "contextWindow": 40960}	chat	f
cmkgs81ac00tx8wbvc0qqxkau	2026-01-16 11:15:19.812	2026-01-16 11:15:19.812	openrouter:tngtech/deepseek-r1t-chimera:free	\N	163840	32768	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	t	f	\N	\N	\N	high	provider_api	{"source": "provider_api", "costPer1k": 0.55, "maxOutput": 32768, "confidence": "high", "capabilities": ["text", "reasoning"], "contextWindow": 163840}	chat	f
cmkgs81ap00u18wbvk58wvdsh	2026-01-16 11:15:19.825	2026-01-16 11:15:19.825	openrouter:mistralai/mistral-small-3.1-24b-instruct:free	\N	128000	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	provider_api	{"source": "provider_api", "costPer1k": 0.5, "confidence": "high", "capabilities": ["text", "tool_use"], "contextWindow": 128000}	chat	f
cmkgs81b100u58wbv4wjg9nps	2026-01-16 11:15:19.837	2026-01-16 11:15:19.837	openrouter:google/gemma-3-4b-it:free	\N	32768	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	provider_api	{"source": "provider_api", "costPer1k": 0, "confidence": "high", "capabilities": ["text", "tool_use"], "contextWindow": 32768}	chat	f
cmkgs81bd00u98wbvgz8x0ye3	2026-01-16 11:15:19.849	2026-01-16 11:15:19.849	openrouter:google/gemma-3-12b-it:free	\N	32768	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	provider_api	{"source": "provider_api", "costPer1k": 0, "confidence": "high", "capabilities": ["text", "tool_use"], "contextWindow": 32768}	chat	f
cmkgs81bp00ud8wbv4qytj5si	2026-01-16 11:15:19.862	2026-01-16 11:15:19.862	openrouter:google/gemma-3-27b-it:free	\N	131072	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	provider_api	{"source": "provider_api", "costPer1k": 0, "confidence": "high", "capabilities": ["text", "tool_use"], "contextWindow": 131072}	chat	f
cmkgs81c200uh8wbvwiqn5whn	2026-01-16 11:15:19.875	2026-01-16 11:15:19.875	openrouter:google/gemini-2.0-flash-exp:free	\N	1048576	8192	t	\N	\N	t	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	provider_api	{"source": "provider_api", "costPer1k": 0.1, "maxOutput": 8192, "confidence": "high", "capabilities": ["text", "vision", "audio_in", "video_in", "tool_use"], "contextWindow": 1048576}	chat	f
cmkgs81cw00up8wbvpdtp1aj4	2026-01-16 11:15:19.905	2026-01-16 11:15:19.905	nvidia:baichuan-inc/baichuan2-13b-chat	\N	32768	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 32768}	chat	f
cmkgs81d900ut8wbvig6hpptf	2026-01-16 11:15:19.918	2026-01-16 11:15:19.918	nvidia:bigcode/starcoder2-15b	\N	16384	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text", "code"], "contextWindow": 16384}	chat	f
cmkgs81dl00ux8wbv3wtoa059	2026-01-16 11:15:19.929	2026-01-16 11:15:19.929	openrouter:meta-llama/llama-3.3-70b-instruct:free	\N	131072	8192	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	provider_api	{"source": "provider_api", "costPer1k": 0, "maxOutput": 8192, "confidence": "high", "capabilities": ["text", "tool_use"], "contextWindow": 131072}	chat	f
cmkgs81dx00v18wbvhll5wnih	2026-01-16 11:15:19.942	2026-01-16 11:15:19.942	openrouter:meta-llama/llama-3.2-3b-instruct:free	\N	131072	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	provider_api	{"source": "provider_api", "costPer1k": 0.2, "confidence": "high", "capabilities": ["text", "tool_use"], "contextWindow": 131072}	chat	f
cmkgs81ea00v58wbv6rdkeizk	2026-01-16 11:15:19.954	2026-01-16 11:15:19.954	openrouter:qwen/qwen-2.5-vl-7b-instruct:free	\N	32768	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0.1, "confidence": "high", "capabilities": ["text"], "contextWindow": 32768}	chat	f
cmkgs81en00v98wbvy1evfhnb	2026-01-16 11:15:19.968	2026-01-16 11:15:19.968	openrouter:nousresearch/hermes-3-llama-3.1-405b:free	\N	131072	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	provider_api	{"source": "provider_api", "costPer1k": 0.2, "confidence": "high", "capabilities": ["text", "tool_use", "uncensored"], "contextWindow": 131072}	chat	f
cmkgs81f700vd8wbvqc5xb9jw	2026-01-16 11:15:19.987	2026-01-16 11:15:19.987	nvidia:bigcode/starcoder2-7b	\N	16384	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text", "code"], "contextWindow": 16384}	chat	f
cmkgs81fj00vh8wbvesrccg1y	2026-01-16 11:15:19.999	2026-01-16 11:15:19.999	nvidia:bytedance/seed-oss-36b-instruct	\N	32768	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 32768}	chat	f
cmkgs81fw00vl8wbv9gwcbxgi	2026-01-16 11:15:20.012	2026-01-16 11:15:20.012	nvidia:databricks/dbrx-instruct	\N	32768	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 32768}	chat	f
cmkgs81g800vp8wbvgfg3k68r	2026-01-16 11:15:20.025	2026-01-16 11:15:20.025	openrouter:meta-llama/llama-3.1-405b-instruct:free	\N	131072	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	provider_api	{"source": "provider_api", "costPer1k": 0.2, "confidence": "high", "capabilities": ["text", "tool_use"], "contextWindow": 131072}	chat	f
cmkgs81gk00vt8wbvi61jdhot	2026-01-16 11:15:20.036	2026-01-16 11:15:20.036	groq:meta-llama/llama-4-maverick-17b-128e-instruct	\N	131072	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	provider_api	{"source": "provider_api", "costPer1k": 0, "confidence": "high", "capabilities": ["text", "tool_use"], "contextWindow": 131072}	chat	f
cmkgs81gw00vx8wbvbl8m3jnp	2026-01-16 11:15:20.048	2026-01-16 11:15:20.048	groq:llama-3.3-70b-versatile	\N	131072	8192	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	provider_api	{"source": "provider_api", "costPer1k": 0, "maxOutput": 8192, "confidence": "high", "capabilities": ["text", "tool_use"], "contextWindow": 131072}	chat	f
cmkgs81h700w18wbvphn57e6f	2026-01-16 11:15:20.06	2026-01-16 11:15:20.06	groq:meta-llama/llama-guard-4-12b	\N	131072	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	provider_api	{"source": "provider_api", "costPer1k": 0, "confidence": "high", "primaryTask": "moderation", "capabilities": ["moderation"], "contextWindow": 131072}	moderation	f
cmkgs81hg00w58wbvottambuz	2026-01-16 11:15:20.068	2026-01-16 11:15:20.068	groq:groq/compound-mini	\N	131072	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	provider_api	{"source": "provider_api", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 131072}	chat	f
cmkgs81hs00w98wbvp2amjhb7	2026-01-16 11:15:20.081	2026-01-16 11:15:20.081	groq:moonshotai/kimi-k2-instruct	\N	131072	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	provider_api	{"source": "provider_api", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 131072}	chat	f
cmkgs81i400wd8wbvo9manwnq	2026-01-16 11:15:20.093	2026-01-16 11:15:20.093	groq:openai/gpt-oss-safeguard-20b	\N	131072	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	provider_api	{"source": "provider_api", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 131072}	chat	f
cmkgs81ig00wh8wbvbhegfpn6	2026-01-16 11:15:20.105	2026-01-16 11:15:20.105	groq:whisper-large-v3	\N	0	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "primaryTask": "tts", "capabilities": ["audio_in"], "contextWindow": 0}	tts	f
cmkgs81is00wl8wbv51tiri6i	2026-01-16 11:15:20.117	2026-01-16 11:15:20.117	groq:moonshotai/kimi-k2-instruct-0905	\N	262144	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	provider_api	{"source": "provider_api", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 262144}	chat	f
cmkgs81j400wp8wbvgi925175	2026-01-16 11:15:20.129	2026-01-16 11:15:20.129	groq:qwen/qwen3-32b	\N	131072	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	provider_api	{"source": "provider_api", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 131072}	chat	f
cmkgs81jh00wt8wbvlgpxng1e	2026-01-16 11:15:20.141	2026-01-16 11:15:20.141	groq:allam-2-7b	\N	32768	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 32768}	chat	f
cmkgs81jt00wx8wbvf7v31toj	2026-01-16 11:15:20.153	2026-01-16 11:15:20.153	groq:meta-llama/llama-prompt-guard-2-22m	\N	8192	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "primaryTask": "moderation", "capabilities": ["moderation"], "contextWindow": 8192}	moderation	f
cmkgs81k200x18wbvh3sgqi9d	2026-01-16 11:15:20.162	2026-01-16 11:15:20.162	groq:openai/gpt-oss-20b	\N	131072	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	provider_api	{"source": "provider_api", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 131072}	chat	f
cmkgs81ke00x58wbvr9nc2w6m	2026-01-16 11:15:20.175	2026-01-16 11:15:20.175	groq:whisper-large-v3-turbo	\N	0	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "primaryTask": "tts", "capabilities": ["audio_in"], "contextWindow": 0}	tts	f
cmkgs81kq00x98wbvlash8ulr	2026-01-16 11:15:20.187	2026-01-16 11:15:20.187	groq:groq/compound	\N	131072	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	provider_api	{"source": "provider_api", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 131072}	chat	f
cmkgs81l300xd8wbvo5ua6cep	2026-01-16 11:15:20.199	2026-01-16 11:15:20.199	groq:canopylabs/orpheus-v1-english	\N	32768	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["audio_in", "text"], "contextWindow": 32768}	chat	f
cmkgs81ll00xj8wbv7rwpu1b1	2026-01-16 11:15:20.217	2026-01-16 11:15:20.217	groq:llama-3.1-8b-instant	\N	131072	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	provider_api	{"source": "provider_api", "confidence": "high", "capabilities": ["text"], "contextWindow": 131072}	chat	f
cmkgs81lx00xn8wbvk3pn98dr	2026-01-16 11:15:20.229	2026-01-16 11:15:20.229	groq:meta-llama/llama-prompt-guard-2-86m	\N	8192	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "primaryTask": "moderation", "capabilities": ["moderation"], "contextWindow": 8192}	moderation	f
cmkgs81m500xr8wbvg3lwzi4u	2026-01-16 11:15:20.238	2026-01-16 11:15:20.238	groq:openai/gpt-oss-120b	\N	131072	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	provider_api	{"source": "provider_api", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 131072}	chat	f
cmkgs81mh00xv8wbv5846k70d	2026-01-16 11:15:20.249	2026-01-16 11:15:20.249	nvidia:deepseek-ai/deepseek-r1-0528	\N	32768	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	t	f	\N	\N	\N	medium	surveyor_heuristic	{"source": "surveyor_heuristic", "confidence": "medium", "capabilities": ["text", "reasoning"], "contextWindow": 32768}	chat	f
cmkgs81mt00xz8wbvpckmwmrx	2026-01-16 11:15:20.261	2026-01-16 11:15:20.261	nvidia:deepseek-ai/deepseek-r1-distill-llama-8b	\N	32768	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	t	f	\N	\N	\N	medium	surveyor_heuristic	{"source": "surveyor_heuristic", "confidence": "medium", "capabilities": ["text", "reasoning"], "contextWindow": 32768}	chat	f
cmkgs81n600y38wbvbtovv17q	2026-01-16 11:15:20.274	2026-01-16 11:15:20.274	nvidia:deepseek-ai/deepseek-r1-distill-qwen-14b	\N	32768	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 32768}	chat	f
cmkgs81ni00y78wbvqvouq96v	2026-01-16 11:15:20.287	2026-01-16 11:15:20.287	nvidia:deepseek-ai/deepseek-r1-distill-qwen-32b	\N	32768	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 32768}	chat	f
cmkgs81nu00yb8wbv1ehlnvza	2026-01-16 11:15:20.298	2026-01-16 11:15:20.298	nvidia:deepseek-ai/deepseek-r1-distill-qwen-7b	\N	32768	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 32768}	chat	f
cmkgs81o600yf8wbvliagjg95	2026-01-16 11:15:20.31	2026-01-16 11:15:20.31	nvidia:deepseek-ai/deepseek-v3.1	\N	128000	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	t	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text", "reasoning"], "contextWindow": 128000}	chat	f
cmkgs81oi00yj8wbvocxdu4s8	2026-01-16 11:15:20.322	2026-01-16 11:15:20.322	nvidia:deepseek-ai/deepseek-v3.1-terminus	\N	128000	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	t	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text", "reasoning"], "contextWindow": 128000}	chat	f
cmkgs81ou00yn8wbvrjkmye9m	2026-01-16 11:15:20.334	2026-01-16 11:15:20.334	nvidia:deepseek-ai/deepseek-v3.2	\N	128000	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	t	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text", "reasoning"], "contextWindow": 128000}	chat	f
cmkgs81p600yr8wbv5ksq7ft9	2026-01-16 11:15:20.346	2026-01-16 11:15:20.346	nvidia:google/codegemma-1.1-7b	\N	8192	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 8192}	chat	f
cmkgs81pi00yv8wbv5p71w9ze	2026-01-16 11:15:20.358	2026-01-16 11:15:20.358	nvidia:google/codegemma-7b	\N	8192	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 8192}	chat	f
cmkgs81pu00yz8wbvq00xbaeb	2026-01-16 11:15:20.37	2026-01-16 11:15:20.37	nvidia:google/deplot	\N	4096	\N	t	\N	\N	t	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "primaryTask": "ocr", "capabilities": ["vision", "ocr"], "contextWindow": 4096}	ocr	f
cmkgs81q500z38wbvkn97nhs8	2026-01-16 11:15:20.381	2026-01-16 11:15:20.381	nvidia:google/gemma-2-27b-it	\N	8192	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 8192}	chat	f
cmkgs81qi00z78wbv5cozusdv	2026-01-16 11:15:20.394	2026-01-16 11:15:20.394	nvidia:google/gemma-2-2b-it	\N	8192	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 8192}	chat	f
cmkgs81qu00zb8wbvnji03qvj	2026-01-16 11:15:20.407	2026-01-16 11:15:20.407	nvidia:google/gemma-2-9b-it	\N	8192	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 8192}	chat	f
cmkgs81r700zf8wbvnczf2at6	2026-01-16 11:15:20.419	2026-01-16 11:15:20.419	nvidia:google/gemma-2b	\N	8192	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 8192}	chat	f
cmkgs81rj00zj8wbv8td95o69	2026-01-16 11:15:20.431	2026-01-16 11:15:20.431	nvidia:google/gemma-3-12b-it	\N	8192	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 8192}	chat	f
cmkgs81rv00zn8wbvubnnzmmu	2026-01-16 11:15:20.443	2026-01-16 11:15:20.443	nvidia:google/gemma-3-1b-it	\N	8192	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 8192}	chat	f
cmkgs81s700zr8wbvak3u5t0o	2026-01-16 11:15:20.455	2026-01-16 11:15:20.455	nvidia:google/gemma-3-27b-it	\N	8192	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 8192}	chat	f
cmkgs81sj00zv8wbv9fvn5fwe	2026-01-16 11:15:20.467	2026-01-16 11:15:20.467	nvidia:google/gemma-3-4b-it	\N	8192	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 8192}	chat	f
cmkgs81sv00zz8wbv6q45wme8	2026-01-16 11:15:20.48	2026-01-16 11:15:20.48	nvidia:google/gemma-3n-e2b-it	\N	8192	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 8192}	chat	f
cmkgs81t801038wbvs2zhwdju	2026-01-16 11:15:20.492	2026-01-16 11:15:20.492	nvidia:google/gemma-3n-e4b-it	\N	8192	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 8192}	chat	f
cmkgs81tk01078wbv8bwal2ql	2026-01-16 11:15:20.505	2026-01-16 11:15:20.505	nvidia:google/gemma-7b	\N	8192	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 8192}	chat	f
cmkgs81tw010b8wbvlbuwh6lp	2026-01-16 11:15:20.516	2026-01-16 11:15:20.516	nvidia:google/paligemma	\N	8192	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 8192}	chat	f
cmkgs81u8010f8wbv8lrpsyxs	2026-01-16 11:15:20.528	2026-01-16 11:15:20.528	nvidia:google/recurrentgemma-2b	\N	8192	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 8192}	chat	f
cmkgs81uk010j8wbv3torbdfs	2026-01-16 11:15:20.54	2026-01-16 11:15:20.54	nvidia:google/shieldgemma-9b	\N	8192	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 8192}	chat	f
cmkgs81uw010n8wbvb5w51c0t	2026-01-16 11:15:20.553	2026-01-16 11:15:20.553	nvidia:gotocompany/gemma-2-9b-cpt-sahabatai-instruct	\N	8192	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 8192}	chat	f
cmkgs81v9010r8wbv2r1qke6v	2026-01-16 11:15:20.565	2026-01-16 11:15:20.565	nvidia:ibm/granite-3.0-3b-a800m-instruct	\N	8192	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 8192}	chat	f
cmkgs81vk010v8wbv02ol5t3s	2026-01-16 11:15:20.577	2026-01-16 11:15:20.577	nvidia:ibm/granite-3.0-8b-instruct	\N	8192	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 8192}	chat	f
cmkgs81vw010z8wbvirvhl37b	2026-01-16 11:15:20.588	2026-01-16 11:15:20.588	nvidia:ibm/granite-3.3-8b-instruct	\N	8192	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 8192}	chat	f
cmkgs81w801138wbvp7th6hsh	2026-01-16 11:15:20.601	2026-01-16 11:15:20.601	nvidia:ibm/granite-34b-code-instruct	\N	32768	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text", "code"], "contextWindow": 32768}	chat	f
cmkgs81wk01178wbvucstnitp	2026-01-16 11:15:20.613	2026-01-16 11:15:20.613	nvidia:ibm/granite-8b-code-instruct	\N	32768	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text", "code"], "contextWindow": 32768}	chat	f
cmkgs81wx011b8wbv4c0z405y	2026-01-16 11:15:20.625	2026-01-16 11:15:20.625	nvidia:ibm/granite-guardian-3.0-8b	\N	8192	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "primaryTask": "moderation", "capabilities": ["moderation"], "contextWindow": 8192}	moderation	f
cmkgs81x5011f8wbv1tpuxo9i	2026-01-16 11:15:20.634	2026-01-16 11:15:20.634	nvidia:igenius/colosseum_355b_instruct_16k	\N	16384	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	medium	name_inference	{"source": "name_inference", "confidence": "medium", "capabilities": ["text"], "contextWindow": 16384}	chat	f
cmkgs81xh011j8wbvsyzune4y	2026-01-16 11:15:20.646	2026-01-16 11:15:20.646	nvidia:igenius/italia_10b_instruct_16k	\N	16384	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	medium	name_inference	{"source": "name_inference", "confidence": "medium", "capabilities": ["text"], "contextWindow": 16384}	chat	f
cmkgs81y7011r8wbvyzv936g8	2026-01-16 11:15:20.671	2026-01-16 11:15:20.671	nvidia:marin/marin-8b-instruct	\N	8192	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 8192}	chat	f
cmkgs820v012p8wbv9y3exhfs	2026-01-16 11:15:20.767	2026-01-16 11:15:20.767	nvidia:meta/llama-4-maverick-17b-128e-instruct	\N	128000	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 128000}	chat	f
cmkgs8217012t8wbvpuzvyqrx	2026-01-16 11:15:20.78	2026-01-16 11:15:20.78	nvidia:meta/llama-4-scout-17b-16e-instruct	\N	128000	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 128000}	chat	f
cmkgs821k012x8wbv7e78fsxd	2026-01-16 11:15:20.792	2026-01-16 11:15:20.792	nvidia:meta/llama-guard-4-12b	\N	8192	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "primaryTask": "moderation", "capabilities": ["moderation"], "contextWindow": 8192}	moderation	f
cmkgs822h01378wbvo113it22	2026-01-16 11:15:20.825	2026-01-16 11:15:20.825	nvidia:microsoft/kosmos-2	\N	32768	\N	t	\N	\N	t	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text", "vision", "ocr"], "contextWindow": 32768}	chat	f
cmkgs822y013d8wbvs91ckus9	2026-01-16 11:15:20.842	2026-01-16 11:15:20.842	nvidia:microsoft/phi-3-medium-128k-instruct	\N	16384	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 16384}	chat	f
cmkgs823a013h8wbv97akv825	2026-01-16 11:15:20.854	2026-01-16 11:15:20.854	nvidia:microsoft/phi-3-medium-4k-instruct	\N	16384	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 16384}	chat	f
cmkgs823n013l8wbvk7d95jx3	2026-01-16 11:15:20.867	2026-01-16 11:15:20.867	nvidia:microsoft/phi-3-mini-128k-instruct	\N	16384	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 16384}	chat	f
cmkgs823z013p8wbvkizit91i	2026-01-16 11:15:20.88	2026-01-16 11:15:20.88	nvidia:microsoft/phi-3-mini-4k-instruct	\N	16384	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 16384}	chat	f
cmkgs824d013t8wbvvsk0l716	2026-01-16 11:15:20.893	2026-01-16 11:15:20.893	nvidia:microsoft/phi-3-small-128k-instruct	\N	16384	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 16384}	chat	f
cmkgs824p013x8wbv5xnlsljx	2026-01-16 11:15:20.906	2026-01-16 11:15:20.906	nvidia:microsoft/phi-3-small-8k-instruct	\N	16384	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 16384}	chat	f
cmkgs825101418wbvfbvrt47j	2026-01-16 11:15:20.917	2026-01-16 11:15:20.917	nvidia:microsoft/phi-3-vision-128k-instruct	\N	16384	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 16384}	chat	f
cmkgs825d01458wbvjlehxnz2	2026-01-16 11:15:20.929	2026-01-16 11:15:20.929	nvidia:microsoft/phi-3.5-mini-instruct	\N	16384	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 16384}	chat	f
cmkgs820b012h8wbv7qdckgy2	2026-01-16 11:15:20.747	2026-01-16 11:23:55.513	nvidia:meta/llama-3.2-90b-vision-instruct	\N	4096	\N	t	\N	\N	t	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	low	surveyor_heuristic	{"source": "surveyor_heuristic", "confidence": "low", "capabilities": ["text", "vision"], "contextWindow": 4096}	chat	f
cmkgs81zk01278wbvpzfteel6	2026-01-16 11:15:20.721	2026-01-16 11:23:55.492	nvidia:meta/llama-3.2-11b-vision-instruct	\N	4096	\N	t	\N	\N	t	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	low	surveyor_heuristic	{"source": "surveyor_heuristic", "confidence": "low", "capabilities": ["text", "vision"], "contextWindow": 4096}	chat	f
cmkgs825q01498wbv2l1q3ms4	2026-01-16 11:15:20.942	2026-01-16 11:15:20.942	nvidia:microsoft/phi-3.5-moe-instruct	\N	16384	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 16384}	chat	f
cmkgs8261014d8wbv1xrv7yhk	2026-01-16 11:15:20.954	2026-01-16 11:15:20.954	nvidia:microsoft/phi-3.5-vision-instruct	\N	16384	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 16384}	chat	f
cmkgs826e014h8wbv8dhkxr7l	2026-01-16 11:15:20.966	2026-01-16 11:15:20.966	nvidia:microsoft/phi-4-mini-flash-reasoning	\N	16384	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	t	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text", "reasoning"], "contextWindow": 16384}	chat	f
cmkgs826p014l8wbvsoyt0mob	2026-01-16 11:15:20.978	2026-01-16 11:15:20.978	nvidia:microsoft/phi-4-mini-instruct	\N	16384	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 16384}	chat	f
cmkgs8271014p8wbv8f082w7v	2026-01-16 11:15:20.989	2026-01-16 11:15:20.989	nvidia:microsoft/phi-4-multimodal-instruct	\N	16384	\N	t	\N	\N	t	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text", "vision"], "contextWindow": 16384}	chat	f
cmkgs827j014v8wbv10ct4blh	2026-01-16 11:15:21.007	2026-01-16 11:15:21.007	nvidia:minimaxai/minimax-m2	\N	32768	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 32768}	chat	f
cmkgs827v014z8wbv5591qsbu	2026-01-16 11:15:21.02	2026-01-16 11:15:21.02	nvidia:minimaxai/minimax-m2.1	\N	32768	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 32768}	chat	f
cmkgs828801538wbve79i45pg	2026-01-16 11:15:21.032	2026-01-16 11:15:21.032	nvidia:mistralai/codestral-22b-instruct-v0.1	\N	32000	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text", "code"], "contextWindow": 32000}	chat	f
cmkgs828k01578wbvj8avt63q	2026-01-16 11:15:21.044	2026-01-16 11:15:21.044	nvidia:mistralai/devstral-2-123b-instruct-2512	\N	128000	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text", "code"], "contextWindow": 128000}	chat	f
cmkgs828w015b8wbvvdcsu791	2026-01-16 11:15:21.056	2026-01-16 11:15:21.056	nvidia:mistralai/magistral-small-2506	\N	32000	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 32000}	chat	f
cmkgs8299015f8wbvkjz5uz7o	2026-01-16 11:15:21.069	2026-01-16 11:15:21.069	nvidia:mistralai/mamba-codestral-7b-v0.1	\N	32000	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text", "code"], "contextWindow": 32000}	chat	f
cmkgs829x015n8wbv2jxzp1qf	2026-01-16 11:15:21.093	2026-01-16 11:15:21.093	nvidia:mistralai/ministral-14b-instruct-2512	\N	128000	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 128000}	chat	f
cmkgs82a8015r8wbvg7pbekli	2026-01-16 11:15:21.105	2026-01-16 11:15:21.105	nvidia:mistralai/mistral-7b-instruct-v0.2	\N	32000	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 32000}	chat	f
cmkgs82al015v8wbvyxqpznof	2026-01-16 11:15:21.117	2026-01-16 11:15:21.117	nvidia:mistralai/mistral-7b-instruct-v0.3	\N	32000	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 32000}	chat	f
cmkgs82ax015z8wbvp5knpfvc	2026-01-16 11:15:21.129	2026-01-16 11:15:21.129	nvidia:mistralai/mistral-large	\N	128000	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 128000}	chat	f
cmkgs82b901638wbv4gfezrrg	2026-01-16 11:15:21.142	2026-01-16 11:15:21.142	nvidia:mistralai/mistral-large-2-instruct	\N	128000	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 128000}	chat	f
cmkgs82bl01678wbvz4c1e3g9	2026-01-16 11:15:21.154	2026-01-16 11:15:21.154	nvidia:mistralai/mistral-large-3-675b-instruct-2512	\N	128000	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	t	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text", "reasoning"], "contextWindow": 128000}	chat	f
cmkgs82ca016f8wbvz48llvc5	2026-01-16 11:15:21.178	2026-01-16 11:15:21.178	nvidia:mistralai/mistral-nemotron	\N	128000	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 128000}	chat	f
cmkgs82cm016j8wbv5jhotc8s	2026-01-16 11:15:21.19	2026-01-16 11:15:21.19	nvidia:mistralai/mistral-small-24b-instruct	\N	32000	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 32000}	chat	f
cmkgs82cz016n8wbvu4r0orre	2026-01-16 11:15:21.203	2026-01-16 11:15:21.203	nvidia:mistralai/mistral-small-3.1-24b-instruct-2503	\N	32000	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 32000}	chat	f
cmkgs82db016r8wbvibqm9v6x	2026-01-16 11:15:21.215	2026-01-16 11:15:21.215	nvidia:mistralai/mixtral-8x22b-instruct-v0.1	\N	64000	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 64000}	chat	f
cmkgs82dn016v8wbvx2rwfef7	2026-01-16 11:15:21.228	2026-01-16 11:15:21.228	nvidia:mistralai/mixtral-8x22b-v0.1	\N	64000	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 64000}	chat	f
cmkgs82dz016z8wbv0qg4syst	2026-01-16 11:15:21.24	2026-01-16 11:15:21.24	nvidia:mistralai/mixtral-8x7b-instruct-v0.1	\N	32000	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 32000}	chat	f
cmkgs82bx016b8wbvm32obzyu	2026-01-16 11:15:21.165	2026-01-16 11:23:55.556	nvidia:mistralai/mistral-medium-3-instruct	\N	4096	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	medium	surveyor_heuristic	{"source": "surveyor_heuristic", "confidence": "medium", "capabilities": ["text", "medical"], "contextWindow": 4096}	chat	f
cmkgs82ec01738wbvflwfba79	2026-01-16 11:15:21.252	2026-01-16 11:15:21.252	nvidia:moonshotai/kimi-k2-instruct	\N	128000	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 128000}	chat	f
cmkgs82eo01778wbvgknsxsi1	2026-01-16 11:15:21.264	2026-01-16 11:15:21.264	nvidia:moonshotai/kimi-k2-instruct-0905	\N	128000	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 128000}	chat	f
cmkgs82f1017b8wbv1naohwxn	2026-01-16 11:15:21.277	2026-01-16 11:15:21.277	nvidia:moonshotai/kimi-k2-thinking	\N	128000	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	t	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text", "reasoning"], "contextWindow": 128000}	chat	f
cmkgs82fj017f8wbve9y6v224	2026-01-16 11:15:21.295	2026-01-16 11:15:21.295	nvidia:nv-mistralai/mistral-nemo-12b-instruct	\N	128000	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 128000}	chat	f
cmkgs82fx017j8wbvai8hbu8l	2026-01-16 11:15:21.309	2026-01-16 11:15:21.309	nvidia:nvidia/cosmos-reason2-8b	\N	32768	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	t	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text", "reasoning"], "contextWindow": 32768}	chat	f
cmkgs82gd017n8wbvl6b0528d	2026-01-16 11:15:21.325	2026-01-16 11:15:21.325	nvidia:nvidia/llama-3.1-nemoguard-8b-content-safety	\N	8192	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "primaryTask": "moderation", "capabilities": ["moderation"], "contextWindow": 8192}	moderation	f
cmkgs82gp017r8wbvrgl00h8o	2026-01-16 11:15:21.337	2026-01-16 11:15:21.337	nvidia:nvidia/llama-3.1-nemoguard-8b-topic-control	\N	8192	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "primaryTask": "moderation", "capabilities": ["moderation"], "contextWindow": 8192}	moderation	f
cmkgs82h0017v8wbvftt3vjuq	2026-01-16 11:15:21.348	2026-01-16 11:15:21.348	nvidia:nvidia/llama-3.1-nemotron-51b-instruct	\N	128000	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	t	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text", "reasoning"], "contextWindow": 128000}	chat	f
cmkgs82hf017z8wbvdst5woyn	2026-01-16 11:15:21.363	2026-01-16 11:15:21.363	nvidia:nvidia/llama-3.1-nemotron-70b-instruct	\N	128000	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	t	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text", "reasoning"], "contextWindow": 128000}	chat	f
cmkgs82hv01838wbvdcg058hz	2026-01-16 11:15:21.379	2026-01-16 11:15:21.379	nvidia:nvidia/llama-3.1-nemotron-70b-reward	\N	4096	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["reward_model"], "contextWindow": 4096}	chat	f
cmkgs82i501878wbvzqc6x38c	2026-01-16 11:15:21.389	2026-01-16 11:15:21.389	nvidia:nvidia/llama-3.1-nemotron-nano-4b-v1.1	\N	32768	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 32768}	chat	f
cmkgs82ii018b8wbvvpvv5io9	2026-01-16 11:15:21.402	2026-01-16 11:15:21.402	nvidia:nvidia/llama-3.1-nemotron-nano-8b-v1	\N	32768	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 32768}	chat	f
cmkgs82j2018f8wbvv6nzq65r	2026-01-16 11:15:21.422	2026-01-16 11:15:21.422	nvidia:nvidia/llama-3.1-nemotron-nano-vl-8b-v1	\N	32768	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 32768}	chat	f
cmkgs82ji018j8wbvc837ahpz	2026-01-16 11:15:21.438	2026-01-16 11:15:21.438	nvidia:nvidia/llama-3.1-nemotron-safety-guard-8b-v3	\N	8192	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "primaryTask": "moderation", "capabilities": ["moderation"], "contextWindow": 8192}	moderation	f
cmkgs82jr018n8wbvx6xwhg7p	2026-01-16 11:15:21.447	2026-01-16 11:15:21.447	nvidia:nvidia/llama-3.1-nemotron-ultra-253b-v1	\N	128000	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	t	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text", "reasoning"], "contextWindow": 128000}	chat	f
cmkgs82k4018r8wbvz6vknrw8	2026-01-16 11:15:21.46	2026-01-16 11:15:21.46	nvidia:nvidia/llama-3.2-nemoretriever-1b-vlm-embed-v1	\N	32768	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text", "rag_optimized"], "contextWindow": 32768}	chat	f
cmkgs82kh018v8wbvfxt4duz1	2026-01-16 11:15:21.473	2026-01-16 11:15:21.473	nvidia:nvidia/llama-3.2-nemoretriever-300m-embed-v1	\N	32768	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text", "rag_optimized"], "contextWindow": 32768}	chat	f
cmkgs82kt018z8wbv5s1slv6t	2026-01-16 11:15:21.485	2026-01-16 11:15:21.485	nvidia:nvidia/llama-3.2-nemoretriever-300m-embed-v2	\N	32768	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text", "rag_optimized"], "contextWindow": 32768}	chat	f
cmkgs82l601938wbvhl9im8ux	2026-01-16 11:15:21.498	2026-01-16 11:15:21.498	nvidia:nvidia/llama-3.2-nv-embedqa-1b-v1	\N	2048	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	t	\N	\N	\N	medium	surveyor_heuristic	{"source": "surveyor_heuristic", "confidence": "medium", "primaryTask": "embedding", "capabilities": ["embedding"], "contextWindow": 2048}	embedding	f
cmkgs82lg01978wbvcrnmypth	2026-01-16 11:15:21.508	2026-01-16 11:15:21.508	nvidia:nvidia/llama-3.2-nv-embedqa-1b-v2	\N	2048	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	t	\N	\N	\N	medium	surveyor_heuristic	{"source": "surveyor_heuristic", "confidence": "medium", "primaryTask": "embedding", "capabilities": ["embedding"], "contextWindow": 2048}	embedding	f
cmkgs82lp019b8wbvknt87pqt	2026-01-16 11:15:21.518	2026-01-16 11:15:21.518	nvidia:nvidia/llama-3.3-nemotron-super-49b-v1	\N	128000	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	t	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text", "reasoning"], "contextWindow": 128000}	chat	f
cmkgs82m2019f8wbvved36rja	2026-01-16 11:15:21.53	2026-01-16 11:15:21.53	nvidia:nvidia/llama-3.3-nemotron-super-49b-v1.5	\N	128000	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	t	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text", "reasoning"], "contextWindow": 128000}	chat	f
cmkgs82mf019j8wbv18dxpcj3	2026-01-16 11:15:21.543	2026-01-16 11:15:21.543	nvidia:nvidia/llama3-chatqa-1.5-70b	\N	128000	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text", "rag_optimized"], "contextWindow": 128000}	chat	f
cmkgs82mr019n8wbvx1cjsekc	2026-01-16 11:15:21.555	2026-01-16 11:15:21.555	nvidia:nvidia/llama3-chatqa-1.5-8b	\N	128000	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text", "rag_optimized"], "contextWindow": 128000}	chat	f
cmkgs82n4019r8wbvtox25pv7	2026-01-16 11:15:21.568	2026-01-16 11:15:21.568	nvidia:nvidia/mistral-nemo-minitron-8b-8k-instruct	\N	128000	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 128000}	chat	f
cmkgs82ni019v8wbvjo3wnozr	2026-01-16 11:15:21.582	2026-01-16 11:15:21.582	nvidia:nvidia/mistral-nemo-minitron-8b-base	\N	128000	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 128000}	chat	f
cmkgs82nu019z8wbvp7y732nh	2026-01-16 11:15:21.594	2026-01-16 11:15:21.594	nvidia:nvidia/nemoretriever-parse	\N	32768	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text", "rag_optimized"], "contextWindow": 32768}	chat	f
cmkgs82o701a38wbvmr8lf0cw	2026-01-16 11:15:21.607	2026-01-16 11:15:21.607	nvidia:nvidia/nemotron-3-nano-30b-a3b	\N	32768	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 32768}	chat	f
cmkgs82oj01a78wbvcbok7dib	2026-01-16 11:15:21.62	2026-01-16 11:15:21.62	nvidia:nvidia/nemotron-4-340b-instruct	\N	128000	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	t	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text", "reasoning"], "contextWindow": 128000}	chat	f
cmkgs82ow01ab8wbvbyjtt5t5	2026-01-16 11:15:21.633	2026-01-16 11:15:21.633	nvidia:nvidia/nemotron-4-340b-reward	\N	4096	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["reward_model"], "contextWindow": 4096}	chat	f
cmkgs82p501af8wbvtkq7g8bi	2026-01-16 11:15:21.641	2026-01-16 11:15:21.641	nvidia:nvidia/nemotron-4-mini-hindi-4b-instruct	\N	32768	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 32768}	chat	f
cmkgs82pi01aj8wbvdw1ckijj	2026-01-16 11:15:21.654	2026-01-16 11:15:21.654	nvidia:nvidia/nemotron-mini-4b-instruct	\N	32768	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 32768}	chat	f
cmkgs82pv01an8wbvfo7u2fbm	2026-01-16 11:15:21.667	2026-01-16 11:15:21.667	nvidia:nvidia/nemotron-nano-12b-v2-vl	\N	32768	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 32768}	chat	f
cmkgs82q701ar8wbvgbg03y1d	2026-01-16 11:15:21.679	2026-01-16 11:15:21.679	nvidia:nvidia/nemotron-nano-3-30b-a3b	\N	32768	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 32768}	chat	f
cmkgs82qk01av8wbvfs6tl0ns	2026-01-16 11:15:21.692	2026-01-16 11:15:21.692	nvidia:nvidia/nemotron-parse	\N	32768	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text", "rag_optimized"], "contextWindow": 32768}	chat	f
cmkgs82qx01az8wbviu0x58lj	2026-01-16 11:15:21.705	2026-01-16 11:15:21.705	nvidia:nvidia/neva-22b	\N	32768	\N	t	\N	\N	t	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text", "vision"], "contextWindow": 32768}	chat	f
cmkgs82re01b58wbv8m3mbsmt	2026-01-16 11:15:21.722	2026-01-16 11:15:21.722	nvidia:nvidia/nv-embed-v1	\N	2048	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	t	\N	\N	\N	medium	surveyor_heuristic	{"source": "surveyor_heuristic", "confidence": "medium", "primaryTask": "embedding", "capabilities": ["embedding"], "contextWindow": 2048}	embedding	f
cmkgs82rn01b98wbvph66rhaa	2026-01-16 11:15:21.732	2026-01-16 11:15:21.732	nvidia:nvidia/nv-embedcode-7b-v1	\N	2048	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	t	\N	\N	\N	medium	surveyor_heuristic	{"source": "surveyor_heuristic", "confidence": "medium", "primaryTask": "embedding", "capabilities": ["embedding"], "contextWindow": 2048}	embedding	f
cmkgs82rw01bd8wbvvrvapseo	2026-01-16 11:15:21.74	2026-01-16 11:15:21.74	nvidia:nvidia/nv-embedqa-e5-v5	\N	2048	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	t	\N	\N	\N	medium	surveyor_heuristic	{"source": "surveyor_heuristic", "confidence": "medium", "primaryTask": "embedding", "capabilities": ["embedding"], "contextWindow": 2048}	embedding	f
cmkgs82s501bh8wbv1edvsx8i	2026-01-16 11:15:21.749	2026-01-16 11:15:21.749	nvidia:nvidia/nv-embedqa-mistral-7b-v2	\N	32000	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 32000}	chat	f
cmkgs82si01bl8wbvxbm2edh3	2026-01-16 11:15:21.762	2026-01-16 11:15:21.762	nvidia:nvidia/nvclip	\N	4096	\N	t	\N	\N	t	\N	\N	\N	\N	\N	\N	f	f	f	t	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "primaryTask": "embedding", "capabilities": ["vision", "embedding"], "contextWindow": 4096}	embedding	f
cmkgs82t001br8wbvvwnvi2oc	2026-01-16 11:15:21.781	2026-01-16 11:15:21.781	nvidia:nvidia/nvidia-nemotron-nano-9b-v2	\N	32768	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 32768}	chat	f
cmkgs82td01bv8wbv44f2op69	2026-01-16 11:15:21.793	2026-01-16 11:15:21.793	nvidia:nvidia/riva-translate-4b-instruct	\N	8192	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text", "translation"], "contextWindow": 8192}	chat	f
cmkgs82tp01bz8wbvoxrfuo96	2026-01-16 11:15:21.805	2026-01-16 11:15:21.805	nvidia:nvidia/riva-translate-4b-instruct-v1.1	\N	8192	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text", "translation"], "contextWindow": 8192}	chat	f
cmkgs82u101c38wbv6i3z70iv	2026-01-16 11:15:21.818	2026-01-16 11:15:21.818	nvidia:nvidia/streampetr	\N	4096	\N	t	\N	\N	t	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["vision"], "contextWindow": 4096}	chat	f
cmkgs82uk01c98wbvquj6miwj	2026-01-16 11:15:21.836	2026-01-16 11:15:21.836	nvidia:nvidia/vila	\N	32768	\N	t	\N	\N	t	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text", "vision"], "contextWindow": 32768}	chat	f
cmkgs82v201cf8wbv98nq6d81	2026-01-16 11:15:21.854	2026-01-16 11:15:21.854	nvidia:openai/gpt-oss-120b	\N	32768	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 32768}	chat	f
cmkgs82ve01cj8wbve7z87b1w	2026-01-16 11:15:21.867	2026-01-16 11:15:21.867	nvidia:openai/gpt-oss-20b	\N	32768	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 32768}	chat	f
cmkgs82vr01cn8wbvibggm7qn	2026-01-16 11:15:21.879	2026-01-16 11:15:21.879	nvidia:opengpt-x/teuken-7b-instruct-commercial-v0.4	\N	8192	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 8192}	chat	f
cmkgs82w401cr8wbvvcvlw8ni	2026-01-16 11:15:21.892	2026-01-16 11:15:21.892	nvidia:qwen/qwen2-7b-instruct	\N	32768	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 32768}	chat	f
cmkgs82wg01cv8wbvi97yaxsq	2026-01-16 11:15:21.905	2026-01-16 11:15:21.905	nvidia:qwen/qwen2.5-7b-instruct	\N	32768	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 32768}	chat	f
cmkgs82ws01cz8wbvf04cmiwi	2026-01-16 11:15:21.917	2026-01-16 11:15:21.917	nvidia:qwen/qwen2.5-coder-32b-instruct	\N	32768	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 32768}	chat	f
cmkgs82x501d38wbvx04m8wky	2026-01-16 11:15:21.929	2026-01-16 11:15:21.929	nvidia:qwen/qwen2.5-coder-7b-instruct	\N	32768	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 32768}	chat	f
cmkgs82xi01d78wbv6wfil9cs	2026-01-16 11:15:21.942	2026-01-16 11:15:21.942	nvidia:qwen/qwen3-235b-a22b	\N	32768	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 32768}	chat	f
cmkgs82xu01db8wbvsqueaje0	2026-01-16 11:15:21.955	2026-01-16 11:15:21.955	nvidia:qwen/qwen3-coder-480b-a35b-instruct	\N	32768	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 32768}	chat	f
cmkgs82y601df8wbvm1dzxkxx	2026-01-16 11:15:21.967	2026-01-16 11:15:21.967	nvidia:qwen/qwen3-next-80b-a3b-instruct	\N	32768	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 32768}	chat	f
cmkgs82yi01dj8wbv08yt4giw	2026-01-16 11:15:21.979	2026-01-16 11:15:21.979	nvidia:qwen/qwen3-next-80b-a3b-thinking	\N	32768	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 32768}	chat	f
cmkgs82yv01dn8wbv9o70lbb9	2026-01-16 11:15:21.991	2026-01-16 11:15:21.991	nvidia:qwen/qwq-32b	\N	32768	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 32768}	chat	f
cmkgs82z701dr8wbvjcodlqlh	2026-01-16 11:15:22.004	2026-01-16 11:15:22.004	nvidia:rakuten/rakutenai-7b-chat	\N	32768	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 32768}	chat	f
cmkgs82zj01dv8wbvdtpw3j23	2026-01-16 11:15:22.016	2026-01-16 11:15:22.016	nvidia:rakuten/rakutenai-7b-instruct	\N	32768	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 32768}	chat	f
cmkgs82zw01dz8wbv1tlladr9	2026-01-16 11:15:22.028	2026-01-16 11:15:22.028	nvidia:sarvamai/sarvam-m	\N	32768	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 32768}	chat	f
cmkgs830801e38wbvrfuo8zo1	2026-01-16 11:15:22.04	2026-01-16 11:15:22.04	nvidia:snowflake/arctic-embed-l	\N	2048	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	t	\N	\N	\N	medium	surveyor_heuristic	{"source": "surveyor_heuristic", "confidence": "medium", "primaryTask": "embedding", "capabilities": ["embedding"], "contextWindow": 2048}	embedding	f
cmkgs830h01e78wbvyw5d0t3h	2026-01-16 11:15:22.049	2026-01-16 11:15:22.049	nvidia:speakleash/bielik-11b-v2.3-instruct	\N	32768	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 32768}	chat	f
cmkgs830t01eb8wbv9xe5r5rr	2026-01-16 11:15:22.061	2026-01-16 11:15:22.061	nvidia:speakleash/bielik-11b-v2.6-instruct	\N	32768	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 32768}	chat	f
cmkgs831501ef8wbvr6rrvjx2	2026-01-16 11:15:22.073	2026-01-16 11:15:22.073	nvidia:stockmark/stockmark-2-100b-instruct	\N	32768	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 32768}	chat	f
cmkgs831h01ej8wbvtbkhgfd3	2026-01-16 11:15:22.086	2026-01-16 11:15:22.086	nvidia:tiiuae/falcon3-7b-instruct	\N	32768	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 32768}	chat	f
cmkgs832001ep8wbvpnqaycor	2026-01-16 11:15:22.105	2026-01-16 11:15:22.105	nvidia:upstage/solar-10.7b-instruct	\N	32768	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 32768}	chat	f
cmkgs832c01et8wbvsxswo8hh	2026-01-16 11:15:22.116	2026-01-16 11:15:22.116	nvidia:utter-project/eurollm-9b-instruct	\N	8192	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 8192}	chat	f
cmkgs832o01ex8wbvkcsmqwkl	2026-01-16 11:15:22.129	2026-01-16 11:15:22.129	nvidia:writer/palmyra-creative-122b	\N	32768	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 32768}	chat	f
cmkgs833101f18wbvm5lg79mp	2026-01-16 11:15:22.141	2026-01-16 11:15:22.141	nvidia:writer/palmyra-fin-70b-32k	\N	32768	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 32768}	chat	f
cmkgs833c01f58wbvzqibocge	2026-01-16 11:15:22.153	2026-01-16 11:15:22.153	nvidia:writer/palmyra-med-70b	\N	32768	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 32768}	chat	f
cmkgs833p01f98wbv2nzt3ind	2026-01-16 11:15:22.165	2026-01-16 11:15:22.165	nvidia:writer/palmyra-med-70b-32k	\N	32768	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 32768}	chat	f
cmkgs834701ff8wbve22ivxwp	2026-01-16 11:15:22.183	2026-01-16 11:15:22.183	nvidia:z-ai/glm4.7	\N	32768	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 32768}	chat	f
cmkgs834i01fj8wbv8k1xmo08	2026-01-16 11:15:22.195	2026-01-16 11:15:22.195	nvidia:zyphra/zamba2-7b-instruct	\N	32768	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "capabilities": ["text"], "contextWindow": 32768}	chat	f
cmkgs834v01fn8wbvf915mdav	2026-01-16 11:15:22.207	2026-01-16 11:15:22.207	cerebras:qwen-3-32b	\N	32768	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0.2, "confidence": "high", "capabilities": ["text"], "contextWindow": 32768}	chat	f
cmkgs835601fr8wbv7yx7fvlo	2026-01-16 11:15:22.218	2026-01-16 11:15:22.218	cerebras:zai-glm-4.7	\N	32768	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0.2, "confidence": "high", "capabilities": ["text"], "contextWindow": 32768}	chat	f
cmkgs835j01fv8wbvbmuvfuac	2026-01-16 11:15:22.231	2026-01-16 11:15:22.231	cerebras:qwen-3-235b-a22b-instruct-2507	\N	32768	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0.2, "confidence": "high", "capabilities": ["text"], "contextWindow": 32768}	chat	f
cmkgs835v01fz8wbvjsi791jp	2026-01-16 11:15:22.243	2026-01-16 11:15:22.243	cerebras:llama-3.3-70b	\N	128000	8192	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0.6, "maxOutput": 8192, "confidence": "high", "capabilities": ["text", "tool_use"], "contextWindow": 128000}	chat	f
cmkgs836701g38wbvbrz9z9sg	2026-01-16 11:15:22.255	2026-01-16 11:15:22.255	cerebras:gpt-oss-120b	\N	32768	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0.4, "confidence": "high", "capabilities": ["text"], "contextWindow": 32768}	chat	f
cmkgs836j01g78wbv8gsk33g9	2026-01-16 11:15:22.267	2026-01-16 11:15:22.267	cerebras:zai-glm-4.6	\N	32768	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0.2, "confidence": "high", "capabilities": ["text"], "contextWindow": 32768}	chat	f
cmkgs836w01gb8wbv8kd0jcng	2026-01-16 11:15:22.28	2026-01-16 11:15:22.28	cerebras:llama3.1-8b	\N	128000	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0.1, "confidence": "high", "capabilities": ["text", "tool_use"], "contextWindow": 128000}	chat	f
cmkgs837801gf8wbv4k3kj3im	2026-01-16 11:15:22.292	2026-01-16 11:15:22.292	ollama-local:mxbai-embed-large:latest	\N	512	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	t	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "primaryTask": "embedding", "capabilities": ["embedding"], "contextWindow": 512}	embedding	f
cmkgs837i01gj8wbvjydqs1ii	2026-01-16 11:15:22.302	2026-01-16 11:15:22.302	ollama-local:granite4:micro	\N	8192	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "primaryTask": "chat", "capabilities": ["text"], "contextWindow": 8192}	chat	f
cmkgs837v01gn8wbvtbyfvz5s	2026-01-16 11:15:22.315	2026-01-16 11:15:22.315	ollama-local:hengwen/watt-tool-8B:latest	\N	8192	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	high	surveyor_pattern	{"source": "surveyor_pattern", "costPer1k": 0, "confidence": "high", "primaryTask": "chat", "capabilities": ["text"], "contextWindow": 8192}	chat	f
cmkgs81yj011v8wbvn5dhes7n	2026-01-16 11:15:20.683	2026-01-16 11:23:55.467	nvidia:mediatek/breeze-7b-instruct	\N	4096	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	medium	surveyor_heuristic	{"source": "surveyor_heuristic", "confidence": "medium", "capabilities": ["text", "medical"], "contextWindow": 4096}	chat	f
cmkgs829k015j8wbvt8wztk2t	2026-01-16 11:15:21.08	2026-01-16 11:23:55.544	nvidia:mistralai/mathstral-7b-v0.1	\N	4096	\N	f	\N	\N	f	\N	\N	\N	\N	\N	\N	f	f	f	f	\N	\N	\N	medium	surveyor_heuristic	{"source": "surveyor_heuristic", "confidence": "medium", "capabilities": ["text", "specialized_science"], "contextWindow": 4096}	chat	f
\.


--
-- Data for Name: nvidia_models; Type: TABLE DATA; Schema: public; Owner: -
--

COPY public.nvidia_models (_id, _loaded_at, id, object, created, owned_by) FROM stdin;
abd6b81a-590f-4cf4-9ac9-5f7ef6b1b821	2026-01-16 11:23:53.106034+00	01-ai/yi-large	model	735790403	01-ai
c138a8f4-41da-457f-ba4e-65801a36f996	2026-01-16 11:23:53.106034+00	abacusai/dracarys-llama-3.1-70b-instruct	model	735790403	abacusai
4e0232ce-b59c-436a-964e-c5a34a9fb8b0	2026-01-16 11:23:53.106034+00	adept/fuyu-8b	model	735790403	adept
ec5be8fa-b491-4781-b1d4-c4897633ba35	2026-01-16 11:23:53.106034+00	ai21labs/jamba-1.5-large-instruct	model	735790403	ai21labs
3385ed73-9c53-40d6-b71f-d76795bd2045	2026-01-16 11:23:53.106034+00	ai21labs/jamba-1.5-mini-instruct	model	735790403	ai21labs
f96c8151-344d-4b98-83df-7cf537171a2e	2026-01-16 11:23:53.106034+00	aisingapore/sea-lion-7b-instruct	model	735790403	aisingapore
8d9de6c3-312d-46ef-b17d-42d75877b476	2026-01-16 11:23:53.106034+00	baai/bge-m3	model	735790403	baai
f49621a3-057c-499f-9c0e-10c75f7ee455	2026-01-16 11:23:53.106034+00	baichuan-inc/baichuan2-13b-chat	model	735790403	baichuan-inc
91145630-82d1-4e5a-99ab-6bdc77d2dd1a	2026-01-16 11:23:53.106034+00	bigcode/starcoder2-15b	model	735790403	bigcode
743a0886-cb13-4078-bbae-49bda27bf56b	2026-01-16 11:23:53.106034+00	bigcode/starcoder2-7b	model	735790403	bigcode
2317d568-5e95-4c1c-969c-0169244f4e3b	2026-01-16 11:23:53.106034+00	bytedance/seed-oss-36b-instruct	model	735790403	bytedance
40332fbd-bfc1-42d9-8c88-ab868f359164	2026-01-16 11:23:53.106034+00	databricks/dbrx-instruct	model	735790403	databricks
6b8b0d4a-b896-4eb4-9d66-3dece5da18a6	2026-01-16 11:23:53.106034+00	deepseek-ai/deepseek-coder-6.7b-instruct	model	735790403	deepseek-ai
eeda527e-8d4e-4a8f-ac04-69eaa9f47174	2026-01-16 11:23:53.106034+00	deepseek-ai/deepseek-r1	model	735790403	deepseek-ai
1479656f-ea88-4380-9bfc-181b16ae85f0	2026-01-16 11:23:53.106034+00	deepseek-ai/deepseek-r1-0528	model	735790403	deepseek-ai
57746fba-1e34-4cfc-9bc4-a0a4f38865ba	2026-01-16 11:23:53.106034+00	deepseek-ai/deepseek-r1-distill-llama-8b	model	735790403	deepseek-ai
92d6c7c9-a632-426d-a2aa-8303bc7ab182	2026-01-16 11:23:53.106034+00	deepseek-ai/deepseek-r1-distill-qwen-14b	model	735790403	deepseek-ai
ee2e5ce5-0d71-4e3e-bc85-4e38ecadc0ab	2026-01-16 11:23:53.106034+00	deepseek-ai/deepseek-r1-distill-qwen-32b	model	735790403	deepseek-ai
083e87bf-9ce8-40cb-9cda-c16e1f61d00a	2026-01-16 11:23:53.106034+00	deepseek-ai/deepseek-r1-distill-qwen-7b	model	735790403	deepseek-ai
d09a733d-2f41-4a7a-9bc0-607e6f1c3eb3	2026-01-16 11:23:53.106034+00	deepseek-ai/deepseek-v3.1	model	735790403	deepseek-ai
9e0ba32d-f6b3-420b-9b28-efbb5ed9918b	2026-01-16 11:23:53.106034+00	deepseek-ai/deepseek-v3.1-terminus	model	735790403	deepseek-ai
890521c9-b2ad-47f0-8575-a07688caa45d	2026-01-16 11:23:53.106034+00	deepseek-ai/deepseek-v3.2	model	735790403	deepseek-ai
7224e662-21ed-4197-9b2b-923f211767bf	2026-01-16 11:23:53.106034+00	google/codegemma-1.1-7b	model	735790403	google
2d3aed33-521c-4814-8a80-cb3b9e268b5d	2026-01-16 11:23:53.106034+00	google/codegemma-7b	model	735790403	google
95fcfea8-8eb0-4f8d-83c9-e707e6b5f294	2026-01-16 11:23:53.106034+00	google/deplot	model	735790403	google
d8752e57-9b2b-4971-9e2f-5725c275582a	2026-01-16 11:23:53.106034+00	google/gemma-2-27b-it	model	735790403	google
96391254-818e-4b89-8bb3-9cc3c72407ce	2026-01-16 11:23:53.106034+00	google/gemma-2-2b-it	model	735790403	google
392a15c8-b76c-413e-9692-682c0ee5deb6	2026-01-16 11:23:53.106034+00	google/gemma-2-9b-it	model	735790403	google
8969b59a-3ec8-44d6-b551-f4707297e01c	2026-01-16 11:23:53.106034+00	google/gemma-2b	model	735790403	google
8d8697ea-1e68-4c52-8157-0f1072a4bfed	2026-01-16 11:23:53.106034+00	google/gemma-3-12b-it	model	735790403	google
4e5999db-85db-4bfe-a7b2-8069b94e8bb7	2026-01-16 11:23:53.106034+00	google/gemma-3-1b-it	model	735790403	google
1290261b-4781-4bda-a3fc-6c0c1882f3ae	2026-01-16 11:23:53.106034+00	google/gemma-3-27b-it	model	735790403	google
f0316691-87c8-4b7d-affb-938348f00c38	2026-01-16 11:23:53.106034+00	google/gemma-3-4b-it	model	735790403	google
ca6605a6-5e2a-4015-b13c-66aef40c189a	2026-01-16 11:23:53.106034+00	google/gemma-3n-e2b-it	model	735790403	google
3e65db21-186b-4d1f-a9c5-cc7603c69c91	2026-01-16 11:23:53.106034+00	google/gemma-3n-e4b-it	model	735790403	google
fc9bf6d5-4bba-4f14-bcc7-de9660073fc9	2026-01-16 11:23:53.106034+00	google/gemma-7b	model	735790403	google
c63da172-9f9f-4bdc-bc70-79d172cfe875	2026-01-16 11:23:53.106034+00	google/paligemma	model	735790403	google
705a01ab-b9a2-4825-a6cb-d1d0861dd868	2026-01-16 11:23:53.106034+00	google/recurrentgemma-2b	model	735790403	google
429518fd-9ba2-4e30-bea7-33b616a4e7f3	2026-01-16 11:23:53.106034+00	google/shieldgemma-9b	model	735790403	google
73a7884b-368c-4011-b9ce-eae1f48bdc5d	2026-01-16 11:23:53.106034+00	gotocompany/gemma-2-9b-cpt-sahabatai-instruct	model	735790403	gotocompany
6c6f508a-92c0-4b79-ac9e-e19a3bda8172	2026-01-16 11:23:53.106034+00	ibm/granite-3.0-3b-a800m-instruct	model	735790403	ibm
55ac5fdd-e7da-43af-bff9-146d5641189c	2026-01-16 11:23:53.106034+00	ibm/granite-3.0-8b-instruct	model	735790403	ibm
b45d96da-afe5-4706-b8e0-9930d4113d15	2026-01-16 11:23:53.106034+00	ibm/granite-3.3-8b-instruct	model	735790403	ibm
40d761c7-8fa1-4557-9e4d-4d81856b562f	2026-01-16 11:23:53.106034+00	ibm/granite-34b-code-instruct	model	735790403	ibm
4099f3f5-39e7-480e-af63-a97e7d3c5a86	2026-01-16 11:23:53.106034+00	ibm/granite-8b-code-instruct	model	735790403	ibm
0da9dba8-9c0d-4ea7-8246-351af5c15c6c	2026-01-16 11:23:53.106034+00	ibm/granite-guardian-3.0-8b	model	735790403	ibm
5e680772-e144-4936-9ead-153cd475b317	2026-01-16 11:23:53.106034+00	igenius/colosseum_355b_instruct_16k	model	735790403	igenius
05a973f4-2c2f-4cdc-8206-9bc719160c6c	2026-01-16 11:23:53.106034+00	igenius/italia_10b_instruct_16k	model	735790403	igenius
76fe1d7d-98bd-42bc-83e7-cae842a4acd1	2026-01-16 11:23:53.106034+00	institute-of-science-tokyo/llama-3.1-swallow-70b-instruct-v0.1	model	735790403	institute-of-science-tokyo
d2b11568-e271-4ea7-8793-b809127fc564	2026-01-16 11:23:53.106034+00	institute-of-science-tokyo/llama-3.1-swallow-8b-instruct-v0.1	model	735790403	institute-of-science-tokyo
56f53e5c-2305-4d4a-9458-226e7fa50ba4	2026-01-16 11:23:53.106034+00	marin/marin-8b-instruct	model	735790403	marin
64126249-c9db-418e-a8ff-6897097d6521	2026-01-16 11:23:53.106034+00	mediatek/breeze-7b-instruct	model	735790403	mediatek
cc5bd00b-f92a-417b-ad49-e4d32ea3d32a	2026-01-16 11:23:53.106034+00	meta/codellama-70b	model	735790403	meta
5070aed2-07b0-4504-a1ca-d887116f59de	2026-01-16 11:23:53.106034+00	meta/llama-3.1-405b-instruct	model	735790403	meta
63708b93-289b-4d7e-b170-f86484d650e0	2026-01-16 11:23:53.106034+00	meta/llama-3.1-70b-instruct	model	735790403	meta
3d0f2a1e-c433-433c-98f6-db296f83b1eb	2026-01-16 11:23:53.106034+00	meta/llama-3.1-8b-instruct	model	735790403	meta
6d7c9403-44d6-4f16-9baf-cdd5d1643dd2	2026-01-16 11:23:53.106034+00	meta/llama-3.2-11b-vision-instruct	model	735790403	meta
e77a5839-215e-4ada-ac41-54c46fc0dbec	2026-01-16 11:23:53.106034+00	meta/llama-3.2-1b-instruct	model	735790403	meta
433b013b-8c25-4b91-96e8-b9cccf7ebd5b	2026-01-16 11:23:53.106034+00	meta/llama-3.2-3b-instruct	model	735790403	meta
41cc52d4-f8b7-4407-a8c5-74fe22455344	2026-01-16 11:23:53.106034+00	meta/llama-3.2-90b-vision-instruct	model	735790403	meta
285aa466-d56b-4798-b2e6-dd2115850286	2026-01-16 11:23:53.106034+00	meta/llama-3.3-70b-instruct	model	735790403	meta
06c9460b-111d-42a6-a104-e61d9812e215	2026-01-16 11:23:53.106034+00	meta/llama-4-maverick-17b-128e-instruct	model	735790403	meta
d68c5c93-bf27-482b-9487-b1dc8825c420	2026-01-16 11:23:53.106034+00	meta/llama-4-scout-17b-16e-instruct	model	735790403	meta
f7bd6733-7c87-4c8e-9ed7-a0a81d408e52	2026-01-16 11:23:53.106034+00	meta/llama-guard-4-12b	model	735790403	meta
85f794cb-2b6b-4a12-af5f-9397becfc196	2026-01-16 11:23:53.106034+00	meta/llama2-70b	model	735790403	meta
b477950a-4492-4d67-96ac-ca0aad947588	2026-01-16 11:23:53.106034+00	meta/llama3-70b-instruct	model	735790403	meta
34416332-9011-48b5-9a5c-11234130855d	2026-01-16 11:23:53.106034+00	meta/llama3-8b-instruct	model	735790403	meta
69fdd213-4c02-4e78-8847-53e17822e9b4	2026-01-16 11:23:53.106034+00	microsoft/kosmos-2	model	735790403	microsoft
5919419b-e832-40ef-a90a-2a60566a8595	2026-01-16 11:23:53.106034+00	microsoft/phi-3-medium-128k-instruct	model	735790403	microsoft
20a2830f-5b0c-420a-b340-a80dcf315b23	2026-01-16 11:23:53.106034+00	microsoft/phi-3-medium-4k-instruct	model	735790403	microsoft
d56490b1-9ff1-4b92-8361-0388fc8f4e4f	2026-01-16 11:23:53.106034+00	microsoft/phi-3-mini-128k-instruct	model	735790403	microsoft
8192595a-3d9a-4167-9913-4eca2a60533a	2026-01-16 11:23:53.106034+00	microsoft/phi-3-mini-4k-instruct	model	735790403	microsoft
dcd76d95-2ec5-43ec-b5b2-6de2b94a2a3e	2026-01-16 11:23:53.106034+00	microsoft/phi-3-small-128k-instruct	model	735790403	microsoft
dc19719e-1f73-459d-afad-6b427b0ba0a2	2026-01-16 11:23:53.106034+00	microsoft/phi-3-small-8k-instruct	model	735790403	microsoft
acd0bd37-68be-48b1-966b-f75ebcc3a9c0	2026-01-16 11:23:53.106034+00	microsoft/phi-3-vision-128k-instruct	model	735790403	microsoft
9aaa3e7c-282b-4f01-9e07-3ab8f7d9578b	2026-01-16 11:23:53.106034+00	microsoft/phi-3.5-mini-instruct	model	735790403	microsoft
4aa32c17-fa17-4c3c-a5f9-8eea997ea3c9	2026-01-16 11:23:53.106034+00	microsoft/phi-3.5-moe-instruct	model	735790403	microsoft
72e0928c-35c6-4f4b-b4b3-b0fbbd60ecfb	2026-01-16 11:23:53.106034+00	microsoft/phi-3.5-vision-instruct	model	735790403	microsoft
ddc75e2d-e90c-4133-aa89-034587415128	2026-01-16 11:23:53.106034+00	microsoft/phi-4-mini-flash-reasoning	model	735790403	microsoft
2589dfcb-3289-4034-b6b9-5d5088665ff6	2026-01-16 11:23:53.106034+00	microsoft/phi-4-mini-instruct	model	735790403	microsoft
a3089f80-45d6-4144-b92d-bacfba662920	2026-01-16 11:23:53.106034+00	microsoft/phi-4-multimodal-instruct	model	735790403	microsoft
1f8f95da-1de6-48db-a627-d12045c10dd7	2026-01-16 11:23:53.106034+00	minimaxai/minimax-m2	model	735790403	minimaxai
6309473e-6a40-468a-bcc4-85f8a54c0f80	2026-01-16 11:23:53.106034+00	minimaxai/minimax-m2.1	model	735790403	minimaxai
a7e59151-03a1-496f-bfcf-745a2de7b692	2026-01-16 11:23:53.106034+00	mistralai/codestral-22b-instruct-v0.1	model	735790403	mistralai
3b25811d-0274-446f-af29-7c17013ee147	2026-01-16 11:23:53.106034+00	mistralai/devstral-2-123b-instruct-2512	model	735790403	mistralai
11130f6d-c3ec-4f1c-85a6-cf8d3291b389	2026-01-16 11:23:53.106034+00	mistralai/magistral-small-2506	model	735790403	mistralai
2696cd7d-6d8c-4179-b7ec-d35116bdb37f	2026-01-16 11:23:53.106034+00	mistralai/mamba-codestral-7b-v0.1	model	735790403	mistralai
241393c7-1441-4a91-98a3-a4f7599454dc	2026-01-16 11:23:53.106034+00	mistralai/mathstral-7b-v0.1	model	735790403	mistralai
5fad0619-284d-477b-9e7e-9f0d4bd586fd	2026-01-16 11:23:53.106034+00	mistralai/ministral-14b-instruct-2512	model	735790403	mistralai
7997bd26-98cb-4005-9288-591454a8b687	2026-01-16 11:23:53.106034+00	mistralai/mistral-7b-instruct-v0.2	model	735790403	mistralai
125d8010-e5c1-44b3-bb9e-bcb7f120fe8b	2026-01-16 11:23:53.106034+00	mistralai/mistral-7b-instruct-v0.3	model	735790403	mistralai
ced2bac4-7b03-4c58-9735-f3cc6eaa7184	2026-01-16 11:23:53.106034+00	mistralai/mistral-large	model	735790403	mistralai
6c4ae00f-a04a-4e08-a160-d15087716b33	2026-01-16 11:23:53.106034+00	mistralai/mistral-large-2-instruct	model	735790403	mistralai
5d10b3cd-d8ab-493d-b9dc-73f3006339a8	2026-01-16 11:23:53.106034+00	mistralai/mistral-large-3-675b-instruct-2512	model	735790403	mistralai
4a363a4a-1d82-4479-acba-e51220cad293	2026-01-16 11:23:53.106034+00	mistralai/mistral-medium-3-instruct	model	735790403	mistralai
59d234e3-4540-4683-bd76-64f6feb7cc0a	2026-01-16 11:23:53.106034+00	mistralai/mistral-nemotron	model	735790403	mistralai
a1a6b28d-4ffa-4890-8915-85287d3b41f4	2026-01-16 11:23:53.106034+00	mistralai/mistral-small-24b-instruct	model	735790403	mistralai
e6d524ec-6e83-420e-9c11-7826ca28b80f	2026-01-16 11:23:53.106034+00	mistralai/mistral-small-3.1-24b-instruct-2503	model	735790403	mistralai
b11a1f95-b6f8-4baf-9c98-91dfb0b716c8	2026-01-16 11:23:53.106034+00	mistralai/mixtral-8x22b-instruct-v0.1	model	735790403	mistralai
df663bda-6c86-4e04-92c4-2e880a6294fc	2026-01-16 11:23:53.106034+00	mistralai/mixtral-8x22b-v0.1	model	735790403	mistralai
3b7d6dab-ea40-4842-852e-12b280a9ae90	2026-01-16 11:23:53.106034+00	mistralai/mixtral-8x7b-instruct-v0.1	model	735790403	mistralai
55b6f8ca-4911-47a2-bb37-0b88c8dfed4e	2026-01-16 11:23:53.106034+00	moonshotai/kimi-k2-instruct	model	735790403	moonshotai
be36cc3f-33ad-4704-88d8-8c50f5c82ff3	2026-01-16 11:23:53.106034+00	moonshotai/kimi-k2-instruct-0905	model	735790403	moonshotai
c228bb53-4b9b-46ec-bce5-48b5a7f8cab2	2026-01-16 11:23:53.106034+00	moonshotai/kimi-k2-thinking	model	735790403	moonshotai
1eedcd3f-db98-4e59-ab6d-66fff492d065	2026-01-16 11:23:53.106034+00	nv-mistralai/mistral-nemo-12b-instruct	model	735790403	nv-mistralai
75421c5d-6a9c-4428-8295-8ac28b1e3557	2026-01-16 11:23:53.106034+00	nvidia/cosmos-reason2-8b	model	735790403	nvidia
1b1a4cd7-fb59-4e9e-945d-5deb09fb8373	2026-01-16 11:23:53.106034+00	nvidia/embed-qa-4	model	735790403	nvidia
d9610cac-2080-471e-af54-b373125c21c9	2026-01-16 11:23:53.106034+00	nvidia/llama-3.1-nemoguard-8b-content-safety	model	735790403	nvidia
0fa83036-e831-42d6-a8e8-3fb814f5ef75	2026-01-16 11:23:53.106034+00	nvidia/llama-3.1-nemoguard-8b-topic-control	model	735790403	nvidia
9099026f-1620-49fb-96d1-6d7f8d3aa00a	2026-01-16 11:23:53.106034+00	nvidia/llama-3.1-nemotron-51b-instruct	model	735790403	nvidia
c74fd94d-631b-4a44-bac6-5574b2512fcf	2026-01-16 11:23:53.106034+00	nvidia/llama-3.1-nemotron-70b-instruct	model	735790403	nvidia
278bfb75-8f89-4621-a238-27afe7b477bc	2026-01-16 11:23:53.106034+00	nvidia/llama-3.1-nemotron-70b-reward	model	735790403	nvidia
7f943e00-2bd1-4404-a86b-ca081528674c	2026-01-16 11:23:53.106034+00	nvidia/llama-3.1-nemotron-nano-4b-v1.1	model	735790403	nvidia
7b1acd6d-ac89-4fed-b9c2-4534a6e56015	2026-01-16 11:23:53.106034+00	nvidia/llama-3.1-nemotron-nano-8b-v1	model	735790403	nvidia
1c90d801-9f0c-4a97-a6fd-c00b30007acd	2026-01-16 11:23:53.106034+00	nvidia/llama-3.1-nemotron-nano-vl-8b-v1	model	735790403	nvidia
9bd6a869-755b-4fed-b879-df3d822eff46	2026-01-16 11:23:53.106034+00	nvidia/llama-3.1-nemotron-safety-guard-8b-v3	model	735790403	nvidia
c496479d-a8d3-490c-a84e-cf6cf52e1791	2026-01-16 11:23:53.106034+00	nvidia/llama-3.1-nemotron-ultra-253b-v1	model	735790403	nvidia
c9d870e2-280f-4201-971a-21463ec6e7a0	2026-01-16 11:23:53.106034+00	nvidia/llama-3.2-nemoretriever-1b-vlm-embed-v1	model	735790403	nvidia
510dfee7-ca10-412d-be82-fd5296080464	2026-01-16 11:23:53.106034+00	nvidia/llama-3.2-nemoretriever-300m-embed-v1	model	735790403	nvidia
77cc703c-5929-4fbb-a679-b030d6860464	2026-01-16 11:23:53.106034+00	nvidia/llama-3.2-nemoretriever-300m-embed-v2	model	735790403	nvidia
b6e6b36f-e081-45dd-8020-a8a5e2df5141	2026-01-16 11:23:53.106034+00	nvidia/llama-3.2-nv-embedqa-1b-v1	model	735790403	nvidia
a56f6fa8-c059-4195-88db-1bf7d2893d0c	2026-01-16 11:23:53.106034+00	nvidia/llama-3.2-nv-embedqa-1b-v2	model	735790403	nvidia
f828bd49-2dfa-4a2f-a543-3ef749979131	2026-01-16 11:23:53.106034+00	nvidia/llama-3.3-nemotron-super-49b-v1	model	735790403	nvidia
d1750f0f-33f3-45ba-a321-2ca684190946	2026-01-16 11:23:53.106034+00	nvidia/llama-3.3-nemotron-super-49b-v1.5	model	735790403	nvidia
d54ed5dc-4692-469e-94f2-15af01b1bd08	2026-01-16 11:23:53.106034+00	nvidia/llama3-chatqa-1.5-70b	model	735790403	nvidia
431321e1-9dee-4320-9ca5-8e5ccfe627d7	2026-01-16 11:23:53.106034+00	nvidia/llama3-chatqa-1.5-8b	model	735790403	nvidia
f2198d82-91d3-474c-a335-a97c3baebecd	2026-01-16 11:23:53.106034+00	nvidia/mistral-nemo-minitron-8b-8k-instruct	model	735790403	nvidia
3cc98087-7cb9-499c-9434-beaa271a18a9	2026-01-16 11:23:53.106034+00	nvidia/mistral-nemo-minitron-8b-base	model	735790403	nvidia
5d7b57d0-7c07-4eb2-825e-c9048e50d129	2026-01-16 11:23:53.106034+00	nvidia/nemoretriever-parse	model	735790403	nvidia
5b4a31e8-c70c-41b7-8fae-784b5255997b	2026-01-16 11:23:53.106034+00	nvidia/nemotron-3-nano-30b-a3b	model	735790403	nvidia
da8d39a7-edb9-4829-9eb8-723d3acdbce8	2026-01-16 11:23:53.106034+00	nvidia/nemotron-4-340b-instruct	model	735790403	nvidia
54df52c9-92df-444d-8d8b-e9039670fc6b	2026-01-16 11:23:53.106034+00	nvidia/nemotron-4-340b-reward	model	735790403	nvidia
0d4f83ce-c850-4ef9-8868-3cc8a674c968	2026-01-16 11:23:53.106034+00	nvidia/nemotron-4-mini-hindi-4b-instruct	model	735790403	nvidia
16c4ee04-f41c-4b44-877e-e930965b9239	2026-01-16 11:23:53.106034+00	nvidia/nemotron-mini-4b-instruct	model	735790403	nvidia
32a55185-ec44-478e-96ab-24f55cdaf497	2026-01-16 11:23:53.106034+00	nvidia/nemotron-nano-12b-v2-vl	model	735790403	nvidia
a6e3b42d-8f36-478a-9b16-fefd8d6d7ebc	2026-01-16 11:23:53.106034+00	nvidia/nemotron-nano-3-30b-a3b	model	735790403	nvidia
62f83c79-571e-4a2a-8d73-e62360ba7c3a	2026-01-16 11:23:53.106034+00	nvidia/nemotron-parse	model	735790403	nvidia
795c38aa-eaac-4dd1-9b95-9e8ab5cd7fe4	2026-01-16 11:23:53.106034+00	nvidia/neva-22b	model	735790403	nvidia
3719ce3d-54f9-4ad2-830f-cfd540696030	2026-01-16 11:23:53.106034+00	nvidia/nv-embed-v1	model	735790403	nvidia
777e8ecc-56cc-4a15-b9e1-0e4908055b7f	2026-01-16 11:23:53.106034+00	nvidia/nv-embedcode-7b-v1	model	735790403	nvidia
9a7f588f-0a5f-4440-8bef-226e597cf0a9	2026-01-16 11:23:53.106034+00	nvidia/nv-embedqa-e5-v5	model	735790403	nvidia
89109610-9c69-416c-ba9e-a586ccb0e820	2026-01-16 11:23:53.106034+00	nvidia/nv-embedqa-mistral-7b-v2	model	735790403	nvidia
964ac9a4-0d41-412f-90af-3b11a4efb51d	2026-01-16 11:23:53.106034+00	nvidia/nvclip	model	735790403	nvidia
99b0188b-530c-4c3e-abde-61f7f5361017	2026-01-16 11:23:53.106034+00	nvidia/nvidia-nemotron-nano-9b-v2	model	735790403	nvidia
f44ad00b-9a4a-4736-a07f-f783deba4b4d	2026-01-16 11:23:53.106034+00	nvidia/riva-translate-4b-instruct	model	735790403	nvidia
d750bc60-a8ce-4343-aac3-25c59b6d708e	2026-01-16 11:23:53.106034+00	nvidia/riva-translate-4b-instruct-v1.1	model	735790403	nvidia
6e25aba8-257c-41ca-b148-cd8629e2849f	2026-01-16 11:23:53.106034+00	nvidia/streampetr	model	735790403	nvidia
6ff10822-787b-4caf-b358-4375e920aa07	2026-01-16 11:23:53.106034+00	nvidia/usdcode-llama-3.1-70b-instruct	model	735790403	nvidia
4cd15f5c-ae17-4fc5-810f-eec5afdaf001	2026-01-16 11:23:53.106034+00	nvidia/vila	model	735790403	nvidia
09146c38-5553-4d93-bb01-fc8c946b7cdb	2026-01-16 11:23:53.106034+00	openai/gpt-oss-120b	model	735790403	openai
cf76a7e6-1d8e-4160-ae48-df49ffd2b770	2026-01-16 11:23:53.106034+00	openai/gpt-oss-120b	model	735790403	openai
147b8ebd-d8d1-4e6d-99db-b104561c2a35	2026-01-16 11:23:53.106034+00	openai/gpt-oss-20b	model	735790403	openai
c26061fb-685f-4524-b860-afd243c16f64	2026-01-16 11:23:53.106034+00	openai/gpt-oss-20b	model	735790403	openai
9b7064b9-0a27-4377-b75c-b85c9dcafdb9	2026-01-16 11:23:53.106034+00	opengpt-x/teuken-7b-instruct-commercial-v0.4	model	735790403	opengpt-x
2cf87269-d710-4b21-b466-06fa9019e2d0	2026-01-16 11:23:53.106034+00	qwen/qwen2-7b-instruct	model	735790403	qwen
0ca874b1-54d7-4a72-9730-96183d40e829	2026-01-16 11:23:53.106034+00	qwen/qwen2.5-7b-instruct	model	735790403	qwen
802c8ea4-3f93-46f2-a588-518cbf55fcee	2026-01-16 11:23:53.106034+00	qwen/qwen2.5-coder-32b-instruct	model	735790403	qwen
ae2d1a66-db8f-4704-be00-baeff63a6003	2026-01-16 11:23:53.106034+00	qwen/qwen2.5-coder-7b-instruct	model	735790403	qwen
1430a194-2cf0-4052-8a7d-d207550cad8a	2026-01-16 11:23:53.106034+00	qwen/qwen3-235b-a22b	model	735790403	qwen
30944f3a-63f5-4922-b3fe-d4b16e47d446	2026-01-16 11:23:53.106034+00	qwen/qwen3-coder-480b-a35b-instruct	model	735790403	qwen
25e10460-e8d7-4029-a33d-eeadce2999ff	2026-01-16 11:23:53.106034+00	qwen/qwen3-next-80b-a3b-instruct	model	735790403	qwen
94c76eff-3a13-4814-b5b1-8b08bcb28341	2026-01-16 11:23:53.106034+00	qwen/qwen3-next-80b-a3b-thinking	model	735790403	qwen
615bf879-94c7-4ba6-b2e4-f29d3fb83142	2026-01-16 11:23:53.106034+00	qwen/qwq-32b	model	735790403	qwen
d6efeb81-ca15-42ed-acc0-d6fcbfe24fd1	2026-01-16 11:23:53.106034+00	rakuten/rakutenai-7b-chat	model	735790403	rakuten
f6a504e9-adfa-41fd-8abc-867473dac7e4	2026-01-16 11:23:53.106034+00	rakuten/rakutenai-7b-instruct	model	735790403	rakuten
56046f50-2b2a-4abe-a7b9-e2bf8f70890e	2026-01-16 11:23:53.106034+00	sarvamai/sarvam-m	model	735790403	sarvamai
31575a2b-01c6-4ccc-8a83-34314148ecae	2026-01-16 11:23:53.106034+00	snowflake/arctic-embed-l	model	735790403	snowflake
95de32ad-64a6-48ac-8a6f-ecd2c3a50f76	2026-01-16 11:23:53.106034+00	speakleash/bielik-11b-v2.3-instruct	model	735790403	speakleash
296858bf-a5ee-42bb-99d9-c3265485df23	2026-01-16 11:23:53.106034+00	speakleash/bielik-11b-v2.6-instruct	model	735790403	speakleash
b88855ab-7ca2-4307-baf6-be5acfdb3842	2026-01-16 11:23:53.106034+00	stockmark/stockmark-2-100b-instruct	model	735790403	stockmark
6cb32034-b639-49fd-b058-7b585f36f14b	2026-01-16 11:23:53.106034+00	thudm/chatglm3-6b	model	735790403	thudm
8b294dc6-1f00-40d7-bbbd-9009bb424805	2026-01-16 11:23:53.106034+00	tiiuae/falcon3-7b-instruct	model	735790403	tiiuae
2e40a11d-dccd-4e13-aa57-942557dae0bd	2026-01-16 11:23:53.106034+00	tokyotech-llm/llama-3-swallow-70b-instruct-v0.1	model	735790403	tokyotech-llm
9f1d0584-7de8-4656-9946-272280299a8a	2026-01-16 11:23:53.106034+00	upstage/solar-10.7b-instruct	model	735790403	upstage
ea82abf8-3102-41fa-891f-15d59859f22d	2026-01-16 11:23:53.106034+00	utter-project/eurollm-9b-instruct	model	735790403	utter-project
8f77440c-ce03-4c67-b933-de8813282016	2026-01-16 11:23:53.106034+00	writer/palmyra-creative-122b	model	735790403	writer
9d3aba22-5c22-4e3c-b8cc-feb92b94c151	2026-01-16 11:23:53.106034+00	writer/palmyra-fin-70b-32k	model	735790403	writer
8a67dc0b-68d4-457c-9f32-4636693197b7	2026-01-16 11:23:53.106034+00	writer/palmyra-med-70b	model	735790403	writer
8bca7601-d876-49a2-85f0-46cd22291b42	2026-01-16 11:23:53.106034+00	writer/palmyra-med-70b-32k	model	735790403	writer
37f96676-eab1-4efb-a80c-69ed2943fe1b	2026-01-16 11:23:53.106034+00	yentinglin/llama-3-taiwan-70b-instruct	model	735790403	yentinglin
6e6fe3ab-1370-49f6-82ee-8f9ba425a68d	2026-01-16 11:23:53.106034+00	z-ai/glm4.7	model	735790403	z-ai
ee513d77-c7ad-48a0-9c17-44b808cddb78	2026-01-16 11:23:53.106034+00	zyphra/zamba2-7b-instruct	model	735790403	zyphra
\.


--
-- Data for Name: ollama_models; Type: TABLE DATA; Schema: public; Owner: -
--

COPY public.ollama_models (_id, _loaded_at, name, model, modified_at, size, digest, details) FROM stdin;
1a5e4c66-84f2-4b76-872f-0116023a81c6	2026-01-16 11:23:55.379614+00	mxbai-embed-large:latest	mxbai-embed-large:latest	2025-12-02T05:03:52.164917351-07:00	669615493	468836162de7f81e041c43663fedbbba921dcea9b9fefea135685a39b2d83dd8	{"family": "bert", "format": "gguf", "families": ["bert"], "parent_model": "", "parameter_size": "334M", "quantization_level": "F16"}
da9aa867-48e4-4012-b7b8-e3c0cf4d7766	2026-01-16 11:23:55.379614+00	granite4:micro	granite4:micro	2025-10-31T03:08:40.326521572-06:00	2099521385	89962fcc75239ac434cdebceb6b7e0669397f92eaef9c487774b718bc36a3e5f	{"family": "granite", "format": "gguf", "families": ["granite"], "parent_model": "", "parameter_size": "3.4B", "quantization_level": "Q4_K_M"}
e99cf2d1-56a3-4801-bca7-790bc1d42d08	2026-01-16 11:23:55.379614+00	hengwen/watt-tool-8B:latest	hengwen/watt-tool-8B:latest	2025-10-25T01:14:38.031573994-06:00	4921248041	99577f6734df650a77b291a49de7562d4757c264a2d8228e77098dd49aa4f8a0	{"family": "llama", "format": "gguf", "families": ["llama"], "parent_model": "", "parameter_size": "8.0B", "quantization_level": "Q4_K_M"}
\.


--
-- Data for Name: openrouter_models; Type: TABLE DATA; Schema: public; Owner: -
--

COPY public.openrouter_models (_id, _loaded_at, id, canonical_slug, hugging_face_id, name, created, description, context_length, architecture, pricing, top_provider, per_request_limits, supported_parameters, default_parameters, expiration_date) FROM stdin;
d563c75b-a214-4617-b01e-5f7d232ef054	2026-01-16 11:18:55.898017+00	openai/gpt-5.2-codex	openai/gpt-5.2-codex-20260114		OpenAI: GPT-5.2-Codex	1768409315	GPT-5.2-Codex is an upgraded version of GPT-5.1-Codex optimized for software engineering and coding workflows. It is designed for both interactive development sessions and long, independent execution of complex engineering tasks. The model supports building projects from scratch, feature development, debugging, large-scale refactoring, and code review. Compared to GPT-5.1-Codex, 5.2-Codex is more steerable, adheres closely to developer instructions, and produces cleaner, higher-quality code outputs. Reasoning effort can be adjusted with the `reasoning.effort` parameter. Read the [docs here](https://openrouter.ai/docs/use-cases/reasoning-tokens#reasoning-effort-level)\n\nCodex integrates into developer environments including the CLI, IDE extensions, GitHub, and cloud tasks. It adapts reasoning effort dynamically—providing fast responses for small tasks while sustaining extended multi-hour runs for large projects. The model is trained to perform structured code reviews, catching critical flaws by reasoning over dependencies and validating behavior against tests. It also supports multimodal inputs such as images or screenshots for UI development and integrates tool use for search, dependency installation, and environment setup. Codex is intended specifically for agentic coding applications.	400000	{"modality": "text+image->text", "tokenizer": "GPT", "instruct_type": null, "input_modalities": ["text", "image"], "output_modalities": ["text"]}	{"prompt": "0.00000175", "completion": "0.000014", "web_search": "0.01", "input_cache_read": "0.000000175"}	{"is_moderated": true, "context_length": 400000, "max_completion_tokens": 128000}	null	["frequency_penalty", "include_reasoning", "logit_bias", "logprobs", "max_tokens", "presence_penalty", "reasoning", "response_format", "seed", "stop", "structured_outputs", "tool_choice", "tools", "top_logprobs"]	{"top_p": null, "temperature": null, "frequency_penalty": null}	null
4faeb840-ed61-4ce8-8c75-984a210b2747	2026-01-16 11:18:55.898017+00	allenai/molmo-2-8b:free	allenai/molmo-2-8b-20260109	allenai/Molmo2-8B	AllenAI: Molmo2 8B (free)	1767996672	Molmo2-8B is an open vision-language model developed by the Allen Institute for AI (Ai2) as part of the Molmo2 family, supporting image, video, and multi-image understanding and grounding. It is based on Qwen3-8B and uses SigLIP 2 as its vision backbone, outperforming other open-weight, open-data models on short videos, counting, and captioning, while remaining competitive on long-video tasks.	36864	{"modality": "text+image->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["text", "image", "video"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0", "request": "0", "completion": "0", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 36864, "max_completion_tokens": 36864}	null	["frequency_penalty", "logit_bias", "max_tokens", "presence_penalty", "repetition_penalty", "response_format", "seed", "stop", "temperature", "top_k", "top_p"]	{"top_p": null, "temperature": null, "frequency_penalty": null}	null
921a62cb-27b7-4930-9cf1-5233a31d03d6	2026-01-16 11:18:55.898017+00	allenai/olmo-3.1-32b-instruct	allenai/olmo-3.1-32b-instruct-20251215	allenai/Olmo-3.1-32B-Instruct	AllenAI: Olmo 3.1 32B Instruct	1767728554	Olmo 3.1 32B Instruct is a large-scale, 32-billion-parameter instruction-tuned language model engineered for high-performance conversational AI, multi-turn dialogue, and practical instruction following. As part of the Olmo 3.1 family, this variant emphasizes responsiveness to complex user directions and robust chat interactions while retaining strong capabilities on reasoning and coding benchmarks. Developed by Ai2 under the Apache 2.0 license, Olmo 3.1 32B Instruct reflects the Olmo initiative’s commitment to openness and transparency.	65536	{"modality": "text->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.0000002", "request": "0", "completion": "0.0000006", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 65536, "max_completion_tokens": null}	null	["frequency_penalty", "max_tokens", "min_p", "presence_penalty", "repetition_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_k", "top_p"]	{"top_p": 0.95, "temperature": 0.6, "frequency_penalty": null}	null
ec63e0c9-9bb8-4576-b1f1-8bddd57abe28	2026-01-16 11:18:55.898017+00	bytedance-seed/seed-1.6-flash	bytedance-seed/seed-1.6-flash-20250625		ByteDance Seed: Seed 1.6 Flash	1766505011	Seed 1.6 Flash is an ultra-fast multimodal deep thinking model by ByteDance Seed, supporting both text and visual understanding. It features a 256k context window and can generate outputs of up to 16k tokens.	262144	{"modality": "text+image->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["image", "text", "video"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.000000075", "request": "0", "completion": "0.0000003", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 262144, "max_completion_tokens": 16384}	null	["frequency_penalty", "include_reasoning", "max_tokens", "reasoning", "response_format", "structured_outputs", "temperature", "tool_choice", "tools", "top_p"]	{"top_p": null, "temperature": null, "frequency_penalty": null}	null
fbe87140-53ef-4462-8923-058dc1dc024f	2026-01-16 11:18:55.898017+00	bytedance-seed/seed-1.6	bytedance-seed/seed-1.6-20250625		ByteDance Seed: Seed 1.6	1766504997	Seed 1.6 is a general-purpose model released by the ByteDance Seed team. It incorporates multimodal capabilities and adaptive deep thinking with a 256K context window.	262144	{"modality": "text+image->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["image", "text", "video"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000025", "request": "0", "completion": "0.000002", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 262144, "max_completion_tokens": 32768}	null	["frequency_penalty", "include_reasoning", "max_tokens", "reasoning", "response_format", "structured_outputs", "temperature", "tool_choice", "tools", "top_p"]	{"top_p": null, "temperature": null, "frequency_penalty": null}	null
1e7646f6-d6e4-40f6-8b63-136e4cadb19c	2026-01-16 11:18:55.898017+00	minimax/minimax-m2.1	minimax/minimax-m2.1	MiniMaxAI/MiniMax-M2.1	MiniMax: MiniMax M2.1	1766454997	MiniMax-M2.1 is a lightweight, state-of-the-art large language model optimized for coding, agentic workflows, and modern application development. With only 10 billion activated parameters, it delivers a major jump in real-world capability while maintaining exceptional latency, scalability, and cost efficiency.\n\nCompared to its predecessor, M2.1 delivers cleaner, more concise outputs and faster perceived response times. It shows leading multilingual coding performance across major systems and application languages, achieving 49.4% on Multi-SWE-Bench and 72.5% on SWE-Bench Multilingual, and serves as a versatile agent “brain” for IDEs, coding tools, and general-purpose assistance.\n\nTo avoid degrading this model's performance, MiniMax highly recommends preserving reasoning between turns. Learn more about using reasoning_details to pass back reasoning in our [docs](https://openrouter.ai/docs/use-cases/reasoning-tokens#preserving-reasoning-blocks).	196608	{"modality": "text->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000027", "request": "0", "completion": "0.00000112", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 196608, "max_completion_tokens": 65536}	null	["frequency_penalty", "include_reasoning", "logit_bias", "logprobs", "max_tokens", "min_p", "presence_penalty", "reasoning", "repetition_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_k", "top_logprobs", "top_p"]	{"top_p": 0.9, "temperature": 1, "frequency_penalty": null}	null
cb88ae4d-17ee-4d13-a24b-df773cb1431d	2026-01-16 11:18:55.898017+00	openai/gpt-5.1-codex-mini	openai/gpt-5.1-codex-mini-20251113		OpenAI: GPT-5.1-Codex-Mini	1763057820	GPT-5.1-Codex-Mini is a smaller and faster version of GPT-5.1-Codex	400000	{"modality": "text+image->text", "tokenizer": "GPT", "instruct_type": null, "input_modalities": ["image", "text"], "output_modalities": ["text"]}	{"prompt": "0.00000025", "completion": "0.000002", "input_cache_read": "0.000000025"}	{"is_moderated": true, "context_length": 400000, "max_completion_tokens": 100000}	null	["include_reasoning", "max_tokens", "reasoning", "response_format", "seed", "structured_outputs", "tool_choice", "tools"]	{"top_p": null, "temperature": null, "frequency_penalty": null}	null
6a403124-5531-4b73-8cf1-cb525ce7349a	2026-01-16 11:18:55.898017+00	z-ai/glm-4.7	z-ai/glm-4.7-20251222	zai-org/GLM-4.7	Z.AI: GLM 4.7	1766378014	GLM-4.7 is Z.AI’s latest flagship model, featuring upgrades in two key areas: enhanced programming capabilities and more stable multi-step reasoning/execution. It demonstrates significant improvements in executing complex agent tasks while delivering more natural conversational experiences and superior front-end aesthetics.	202752	{"modality": "text->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.0000004", "request": "0", "completion": "0.0000015", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 202752, "max_completion_tokens": 65535}	null	["frequency_penalty", "include_reasoning", "logit_bias", "logprobs", "max_tokens", "min_p", "presence_penalty", "reasoning", "repetition_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_a", "top_k", "top_logprobs", "top_p"]	{"top_p": 0.95, "temperature": 1, "frequency_penalty": null}	null
dec38558-55c6-42c3-aaee-1b8f21f7a9dc	2026-01-16 11:18:55.898017+00	google/gemini-3-flash-preview	google/gemini-3-flash-preview-20251217		Google: Gemini 3 Flash Preview	1765987078	Gemini 3 Flash Preview is a high speed, high value thinking model designed for agentic workflows, multi turn chat, and coding assistance. It delivers near Pro level reasoning and tool use performance with substantially lower latency than larger Gemini variants, making it well suited for interactive development, long running agent loops, and collaborative coding tasks. Compared to Gemini 2.5 Flash, it provides broad quality improvements across reasoning, multimodal understanding, and reliability.\n\nThe model supports a 1M token context window and multimodal inputs including text, images, audio, video, and PDFs, with text output. It includes configurable reasoning via thinking levels (minimal, low, medium, high), structured output, tool use, and automatic context caching. Gemini 3 Flash Preview is optimized for users who want strong reasoning and agentic behavior without the cost or latency of full scale frontier models.	1048576	{"modality": "text+image->text", "tokenizer": "Gemini", "instruct_type": null, "input_modalities": ["text", "image", "file", "audio", "video"], "output_modalities": ["text"]}	{"audio": "0.000001", "image": "0", "prompt": "0.0000005", "request": "0", "completion": "0.000003", "web_search": "0", "input_cache_read": "0.00000005", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 1048576, "max_completion_tokens": 65535}	null	["include_reasoning", "max_tokens", "reasoning", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_p"]	{"top_p": null, "temperature": null, "frequency_penalty": null}	null
3b5f4bf6-150a-4a43-b870-441818297809	2026-01-16 11:18:55.898017+00	mistralai/mistral-small-creative	mistralai/mistral-small-creative-20251216	null	Mistral: Mistral Small Creative	1765908653	Mistral Small Creative is an experimental small model designed for creative writing, narrative generation, roleplay and character-driven dialogue, general-purpose instruction following, and conversational agents.	32768	{"modality": "text->text", "tokenizer": "Mistral", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"prompt": "0.0000001", "completion": "0.0000003"}	{"is_moderated": false, "context_length": 32768, "max_completion_tokens": null}	null	["tool_choice", "tools"]	{"top_p": 0.95, "temperature": 0.3, "frequency_penalty": null}	null
ee059933-6c05-49b1-8bb1-0c8cf2c12f72	2026-01-16 11:18:55.898017+00	allenai/olmo-3.1-32b-think	allenai/olmo-3.1-32b-think-20251215	allenai/Olmo-3.1-32B-Think	AllenAI: Olmo 3.1 32B Think	1765907719	Olmo 3.1 32B Think is a large-scale, 32-billion-parameter model designed for deep reasoning, complex multi-step logic, and advanced instruction following. Building on the Olmo 3 series, version 3.1 delivers refined reasoning behavior and stronger performance across demanding evaluations and nuanced conversational tasks. Developed by Ai2 under the Apache 2.0 license, Olmo 3.1 32B Think continues the Olmo initiative’s commitment to openness, providing full transparency across model weights, code, and training methodology.	65536	{"modality": "text->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000015", "request": "0", "completion": "0.0000005", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 65536, "max_completion_tokens": 65536}	null	["frequency_penalty", "include_reasoning", "logit_bias", "max_tokens", "presence_penalty", "reasoning", "repetition_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "top_k", "top_p"]	{"top_p": 0.95, "temperature": 0.6, "frequency_penalty": null}	null
13f7eaa9-17ea-4062-b938-6da92fa46a51	2026-01-16 11:18:55.898017+00	xiaomi/mimo-v2-flash:free	xiaomi/mimo-v2-flash-20251210	XiaomiMiMo/MiMo-V2-Flash	Xiaomi: MiMo-V2-Flash (free)	1765731308	MiMo-V2-Flash is an open-source foundation language model developed by Xiaomi. It is a Mixture-of-Experts model with 309B total parameters and 15B active parameters, adopting hybrid attention architecture. MiMo-V2-Flash supports a hybrid-thinking toggle and a 256K context window, and excels at reasoning, coding, and agent scenarios. On SWE-bench Verified and SWE-bench Multilingual, MiMo-V2-Flash ranks as the top #1 open-source model globally, delivering performance comparable to Claude Sonnet 4.5 while costing only about 3.5% as much.\n\nNote: when integrating with agentic tools such as Claude Code, Cline, or Roo Code, **turn off reasoning mode** for the best and fastest performance—this model is deeply optimized for this scenario.\n\nUsers can control the reasoning behaviour with the `reasoning` `enabled` boolean. [Learn more in our docs](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config).	262144	{"modality": "text->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0", "request": "0", "completion": "0", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 262144, "max_completion_tokens": 65536}	null	["frequency_penalty", "include_reasoning", "max_tokens", "presence_penalty", "reasoning", "response_format", "stop", "temperature", "tool_choice", "tools", "top_p"]	{"top_p": 0.95, "temperature": null, "frequency_penalty": null}	null
8c006957-d7c1-40b3-acb2-10f0d7e4bb91	2026-01-16 11:18:55.898017+00	openai/gpt-5.2	openai/gpt-5.2-20251211		OpenAI: GPT-5.2	1765389775	GPT-5.2 is the latest frontier-grade model in the GPT-5 series, offering stronger agentic and long context perfomance compared to GPT-5.1. It uses adaptive reasoning to allocate computation dynamically, responding quickly to simple queries while spending more depth on complex tasks.\n\nBuilt for broad task coverage, GPT-5.2 delivers consistent gains across math, coding, sciende, and tool calling workloads, with more coherent long-form answers and improved tool-use reliability.	400000	{"modality": "text+image->text", "tokenizer": "GPT", "instruct_type": null, "input_modalities": ["file", "image", "text"], "output_modalities": ["text"]}	{"prompt": "0.00000175", "completion": "0.000014", "web_search": "0.01", "input_cache_read": "0.000000175"}	{"is_moderated": true, "context_length": 400000, "max_completion_tokens": 128000}	null	["include_reasoning", "max_tokens", "reasoning", "response_format", "seed", "structured_outputs", "tool_choice", "tools"]	{"top_p": null, "temperature": null, "frequency_penalty": null}	null
249adbdc-91fd-4350-ad02-2115ac38b6ce	2026-01-16 11:18:55.898017+00	xiaomi/mimo-v2-flash	xiaomi/mimo-v2-flash-20251210	XiaomiMiMo/MiMo-V2-Flash	Xiaomi: MiMo-V2-Flash	1765731308	MiMo-V2-Flash is an open-source foundation language model developed by Xiaomi. It is a Mixture-of-Experts model with 309B total parameters and 15B active parameters, adopting hybrid attention architecture. MiMo-V2-Flash supports a hybrid-thinking toggle and a 256K context window, and excels at reasoning, coding, and agent scenarios. On SWE-bench Verified and SWE-bench Multilingual, MiMo-V2-Flash ranks as the top #1 open-source model globally, delivering performance comparable to Claude Sonnet 4.5 while costing only about 3.5% as much.\n\nNote: when integrating with agentic tools such as Claude Code, Cline, or Roo Code, **turn off reasoning mode** for the best and fastest performance—this model is deeply optimized for this scenario.\n\nUsers can control the reasoning behaviour with the `reasoning` `enabled` boolean. [Learn more in our docs](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config).	262144	{"modality": "text->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.0000001", "request": "0", "completion": "0.0000003", "web_search": "0", "input_cache_read": "0.00000002", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 262144, "max_completion_tokens": 32000}	null	["frequency_penalty", "include_reasoning", "max_tokens", "presence_penalty", "reasoning", "repetition_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_k", "top_p"]	{"top_p": 0.95, "temperature": null, "frequency_penalty": null}	null
d8026ca7-e2fd-4780-ac71-71150054a1d9	2026-01-16 11:18:55.898017+00	nvidia/nemotron-3-nano-30b-a3b:free	nvidia/nemotron-3-nano-30b-a3b	nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16	NVIDIA: Nemotron 3 Nano 30B A3B (free)	1765731275	NVIDIA Nemotron 3 Nano 30B A3B is a small language MoE model with highest compute efficiency and accuracy for developers to build specialized agentic AI systems.\n\nThe model is fully open with open-weights, datasets and recipes so developers can easily\ncustomize, optimize, and deploy the model on their infrastructure for maximum privacy and\nsecurity.\n\nNote: For the free endpoint, all prompts and output are logged to improve the provider's model and its product and services. Please do not upload any personal, confidential, or otherwise sensitive information. This is a trial use only. Do not use for production or business-critical systems.	256000	{"modality": "text->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0", "request": "0", "completion": "0", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 256000, "max_completion_tokens": null}	null	["include_reasoning", "max_tokens", "reasoning", "seed", "temperature", "tool_choice", "tools", "top_p"]	{"top_p": null, "temperature": null, "frequency_penalty": null}	null
41d04096-9020-486b-b381-b07b5c4f98bd	2026-01-16 11:18:55.898017+00	nvidia/nemotron-3-nano-30b-a3b	nvidia/nemotron-3-nano-30b-a3b	nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16	NVIDIA: Nemotron 3 Nano 30B A3B	1765731275	NVIDIA Nemotron 3 Nano 30B A3B is a small language MoE model with highest compute efficiency and accuracy for developers to build specialized agentic AI systems.\n\nThe model is fully open with open-weights, datasets and recipes so developers can easily\ncustomize, optimize, and deploy the model on their infrastructure for maximum privacy and\nsecurity.\n\nNote: For the free endpoint, all prompts and output are logged to improve the provider's model and its product and services. Please do not upload any personal, confidential, or otherwise sensitive information. This is a trial use only. Do not use for production or business-critical systems.	262144	{"modality": "text->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000006", "request": "0", "completion": "0.00000024", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 262144, "max_completion_tokens": 262144}	null	["frequency_penalty", "include_reasoning", "max_tokens", "min_p", "presence_penalty", "reasoning", "repetition_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_k", "top_p"]	{"top_p": null, "temperature": null, "frequency_penalty": null}	null
54f90254-3b81-4b16-b20a-c9a1f27caa25	2026-01-16 11:18:55.898017+00	openai/gpt-5.2-chat	openai/gpt-5.2-chat-20251211		OpenAI: GPT-5.2 Chat	1765389783	GPT-5.2 Chat (AKA Instant) is the fast, lightweight member of the 5.2 family, optimized for low-latency chat while retaining strong general intelligence. It uses adaptive reasoning to selectively “think” on harder queries, improving accuracy on math, coding, and multi-step tasks without slowing down typical conversations. The model is warmer and more conversational by default, with better instruction following and more stable short-form reasoning. GPT-5.2 Chat is designed for high-throughput, interactive workloads where responsiveness and consistency matter more than deep deliberation.	128000	{"modality": "text+image->text", "tokenizer": "GPT", "instruct_type": null, "input_modalities": ["file", "image", "text"], "output_modalities": ["text"]}	{"prompt": "0.00000175", "completion": "0.000014", "web_search": "0.01", "input_cache_read": "0.000000175"}	{"is_moderated": true, "context_length": 128000, "max_completion_tokens": 16384}	null	["max_tokens", "response_format", "seed", "structured_outputs", "tool_choice", "tools"]	{"top_p": null, "temperature": null, "frequency_penalty": null}	null
76d3f58e-daa1-4610-9748-09a686d6e6ba	2026-01-16 11:18:55.898017+00	openai/gpt-5.2-pro	openai/gpt-5.2-pro-20251211		OpenAI: GPT-5.2 Pro	1765389780	GPT-5.2 Pro is OpenAI’s most advanced model, offering major improvements in agentic coding and long context performance over GPT-5 Pro. It is optimized for complex tasks that require step-by-step reasoning, instruction following, and accuracy in high-stakes use cases. It supports test-time routing features and advanced prompt understanding, including user-specified intent like "think hard about this." Improvements include reductions in hallucination, sycophancy, and better performance in coding, writing, and health-related tasks.	400000	{"modality": "text+image->text", "tokenizer": "GPT", "instruct_type": null, "input_modalities": ["image", "text", "file"], "output_modalities": ["text"]}	{"prompt": "0.000021", "completion": "0.000168", "web_search": "0.01"}	{"is_moderated": true, "context_length": 400000, "max_completion_tokens": 128000}	null	["include_reasoning", "max_tokens", "reasoning", "response_format", "seed", "structured_outputs", "tool_choice", "tools"]	{"top_p": null, "temperature": null, "frequency_penalty": null}	null
f1e7b254-4961-4132-ace0-5668c430d024	2026-01-16 11:18:55.898017+00	openrouter/bodybuilder	openrouter/bodybuilder		Body Builder (beta)	1764903653	Transform your natural language requests into structured OpenRouter API request objects. Describe what you want to accomplish with AI models, and Body Builder will construct the appropriate API calls. Example: "count to 10 using gemini and opus."\n\nThis is useful for creating multi-model requests, custom model routers, or programmatic generation of API calls from human descriptions.\n\n**BETA NOTICE**: Body Builder is in beta, and currently free. Pricing and functionality may change in the future.	128000	{"modality": "text->text", "tokenizer": "Router", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"prompt": "-1", "completion": "-1"}	{"is_moderated": false, "context_length": null, "max_completion_tokens": null}	null	[]	{"top_p": null, "temperature": null, "frequency_penalty": null}	null
5e8fa635-c7b3-4761-836b-46dcefd5a045	2026-01-16 11:18:55.898017+00	mistralai/devstral-2512:free	mistralai/devstral-2512	mistralai/Devstral-2-123B-Instruct-2512	Mistral: Devstral 2 2512 (free)	1765285419	Devstral 2 is a state-of-the-art open-source model by Mistral AI specializing in agentic coding. It is a 123B-parameter dense transformer model supporting a 256K context window.\n\nDevstral 2 supports exploring codebases and orchestrating changes across multiple files while maintaining architecture-level context. It tracks framework dependencies, detects failures, and retries with corrections—solving challenges like bug fixing and modernizing legacy systems. The model can be fine-tuned to prioritize specific languages or optimize for large enterprise codebases. It is available under a modified MIT license.	262144	{"modality": "text->text", "tokenizer": "Mistral", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"prompt": "0", "completion": "0"}	{"is_moderated": false, "context_length": 262144, "max_completion_tokens": null}	null	["frequency_penalty", "max_tokens", "presence_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_p"]	{"top_p": null, "temperature": 0.3, "frequency_penalty": null}	null
8ff7796f-0e89-4215-ba80-c4bbcc89670e	2026-01-16 11:18:55.898017+00	mistralai/devstral-2512	mistralai/devstral-2512	mistralai/Devstral-2-123B-Instruct-2512	Mistral: Devstral 2 2512	1765285419	Devstral 2 is a state-of-the-art open-source model by Mistral AI specializing in agentic coding. It is a 123B-parameter dense transformer model supporting a 256K context window.\n\nDevstral 2 supports exploring codebases and orchestrating changes across multiple files while maintaining architecture-level context. It tracks framework dependencies, detects failures, and retries with corrections—solving challenges like bug fixing and modernizing legacy systems. The model can be fine-tuned to prioritize specific languages or optimize for large enterprise codebases. It is available under a modified MIT license.	262144	{"modality": "text->text", "tokenizer": "Mistral", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000005", "request": "0", "completion": "0.00000022", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 262144, "max_completion_tokens": 65536}	null	["frequency_penalty", "max_tokens", "presence_penalty", "repetition_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_k", "top_p"]	{"top_p": null, "temperature": 0.3, "frequency_penalty": null}	null
acac8831-2633-4ce9-b460-acbcc2c3eee4	2026-01-16 11:18:55.898017+00	relace/relace-search	relace/relace-search-20251208	null	Relace: Relace Search	1765213560	The relace-search model uses 4-12 `view_file` and `grep` tools in parallel to explore a codebase and return relevant files to the user request. \n\nIn contrast to RAG, relace-search performs agentic multi-step reasoning to produce highly precise results 4x faster than any frontier model. It's designed to serve as a subagent that passes its findings to an "oracle" coding agent, who orchestrates/performs the rest of the coding task.\n\nTo use relace-search you need to build an appropriate agent harness, and parse the response for relevant information to hand off to the oracle. Read more about it in the [Relace documentation](https://docs.relace.ai/docs/fast-agentic-search/agent).	256000	{"modality": "text->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.000001", "request": "0", "completion": "0.000003", "web_search": "0", "input_cache_read": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 256000, "max_completion_tokens": 128000}	null	["max_tokens", "seed", "stop", "temperature", "tool_choice", "tools", "top_p"]	{"top_p": null, "temperature": null, "frequency_penalty": null}	null
d6a65c57-38d2-419f-b2ec-81def96c49f0	2026-01-16 11:18:55.898017+00	z-ai/glm-4.6v	z-ai/glm-4.6-20251208	zai-org/GLM-4.6V	Z.AI: GLM 4.6V	1765207462	GLM-4.6V is a large multimodal model designed for high-fidelity visual understanding and long-context reasoning across images, documents, and mixed media. It supports up to 128K tokens, processes complex page layouts and charts directly as visual inputs, and integrates native multimodal function calling to connect perception with downstream tool execution. The model also enables interleaved image-text generation and UI reconstruction workflows, including screenshot-to-HTML synthesis and iterative visual editing.	131072	{"modality": "text+image->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["image", "text", "video"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.0000003", "request": "0", "completion": "0.0000009", "web_search": "0", "input_cache_read": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 131072, "max_completion_tokens": 131072}	null	["frequency_penalty", "include_reasoning", "logit_bias", "max_tokens", "min_p", "presence_penalty", "reasoning", "repetition_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_k", "top_p"]	{"top_p": 0.6, "temperature": 0.8, "frequency_penalty": null}	null
f236d1e6-b751-44a6-8887-bbce72d6d9cf	2026-01-16 11:18:55.898017+00	nex-agi/deepseek-v3.1-nex-n1	nex-agi/deepseek-v3.1-nex-n1	nex-agi/DeepSeek-V3.1-Nex-N1	Nex AGI: DeepSeek V3.1 Nex N1	1765204393	DeepSeek V3.1 Nex-N1 is the flagship release of the Nex-N1 series — a post-trained model designed to highlight agent autonomy, tool use, and real-world productivity. \n\nNex-N1 demonstrates competitive performance across all evaluation scenarios, showing particularly strong results in practical coding and HTML generation tasks.	131072	{"modality": "text->text", "tokenizer": "DeepSeek", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000027", "request": "0", "completion": "0.000001", "web_search": "0", "input_cache_read": "0", "input_cache_write": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 131072, "max_completion_tokens": 163840}	null	["frequency_penalty", "response_format", "structured_outputs", "temperature", "tool_choice", "tools", "top_k", "top_p"]	{"top_p": null, "temperature": null, "frequency_penalty": null}	null
f874dd83-236f-40a8-88ff-79c5ff567b90	2026-01-16 11:18:55.898017+00	essentialai/rnj-1-instruct	essentialai/rnj-1-instruct	EssentialAI/rnj-1-instruct	EssentialAI: Rnj 1 Instruct	1765094847	Rnj-1 is an 8B-parameter, dense, open-weight model family developed by Essential AI and trained from scratch with a focus on programming, math, and scientific reasoning. The model demonstrates strong performance across multiple programming languages, tool-use workflows, and agentic execution environments (e.g., mini-SWE-agent). 	32768	{"modality": "text->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000015", "request": "0", "completion": "0.00000015", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 32768, "max_completion_tokens": null}	null	["frequency_penalty", "logit_bias", "max_tokens", "min_p", "presence_penalty", "repetition_penalty", "response_format", "stop", "structured_outputs", "temperature", "top_k", "top_p"]	{"top_p": null, "temperature": null, "frequency_penalty": null}	null
2fdaa649-d08d-486e-9bbe-9a477c11029f	2026-01-16 11:18:55.898017+00	openai/gpt-5.1-codex-max	openai/gpt-5.1-codex-max-20251204		OpenAI: GPT-5.1-Codex-Max	1764878934	GPT-5.1-Codex-Max is OpenAI’s latest agentic coding model, designed for long-running, high-context software development tasks. It is based on an updated version of the 5.1 reasoning stack and trained on agentic workflows spanning software engineering, mathematics, and research. \nGPT-5.1-Codex-Max delivers faster performance, improved reasoning, and higher token efficiency across the development lifecycle. 	400000	{"modality": "text+image->text", "tokenizer": "GPT", "instruct_type": null, "input_modalities": ["text", "image"], "output_modalities": ["text"]}	{"prompt": "0.00000125", "completion": "0.00001", "input_cache_read": "0.000000125"}	{"is_moderated": true, "context_length": 400000, "max_completion_tokens": 128000}	null	["include_reasoning", "max_tokens", "reasoning", "response_format", "seed", "structured_outputs", "tool_choice", "tools"]	{"top_p": null, "temperature": null, "frequency_penalty": null}	null
c4ce7982-a3a8-4c69-9602-44777be2e426	2026-01-16 11:18:55.898017+00	amazon/nova-2-lite-v1	amazon/nova-2-lite-v1		Amazon: Nova 2 Lite	1764696672	Nova 2 Lite is a fast, cost-effective reasoning model for everyday workloads that can process text, images, and videos to generate text. \n\nNova 2 Lite demonstrates standout capabilities in processing documents, extracting information from videos, generating code, providing accurate grounded answers, and automating multi-step agentic workflows.	1000000	{"modality": "text+image->text", "tokenizer": "Nova", "instruct_type": null, "input_modalities": ["text", "image", "video", "file"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.0000003", "request": "0", "completion": "0.0000025", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": true, "context_length": 1000000, "max_completion_tokens": 65535}	null	["include_reasoning", "max_tokens", "reasoning", "stop", "temperature", "tool_choice", "tools", "top_k", "top_p"]	{"top_p": null, "temperature": null, "frequency_penalty": null}	null
0c63b061-5bc2-4f87-b21d-c8c6986d3560	2026-01-16 11:18:55.898017+00	mistralai/ministral-14b-2512	mistralai/ministral-14b-2512	mistralai/Ministral-3-14B-Instruct-2512	Mistral: Ministral 3 14B 2512	1764681735	The largest model in the Ministral 3 family, Ministral 3 14B offers frontier capabilities and performance comparable to its larger Mistral Small 3.2 24B counterpart. A powerful and efficient language model with vision capabilities.	262144	{"modality": "text+image->text", "tokenizer": "Mistral", "instruct_type": null, "input_modalities": ["text", "image"], "output_modalities": ["text"]}	{"prompt": "0.0000002", "completion": "0.0000002"}	{"is_moderated": false, "context_length": 262144, "max_completion_tokens": null}	null	["frequency_penalty", "logit_bias", "max_tokens", "min_p", "presence_penalty", "repetition_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_k", "top_p"]	{"top_p": null, "temperature": 0.3, "frequency_penalty": null}	null
4bc86ef3-be40-4ca3-8129-7febf51a367a	2026-01-16 11:18:55.898017+00	mistralai/ministral-8b-2512	mistralai/ministral-8b-2512	mistralai/Ministral-3-8B-Instruct-2512	Mistral: Ministral 3 8B 2512	1764681654	A balanced model in the Ministral 3 family, Ministral 3 8B is a powerful, efficient tiny language model with vision capabilities.	262144	{"modality": "text+image->text", "tokenizer": "Mistral", "instruct_type": null, "input_modalities": ["text", "image"], "output_modalities": ["text"]}	{"prompt": "0.00000015", "completion": "0.00000015"}	{"is_moderated": false, "context_length": 262144, "max_completion_tokens": null}	null	["frequency_penalty", "max_tokens", "presence_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_p"]	{"top_p": null, "temperature": 0.3, "frequency_penalty": null}	null
c7eb76a3-609b-40d1-b4cf-d70a79d4ec2e	2026-01-16 11:18:55.898017+00	mistralai/ministral-3b-2512	mistralai/ministral-3b-2512	mistralai/Ministral-3-3B-Instruct-2512	Mistral: Ministral 3 3B 2512	1764681560	The smallest model in the Ministral 3 family, Ministral 3 3B is a powerful, efficient tiny language model with vision capabilities.	131072	{"modality": "text+image->text", "tokenizer": "Mistral", "instruct_type": null, "input_modalities": ["text", "image"], "output_modalities": ["text"]}	{"prompt": "0.0000001", "completion": "0.0000001"}	{"is_moderated": false, "context_length": 131072, "max_completion_tokens": null}	null	["frequency_penalty", "max_tokens", "presence_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_p"]	{"top_p": null, "temperature": 0.3, "frequency_penalty": null}	null
25a1bffc-3439-4506-8e08-9c74c2c0850f	2026-01-16 11:18:55.898017+00	mistralai/mistral-large-2512	mistralai/mistral-large-2512		Mistral: Mistral Large 3 2512	1764624472	Mistral Large 3 2512 is Mistral’s most capable model to date, featuring a sparse mixture-of-experts architecture with 41B active parameters (675B total), and released under the Apache 2.0 license.	262144	{"modality": "text+image->text", "tokenizer": "Mistral", "instruct_type": null, "input_modalities": ["text", "image"], "output_modalities": ["text"]}	{"prompt": "0.0000005", "completion": "0.0000015"}	{"is_moderated": false, "context_length": 262144, "max_completion_tokens": null}	null	["frequency_penalty", "max_tokens", "presence_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_p"]	{"top_p": null, "temperature": 0.0645, "frequency_penalty": null}	null
194ae996-7718-4da5-ae62-9f23830a64c6	2026-01-16 11:18:55.898017+00	arcee-ai/trinity-mini:free	arcee-ai/trinity-mini-20251201	arcee-ai/Trinity-Mini	Arcee AI: Trinity Mini (free)	1764601720	Trinity Mini is a 26B-parameter (3B active) sparse mixture-of-experts language model featuring 128 experts with 8 active per token. Engineered for efficient reasoning over long contexts (131k) with robust function calling and multi-step agent workflows.	131072	{"modality": "text->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0", "request": "0", "completion": "0", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 131072, "max_completion_tokens": null}	null	["include_reasoning", "max_tokens", "reasoning", "response_format", "structured_outputs", "temperature", "tool_choice", "tools", "top_k", "top_p"]	{"top_p": 0.75, "temperature": 0.15, "frequency_penalty": null}	null
a4c4974c-a15e-4657-809b-17d46379977b	2026-01-16 11:18:55.898017+00	arcee-ai/trinity-mini	arcee-ai/trinity-mini-20251201	arcee-ai/Trinity-Mini	Arcee AI: Trinity Mini	1764601720	Trinity Mini is a 26B-parameter (3B active) sparse mixture-of-experts language model featuring 128 experts with 8 active per token. Engineered for efficient reasoning over long contexts (131k) with robust function calling and multi-step agent workflows.	131072	{"modality": "text->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.000000045", "request": "0", "completion": "0.00000015", "web_search": "0", "input_cache_read": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 131072, "max_completion_tokens": 131072}	null	["frequency_penalty", "include_reasoning", "logit_bias", "max_tokens", "min_p", "presence_penalty", "reasoning", "repetition_penalty", "response_format", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_k", "top_p"]	{"top_p": 0.75, "temperature": 0.15, "frequency_penalty": null}	null
e7596b01-c2ae-4c73-8a0b-7fd73ce83ec0	2026-01-16 11:18:55.898017+00	deepseek/deepseek-v3.2-speciale	deepseek/deepseek-v3.2-speciale-20251201	deepseek-ai/DeepSeek-V3.2-Speciale	DeepSeek: DeepSeek V3.2 Speciale	1764594837	DeepSeek-V3.2-Speciale is a high-compute variant of DeepSeek-V3.2 optimized for maximum reasoning and agentic performance. It builds on DeepSeek Sparse Attention (DSA) for efficient long-context processing, then scales post-training reinforcement learning to push capability beyond the base model. Reported evaluations place Speciale ahead of GPT-5 on difficult reasoning workloads, with proficiency comparable to Gemini-3.0-Pro, while retaining strong coding and tool-use reliability. Like V3.2, it benefits from a large-scale agentic task synthesis pipeline that improves compliance and generalization in interactive environments.	163840	{"modality": "text->text", "tokenizer": "DeepSeek", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000027", "request": "0", "completion": "0.00000041", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 163840, "max_completion_tokens": 65536}	null	["frequency_penalty", "include_reasoning", "logit_bias", "max_tokens", "presence_penalty", "reasoning", "repetition_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "top_k", "top_p"]	{"top_p": 0.95, "temperature": 1, "frequency_penalty": null}	null
dfc72496-25c8-444f-b3da-c1404b5ee8e9	2026-01-16 11:18:55.898017+00	deepseek/deepseek-v3.2	deepseek/deepseek-v3.2-20251201	deepseek-ai/DeepSeek-V3.2	DeepSeek: DeepSeek V3.2	1764594642	DeepSeek-V3.2 is a large language model designed to harmonize high computational efficiency with strong reasoning and agentic tool-use performance. It introduces DeepSeek Sparse Attention (DSA), a fine-grained sparse attention mechanism that reduces training and inference cost while preserving quality in long-context scenarios. A scalable reinforcement learning post-training framework further improves reasoning, with reported performance in the GPT-5 class, and the model has demonstrated gold-medal results on the 2025 IMO and IOI. V3.2 also uses a large-scale agentic task synthesis pipeline to better integrate reasoning into tool-use settings, boosting compliance and generalization in interactive environments.\n\nUsers can control the reasoning behaviour with the `reasoning` `enabled` boolean. [Learn more in our docs](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config)	163840	{"modality": "text->text", "tokenizer": "DeepSeek", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000025", "request": "0", "completion": "0.00000038", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 163840, "max_completion_tokens": 65536}	null	["frequency_penalty", "include_reasoning", "logit_bias", "logprobs", "max_tokens", "min_p", "presence_penalty", "reasoning", "repetition_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_k", "top_logprobs", "top_p"]	{"top_p": 0.95, "temperature": 1, "frequency_penalty": null}	null
fb6512ff-6d83-4daa-8094-0fab9f19f70a	2026-01-16 11:18:55.898017+00	prime-intellect/intellect-3	prime-intellect/intellect-3-20251126	PrimeIntellect/INTELLECT-3-FP8	Prime Intellect: INTELLECT-3	1764212534	INTELLECT-3 is a 106B-parameter Mixture-of-Experts model (12B active) post-trained from GLM-4.5-Air-Base using supervised fine-tuning (SFT) followed by large-scale reinforcement learning (RL). It offers state-of-the-art performance for its size across math, code, science, and general reasoning, consistently outperforming many larger frontier models. Designed for strong multi-step problem solving, it maintains high accuracy on structured tasks while remaining efficient at inference thanks to its MoE architecture.	131072	{"modality": "text->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.0000002", "request": "0", "completion": "0.0000011", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 131072, "max_completion_tokens": 131072}	null	["frequency_penalty", "include_reasoning", "logit_bias", "max_tokens", "presence_penalty", "reasoning", "repetition_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_k", "top_p"]	{"top_p": null, "temperature": 0.6, "frequency_penalty": null}	null
95c7235f-8a36-4542-a51d-f94d7436d81d	2026-01-16 11:18:55.898017+00	tngtech/tng-r1t-chimera:free	tngtech/tng-r1t-chimera	null	TNG: R1T Chimera (free)	1764184161	TNG-R1T-Chimera is an experimental LLM with a faible for creative storytelling and character interaction. It is a derivate of the original TNG/DeepSeek-R1T-Chimera released in April 2025 and is available exclusively via Chutes and OpenRouter.\n\nCharacteristics and improvements include:\n\nWe think that it has a creative and pleasant personality.\nIt has a preliminary EQ-Bench3 value of about 1305.\nIt is quite a bit more intelligent than the original, albeit a slightly slower.\nIt is much more think-token consistent, i.e. reasoning and answer blocks are properly delineated.\nTool calling is much improved.\n\nTNG Tech, the model authors, ask that users follow the careful guidelines that Microsoft has created for their "MAI-DS-R1" DeepSeek-based model. These guidelines are available on Hugging Face (https://huggingface.co/microsoft/MAI-DS-R1).	163840	{"modality": "text->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0", "request": "0", "completion": "0", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 163840, "max_completion_tokens": 65536}	null	["frequency_penalty", "include_reasoning", "max_tokens", "presence_penalty", "reasoning", "repetition_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_k", "top_p"]	{"top_p": null, "temperature": null, "frequency_penalty": null}	null
8084e777-c6c6-445a-a644-a844529abdac	2026-01-16 11:18:55.898017+00	tngtech/tng-r1t-chimera	tngtech/tng-r1t-chimera	null	TNG: R1T Chimera	1764184161	TNG-R1T-Chimera is an experimental LLM with a faible for creative storytelling and character interaction. It is a derivate of the original TNG/DeepSeek-R1T-Chimera released in April 2025 and is available exclusively via Chutes and OpenRouter.\n\nCharacteristics and improvements include:\n\nWe think that it has a creative and pleasant personality.\nIt has a preliminary EQ-Bench3 value of about 1305.\nIt is quite a bit more intelligent than the original, albeit a slightly slower.\nIt is much more think-token consistent, i.e. reasoning and answer blocks are properly delineated.\nTool calling is much improved.\n\nTNG Tech, the model authors, ask that users follow the careful guidelines that Microsoft has created for their "MAI-DS-R1" DeepSeek-based model. These guidelines are available on Hugging Face (https://huggingface.co/microsoft/MAI-DS-R1).	163840	{"modality": "text->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000025", "request": "0", "completion": "0.00000085", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 163840, "max_completion_tokens": 65536}	null	["frequency_penalty", "include_reasoning", "max_tokens", "presence_penalty", "reasoning", "repetition_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_k", "top_p"]	{"top_p": null, "temperature": null, "frequency_penalty": null}	null
c2fb8495-487e-4057-813a-d35f53f32acc	2026-01-16 11:18:55.898017+00	anthropic/claude-opus-4.5	anthropic/claude-4.5-opus-20251124		Anthropic: Claude Opus 4.5	1764010580	Claude Opus 4.5 is Anthropic’s frontier reasoning model optimized for complex software engineering, agentic workflows, and long-horizon computer use. It offers strong multimodal capabilities, competitive performance across real-world coding and reasoning benchmarks, and improved robustness to prompt injection. The model is designed to operate efficiently across varied effort levels, enabling developers to trade off speed, depth, and token usage depending on task requirements. It comes with a new parameter to control token efficiency, which can be accessed using the OpenRouter Verbosity parameter with low, medium, or high.\n\nOpus 4.5 supports advanced tool use, extended context management, and coordinated multi-agent setups, making it well-suited for autonomous research, debugging, multi-step planning, and spreadsheet/browser manipulation. It delivers substantial gains in structured reasoning, execution reliability, and alignment compared to prior Opus generations, while reducing token overhead and improving performance on long-running tasks.	200000	{"modality": "text+image->text", "tokenizer": "Claude", "instruct_type": null, "input_modalities": ["file", "image", "text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.000005", "request": "0", "completion": "0.000025", "web_search": "0.01", "input_cache_read": "0.0000005", "input_cache_write": "0.00000625", "internal_reasoning": "0"}	{"is_moderated": true, "context_length": 200000, "max_completion_tokens": 64000}	null	["include_reasoning", "max_tokens", "reasoning", "response_format", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_k", "verbosity"]	{"top_p": null, "temperature": null, "frequency_penalty": null}	null
03ee07a6-e1d5-41de-b510-dbb5687a999e	2026-01-16 11:18:55.898017+00	allenai/olmo-3-32b-think	allenai/olmo-3-32b-think-20251121	allenai/Olmo-3-32B-Think	AllenAI: Olmo 3 32B Think	1763758276	Olmo 3 32B Think is a large-scale, 32-billion-parameter model purpose-built for deep reasoning, complex logic chains and advanced instruction-following scenarios. Its capacity enables strong performance on demanding evaluation tasks and highly nuanced conversational reasoning. Developed by Ai2 under the Apache 2.0 license, Olmo 3 32B Think embodies the Olmo initiative’s commitment to openness, offering full transparency across weights, code and training methodology.	65536	{"modality": "text->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000015", "request": "0", "completion": "0.0000005", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 65536, "max_completion_tokens": 65536}	null	["frequency_penalty", "include_reasoning", "logit_bias", "max_tokens", "presence_penalty", "reasoning", "repetition_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "top_k", "top_p"]	{"top_p": 0.95, "temperature": 0.6, "frequency_penalty": null}	null
b8e8f7b4-fac1-476f-b205-d9fa19ebac97	2026-01-16 11:18:55.898017+00	allenai/olmo-3-7b-instruct	allenai/olmo-3-7b-instruct-20251121	allenai/Olmo-3-7B-Instruct	AllenAI: Olmo 3 7B Instruct	1763758273	Olmo 3 7B Instruct is a supervised instruction-fine-tuned variant of the Olmo 3 7B base model, optimized for instruction-following, question-answering, and natural conversational dialogue. By leveraging high-quality instruction data and an open training pipeline, it delivers strong performance across everyday NLP tasks while remaining accessible and easy to integrate. Developed by Ai2 under the Apache 2.0 license, the model offers a transparent, community-friendly option for instruction-driven applications.	65536	{"modality": "text->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.0000001", "request": "0", "completion": "0.0000002", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 65536, "max_completion_tokens": 65536}	null	["frequency_penalty", "logit_bias", "max_tokens", "presence_penalty", "repetition_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "top_k", "top_p"]	{"top_p": 0.95, "temperature": 0.6, "frequency_penalty": null}	null
039fb9f3-6397-4ebc-9410-6427609556b6	2026-01-16 11:18:55.898017+00	allenai/olmo-3-7b-think	allenai/olmo-3-7b-think-20251121	allenai/Olmo-3-7B-Think	AllenAI: Olmo 3 7B Think	1763758270	Olmo 3 7B Think is a research-oriented language model in the Olmo family designed for advanced reasoning and instruction-driven tasks. It excels at multi-step problem solving, logical inference, and maintaining coherent conversational context. Developed by Ai2 under the Apache 2.0 license, Olmo 3 7B Think supports transparent, fully open experimentation and provides a lightweight yet capable foundation for academic research and practical NLP workflows.	65536	{"modality": "text->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000012", "request": "0", "completion": "0.0000002", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 65536, "max_completion_tokens": 65536}	null	["frequency_penalty", "include_reasoning", "logit_bias", "max_tokens", "presence_penalty", "reasoning", "repetition_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "top_k", "top_p"]	{"top_p": 0.95, "temperature": 0.6, "frequency_penalty": null}	null
a370e557-4e88-400f-932b-03812e5b9cc6	2026-01-16 11:18:55.898017+00	google/gemini-3-pro-image-preview	google/gemini-3-pro-image-preview-20251120		Google: Nano Banana Pro (Gemini 3 Pro Image Preview)	1763653797	Nano Banana Pro is Google’s most advanced image-generation and editing model, built on Gemini 3 Pro. It extends the original Nano Banana with significantly improved multimodal reasoning, real-world grounding, and high-fidelity visual synthesis. The model generates context-rich graphics, from infographics and diagrams to cinematic composites, and can incorporate real-time information via Search grounding.\n\nIt offers industry-leading text rendering in images (including long passages and multilingual layouts), consistent multi-image blending, and accurate identity preservation across up to five subjects. Nano Banana Pro adds fine-grained creative controls such as localized edits, lighting and focus adjustments, camera transformations, and support for 2K/4K outputs and flexible aspect ratios. It is designed for professional-grade design, product visualization, storyboarding, and complex multi-element compositions while remaining efficient for general image creation workflows.	65536	{"modality": "text+image->text+image", "tokenizer": "Gemini", "instruct_type": null, "input_modalities": ["image", "text"], "output_modalities": ["image", "text"]}	{"image": "0.067", "prompt": "0.000002", "request": "0", "completion": "0.000012", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 65536, "max_completion_tokens": 32768}	null	["include_reasoning", "max_tokens", "reasoning", "response_format", "seed", "stop", "structured_outputs", "temperature", "top_p"]	{"top_p": null, "temperature": null, "frequency_penalty": null}	null
ecacc8dd-694b-4c8b-ad10-ddcc64543299	2026-01-16 11:18:55.898017+00	x-ai/grok-4.1-fast	x-ai/grok-4.1-fast		xAI: Grok 4.1 Fast	1763587502	Grok 4.1 Fast is xAI's best agentic tool calling model that shines in real-world use cases like customer support and deep research. 2M context window.\n\nReasoning can be enabled/disabled using the `reasoning` `enabled` parameter in the API. [Learn more in our docs](https://openrouter.ai/docs/use-cases/reasoning-tokens#controlling-reasoning-tokens)	2000000	{"modality": "text+image->text", "tokenizer": "Grok", "instruct_type": null, "input_modalities": ["text", "image"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.0000002", "request": "0", "completion": "0.0000005", "web_search": "0", "input_cache_read": "0.00000005", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 2000000, "max_completion_tokens": 30000}	null	["include_reasoning", "logprobs", "max_tokens", "reasoning", "response_format", "seed", "structured_outputs", "temperature", "tool_choice", "tools", "top_logprobs", "top_p"]	{"top_p": 0.95, "temperature": 0.7, "frequency_penalty": null}	null
ce43dceb-6036-4224-8bf3-9a46952ef3c4	2026-01-16 11:18:55.898017+00	google/gemini-3-pro-preview	google/gemini-3-pro-preview-20251117		Google: Gemini 3 Pro Preview	1763474668	Gemini 3 Pro is Google’s flagship frontier model for high-precision multimodal reasoning, combining strong performance across text, image, video, audio, and code with a 1M-token context window. Reasoning Details must be preserved when using multi-turn tool calling, see our docs here: https://openrouter.ai/docs/use-cases/reasoning-tokens#preserving-reasoning-blocks. It delivers state-of-the-art benchmark results in general reasoning, STEM problem solving, factual QA, and multimodal understanding, including leading scores on LMArena, GPQA Diamond, MathArena Apex, MMMU-Pro, and Video-MMMU. Interactions emphasize depth and interpretability: the model is designed to infer intent with minimal prompting and produce direct, insight-focused responses.\n\nBuilt for advanced development and agentic workflows, Gemini 3 Pro provides robust tool-calling, long-horizon planning stability, and strong zero-shot generation for complex UI, visualization, and coding tasks. It excels at agentic coding (SWE-Bench Verified, Terminal-Bench 2.0), multimodal analysis, and structured long-form tasks such as research synthesis, planning, and interactive learning experiences. Suitable applications include autonomous agents, coding assistants, multimodal analytics, scientific reasoning, and high-context information processing.	1048576	{"modality": "text+image->text", "tokenizer": "Gemini", "instruct_type": null, "input_modalities": ["text", "image", "file", "audio", "video"], "output_modalities": ["text"]}	{"image": "0.008256", "prompt": "0.000002", "request": "0", "completion": "0.000012", "web_search": "0", "input_cache_read": "0.0000002", "input_cache_write": "0.000002375", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 1048576, "max_completion_tokens": 65536}	null	["include_reasoning", "max_tokens", "reasoning", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_p"]	{"top_p": null, "temperature": null, "frequency_penalty": null}	null
2895c3ec-a008-408d-aa9d-6b09dcf05c39	2026-01-16 11:18:55.898017+00	deepcogito/cogito-v2.1-671b	deepcogito/cogito-v2.1-671b-20251118		Deep Cogito: Cogito v2.1 671B	1763071233	Cogito v2.1 671B MoE represents one of the strongest open models globally, matching performance of frontier closed and open models. This model is trained using self play with reinforcement learning to reach state-of-the-art performance on multiple categories (instruction following, coding, longer queries and creative writing). This advanced system demonstrates significant progress toward scalable superintelligence through policy improvement.	128000	{"modality": "text->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000125", "request": "0", "completion": "0.00000125", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 128000, "max_completion_tokens": null}	null	["frequency_penalty", "include_reasoning", "logit_bias", "max_tokens", "min_p", "presence_penalty", "reasoning", "repetition_penalty", "response_format", "stop", "structured_outputs", "temperature", "top_k", "top_p"]	{"top_p": null, "temperature": null, "frequency_penalty": null}	null
3ef43b9d-0394-40c1-bdc9-0d5fbe623677	2026-01-16 11:18:55.898017+00	openai/gpt-5.1	openai/gpt-5.1-20251113		OpenAI: GPT-5.1	1763060305	GPT-5.1 is the latest frontier-grade model in the GPT-5 series, offering stronger general-purpose reasoning, improved instruction adherence, and a more natural conversational style compared to GPT-5. It uses adaptive reasoning to allocate computation dynamically, responding quickly to simple queries while spending more depth on complex tasks. The model produces clearer, more grounded explanations with reduced jargon, making it easier to follow even on technical or multi-step problems.\n\nBuilt for broad task coverage, GPT-5.1 delivers consistent gains across math, coding, and structured analysis workloads, with more coherent long-form answers and improved tool-use reliability. It also features refined conversational alignment, enabling warmer, more intuitive responses without compromising precision. GPT-5.1 serves as the primary full-capability successor to GPT-5	400000	{"modality": "text+image->text", "tokenizer": "GPT", "instruct_type": null, "input_modalities": ["image", "text", "file"], "output_modalities": ["text"]}	{"prompt": "0.00000125", "completion": "0.00001", "web_search": "0.01", "input_cache_read": "0.000000125"}	{"is_moderated": true, "context_length": 400000, "max_completion_tokens": 128000}	null	["include_reasoning", "max_tokens", "reasoning", "response_format", "seed", "structured_outputs", "tool_choice", "tools"]	{"top_p": null, "temperature": null, "frequency_penalty": null}	null
362cdd28-5879-4c70-8d97-3466f225b1b0	2026-01-16 11:18:55.898017+00	openai/gpt-5.1-chat	openai/gpt-5.1-chat-20251113		OpenAI: GPT-5.1 Chat	1763060302	GPT-5.1 Chat (AKA Instant is the fast, lightweight member of the 5.1 family, optimized for low-latency chat while retaining strong general intelligence. It uses adaptive reasoning to selectively “think” on harder queries, improving accuracy on math, coding, and multi-step tasks without slowing down typical conversations. The model is warmer and more conversational by default, with better instruction following and more stable short-form reasoning. GPT-5.1 Chat is designed for high-throughput, interactive workloads where responsiveness and consistency matter more than deep deliberation.\n	128000	{"modality": "text+image->text", "tokenizer": "GPT", "instruct_type": null, "input_modalities": ["file", "image", "text"], "output_modalities": ["text"]}	{"prompt": "0.00000125", "completion": "0.00001", "web_search": "0.01", "input_cache_read": "0.000000125"}	{"is_moderated": true, "context_length": 128000, "max_completion_tokens": 16384}	null	["max_tokens", "response_format", "seed", "structured_outputs", "tool_choice", "tools"]	{"top_p": null, "temperature": null, "frequency_penalty": null}	null
36310b07-8335-46b9-a401-dd918af8e753	2026-01-16 11:18:55.898017+00	openai/gpt-5.1-codex	openai/gpt-5.1-codex-20251113		OpenAI: GPT-5.1-Codex	1763060298	GPT-5.1-Codex is a specialized version of GPT-5.1 optimized for software engineering and coding workflows. It is designed for both interactive development sessions and long, independent execution of complex engineering tasks. The model supports building projects from scratch, feature development, debugging, large-scale refactoring, and code review. Compared to GPT-5.1, Codex is more steerable, adheres closely to developer instructions, and produces cleaner, higher-quality code outputs. Reasoning effort can be adjusted with the `reasoning.effort` parameter. Read the [docs here](https://openrouter.ai/docs/use-cases/reasoning-tokens#reasoning-effort-level)\n\nCodex integrates into developer environments including the CLI, IDE extensions, GitHub, and cloud tasks. It adapts reasoning effort dynamically—providing fast responses for small tasks while sustaining extended multi-hour runs for large projects. The model is trained to perform structured code reviews, catching critical flaws by reasoning over dependencies and validating behavior against tests. It also supports multimodal inputs such as images or screenshots for UI development and integrates tool use for search, dependency installation, and environment setup. Codex is intended specifically for agentic coding applications.	400000	{"modality": "text+image->text", "tokenizer": "GPT", "instruct_type": null, "input_modalities": ["text", "image"], "output_modalities": ["text"]}	{"prompt": "0.00000125", "completion": "0.00001", "input_cache_read": "0.000000125"}	{"is_moderated": true, "context_length": 400000, "max_completion_tokens": 128000}	null	["include_reasoning", "max_tokens", "reasoning", "response_format", "seed", "structured_outputs", "tool_choice", "tools"]	{"top_p": null, "temperature": null, "frequency_penalty": null}	null
84c73e2a-0175-47e4-a24a-c80fc2cba8df	2026-01-16 11:18:55.898017+00	kwaipilot/kat-coder-pro	kwaipilot/kat-coder-pro-v1		Kwaipilot: KAT-Coder-Pro V1	1762745912	KAT-Coder-Pro V1 is KwaiKAT's most advanced agentic coding model in the KAT-Coder series. Designed specifically for agentic coding tasks, it excels in real-world software engineering scenarios, achieving 73.4% solve rate on the SWE-Bench Verified benchmark. \n\nThe model has been optimized for tool-use capability, multi-turn interaction, instruction following, generalization, and comprehensive capabilities through a multi-stage training process, including mid-training, supervised fine-tuning (SFT), reinforcement fine-tuning (RFT), and scalable agentic RL.	256000	{"modality": "text->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.000000207", "request": "0", "completion": "0.000000828", "web_search": "0", "input_cache_read": "0.0000000414", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 256000, "max_completion_tokens": 128000}	null	["frequency_penalty", "max_tokens", "presence_penalty", "repetition_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_k", "top_p"]	{"top_p": null, "temperature": null, "frequency_penalty": null}	null
55e1aa44-b6d6-48e1-a2be-73d7b43df029	2026-01-16 11:18:55.898017+00	moonshotai/kimi-k2-thinking	moonshotai/kimi-k2-thinking-20251106	moonshotai/Kimi-K2-Thinking	MoonshotAI: Kimi K2 Thinking	1762440622	Kimi K2 Thinking is Moonshot AI’s most advanced open reasoning model to date, extending the K2 series into agentic, long-horizon reasoning. Built on the trillion-parameter Mixture-of-Experts (MoE) architecture introduced in Kimi K2, it activates 32 billion parameters per forward pass and supports 256 k-token context windows. The model is optimized for persistent step-by-step thought, dynamic tool invocation, and complex reasoning workflows that span hundreds of turns. It interleaves step-by-step reasoning with tool use, enabling autonomous research, coding, and writing that can persist for hundreds of sequential actions without drift.\n\nIt sets new open-source benchmarks on HLE, BrowseComp, SWE-Multilingual, and LiveCodeBench, while maintaining stable multi-agent behavior through 200–300 tool calls. Built on a large-scale MoE architecture with MuonClip optimization, it combines strong reasoning depth with high inference efficiency for demanding agentic and analytical tasks.	262144	{"modality": "text->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.0000004", "request": "0", "completion": "0.00000175", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 262144, "max_completion_tokens": 65535}	null	["frequency_penalty", "include_reasoning", "logit_bias", "logprobs", "max_tokens", "min_p", "presence_penalty", "reasoning", "repetition_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_k", "top_logprobs", "top_p"]	{"top_p": null, "temperature": null, "frequency_penalty": null}	null
f2250d49-1f54-4945-9fd9-a17ff62ba9eb	2026-01-16 11:18:55.898017+00	amazon/nova-premier-v1	amazon/nova-premier-v1		Amazon: Nova Premier 1.0	1761950332	Amazon Nova Premier is the most capable of Amazon’s multimodal models for complex reasoning tasks and for use as the best teacher for distilling custom models.	1000000	{"modality": "text+image->text", "tokenizer": "Nova", "instruct_type": null, "input_modalities": ["text", "image"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.0000025", "request": "0", "completion": "0.0000125", "web_search": "0", "input_cache_read": "0.000000625", "internal_reasoning": "0"}	{"is_moderated": true, "context_length": 1000000, "max_completion_tokens": 32000}	null	["max_tokens", "stop", "temperature", "tools", "top_k", "top_p"]	{"top_p": null, "temperature": null, "frequency_penalty": null}	null
858b5b37-8b71-4c25-aecc-65c0f7b0dabf	2026-01-16 11:18:55.898017+00	perplexity/sonar-pro-search	perplexity/sonar-pro-search		Perplexity: Sonar Pro Search	1761854366	Exclusively available on the OpenRouter API, Sonar Pro's new Pro Search mode is Perplexity's most advanced agentic search system. It is designed for deeper reasoning and analysis. Pricing is based on tokens plus $18 per thousand requests. This model powers the Pro Search mode on the Perplexity platform.\n\nSonar Pro Search adds autonomous, multi-step reasoning to Sonar Pro. So, instead of just one query + synthesis, it plans and executes entire research workflows using tools.	200000	{"modality": "text+image->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["text", "image"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.000003", "request": "0.018", "completion": "0.000015", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 200000, "max_completion_tokens": 8000}	null	["frequency_penalty", "include_reasoning", "max_tokens", "presence_penalty", "reasoning", "structured_outputs", "temperature", "top_k", "top_p", "web_search_options"]	{"top_p": null, "temperature": null, "frequency_penalty": null}	null
5b45d9b8-d737-4325-bd0a-bd65a2bb336b	2026-01-16 11:18:55.898017+00	mistralai/voxtral-small-24b-2507	mistralai/voxtral-small-24b-2507	mistralai/Voxtral-Small-24B-2507	Mistral: Voxtral Small 24B 2507	1761835144	Voxtral Small is an enhancement of Mistral Small 3, incorporating state-of-the-art audio input capabilities while retaining best-in-class text performance. It excels at speech transcription, translation and audio understanding. Input audio is priced at $100 per million seconds.	32000	{"modality": "text->text", "tokenizer": "Mistral", "instruct_type": null, "input_modalities": ["text", "audio"], "output_modalities": ["text"]}	{"audio": "0.0001", "prompt": "0.0000001", "completion": "0.0000003"}	{"is_moderated": false, "context_length": 32000, "max_completion_tokens": null}	null	["frequency_penalty", "max_tokens", "presence_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_p"]	{"top_p": 0.95, "temperature": 0.2, "frequency_penalty": null}	null
9b405b67-c17b-47c9-8ae1-71f93a853feb	2026-01-16 11:18:55.898017+00	openai/gpt-oss-safeguard-20b	openai/gpt-oss-safeguard-20b	openai/gpt-oss-safeguard-20b	OpenAI: gpt-oss-safeguard-20b	1761752836	gpt-oss-safeguard-20b is a safety reasoning model from OpenAI built upon gpt-oss-20b. This open-weight, 21B-parameter Mixture-of-Experts (MoE) model offers lower latency for safety tasks like content classification, LLM filtering, and trust & safety labeling.\n\nLearn more about this model in OpenAI's gpt-oss-safeguard [user guide](https://cookbook.openai.com/articles/gpt-oss-safeguard-guide).	131072	{"modality": "text->text", "tokenizer": "GPT", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.000000075", "request": "0", "completion": "0.0000003", "web_search": "0", "input_cache_read": "0.000000037", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 131072, "max_completion_tokens": 65536}	null	["include_reasoning", "max_tokens", "reasoning", "response_format", "seed", "stop", "temperature", "tool_choice", "tools", "top_p"]	{"top_p": null, "temperature": null, "frequency_penalty": null}	null
cd4fa85d-4e09-4db6-8dbc-3ae5c5ad7664	2026-01-16 11:18:55.898017+00	nvidia/nemotron-nano-12b-v2-vl:free	nvidia/nemotron-nano-12b-v2-vl	nvidia/NVIDIA-Nemotron-Nano-12B-v2-VL-BF16	NVIDIA: Nemotron Nano 12B 2 VL (free)	1761675565	NVIDIA Nemotron Nano 2 VL is a 12-billion-parameter open multimodal reasoning model designed for video understanding and document intelligence. It introduces a hybrid Transformer-Mamba architecture, combining transformer-level accuracy with Mamba’s memory-efficient sequence modeling for significantly higher throughput and lower latency.\n\nThe model supports inputs of text and multi-image documents, producing natural-language outputs. It is trained on high-quality NVIDIA-curated synthetic datasets optimized for optical-character recognition, chart reasoning, and multimodal comprehension.\n\nNemotron Nano 2 VL achieves leading results on OCRBench v2 and scores ≈ 74 average across MMMU, MathVista, AI2D, OCRBench, OCR-Reasoning, ChartQA, DocVQA, and Video-MME—surpassing prior open VL baselines. With Efficient Video Sampling (EVS), it handles long-form videos while reducing inference cost.\n\nOpen-weights, training data, and fine-tuning recipes are released under a permissive NVIDIA open license, with deployment supported across NeMo, NIM, and major inference runtimes.	128000	{"modality": "text+image->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["image", "text", "video"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0", "request": "0", "completion": "0", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 128000, "max_completion_tokens": 128000}	null	["include_reasoning", "max_tokens", "reasoning", "seed", "temperature", "tool_choice", "tools", "top_p"]	{"top_p": null, "temperature": null, "frequency_penalty": null}	null
d0e238a6-5590-484e-b9ce-a896cca670af	2026-01-16 11:18:55.898017+00	nvidia/nemotron-nano-12b-v2-vl	nvidia/nemotron-nano-12b-v2-vl	nvidia/NVIDIA-Nemotron-Nano-12B-v2-VL-BF16	NVIDIA: Nemotron Nano 12B 2 VL	1761675565	NVIDIA Nemotron Nano 2 VL is a 12-billion-parameter open multimodal reasoning model designed for video understanding and document intelligence. It introduces a hybrid Transformer-Mamba architecture, combining transformer-level accuracy with Mamba’s memory-efficient sequence modeling for significantly higher throughput and lower latency.\n\nThe model supports inputs of text and multi-image documents, producing natural-language outputs. It is trained on high-quality NVIDIA-curated synthetic datasets optimized for optical-character recognition, chart reasoning, and multimodal comprehension.\n\nNemotron Nano 2 VL achieves leading results on OCRBench v2 and scores ≈ 74 average across MMMU, MathVista, AI2D, OCRBench, OCR-Reasoning, ChartQA, DocVQA, and Video-MME—surpassing prior open VL baselines. With Efficient Video Sampling (EVS), it handles long-form videos while reducing inference cost.\n\nOpen-weights, training data, and fine-tuning recipes are released under a permissive NVIDIA open license, with deployment supported across NeMo, NIM, and major inference runtimes.	131072	{"modality": "text+image->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["image", "text", "video"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.0000002", "request": "0", "completion": "0.0000006", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 131072, "max_completion_tokens": null}	null	["frequency_penalty", "include_reasoning", "max_tokens", "min_p", "presence_penalty", "reasoning", "repetition_penalty", "response_format", "seed", "stop", "temperature", "top_k", "top_p"]	{"top_p": null, "temperature": null, "frequency_penalty": null}	null
0013503f-e8be-4b01-9877-358620ac28bc	2026-01-16 11:18:55.898017+00	minimax/minimax-m2	minimax/minimax-m2	MiniMaxAI/MiniMax-M2	MiniMax: MiniMax M2	1761252093	MiniMax-M2 is a compact, high-efficiency large language model optimized for end-to-end coding and agentic workflows. With 10 billion activated parameters (230 billion total), it delivers near-frontier intelligence across general reasoning, tool use, and multi-step task execution while maintaining low latency and deployment efficiency.\n\nThe model excels in code generation, multi-file editing, compile-run-fix loops, and test-validated repair, showing strong results on SWE-Bench Verified, Multi-SWE-Bench, and Terminal-Bench. It also performs competitively in agentic evaluations such as BrowseComp and GAIA, effectively handling long-horizon planning, retrieval, and recovery from execution errors.\n\nBenchmarked by [Artificial Analysis](https://artificialanalysis.ai/models/minimax-m2), MiniMax-M2 ranks among the top open-source models for composite intelligence, spanning mathematics, science, and instruction-following. Its small activation footprint enables fast inference, high concurrency, and improved unit economics, making it well-suited for large-scale agents, developer assistants, and reasoning-driven applications that require responsiveness and cost efficiency.\n\nTo avoid degrading this model's performance, MiniMax highly recommends preserving reasoning between turns. Learn more about using reasoning_details to pass back reasoning in our [docs](https://openrouter.ai/docs/use-cases/reasoning-tokens#preserving-reasoning-blocks).	196608	{"modality": "text->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.0000002", "request": "0", "completion": "0.000001", "web_search": "0", "input_cache_read": "0.00000003", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 196608, "max_completion_tokens": 65536}	null	["frequency_penalty", "include_reasoning", "max_tokens", "min_p", "presence_penalty", "reasoning", "repetition_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_k", "top_p"]	{"top_p": 0.95, "temperature": 1, "frequency_penalty": null}	null
aab868d3-b1bd-4cc7-abe7-660952d742e9	2026-01-16 11:18:55.898017+00	qwen/qwen3-vl-32b-instruct	qwen/qwen3-vl-32b-instruct	Qwen/Qwen3-VL-32B-Instruct	Qwen: Qwen3 VL 32B Instruct	1761231332	Qwen3-VL-32B-Instruct is a large-scale multimodal vision-language model designed for high-precision understanding and reasoning across text, images, and video. With 32 billion parameters, it combines deep visual perception with advanced text comprehension, enabling fine-grained spatial reasoning, document and scene analysis, and long-horizon video understanding.Robust OCR in 32 languages, and enhanced multimodal fusion through Interleaved-MRoPE and DeepStack architectures. Optimized for agentic interaction and visual tool use, Qwen3-VL-32B delivers state-of-the-art performance for complex real-world multimodal tasks.	262144	{"modality": "text+image->text", "tokenizer": "Qwen", "instruct_type": null, "input_modalities": ["text", "image"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.0000005", "request": "0", "completion": "0.0000015", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 262144, "max_completion_tokens": null}	null	["frequency_penalty", "logit_bias", "max_tokens", "min_p", "presence_penalty", "repetition_penalty", "response_format", "stop", "structured_outputs", "temperature", "top_k", "top_p"]	{"top_p": null, "temperature": null, "frequency_penalty": null}	null
4cd331ca-da79-40d6-b6e7-c1fbe918efa6	2026-01-16 11:18:55.898017+00	liquid/lfm2-8b-a1b	liquid/lfm2-8b-a1b	LiquidAI/LFM2-8B-A1B	LiquidAI/LFM2-8B-A1B	1760970984	Model created via inbox interface	32768	{"modality": "text->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000001", "request": "0", "completion": "0.00000002", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 32768, "max_completion_tokens": null}	null	["frequency_penalty", "max_tokens", "min_p", "presence_penalty", "repetition_penalty", "seed", "stop", "temperature", "top_k", "top_p"]	{}	null
3eeae853-407b-40e4-bbfa-4729401bb1a3	2026-01-16 11:18:55.898017+00	liquid/lfm-2.2-6b	liquid/lfm-2.2-6b	LiquidAI/LFM2-2.6B	LiquidAI/LFM2-2.6B	1760970889	LFM2 is a new generation of hybrid models developed by Liquid AI, specifically designed for edge AI and on-device deployment. It sets a new standard in terms of quality, speed, and memory efficiency.	32768	{"modality": "text->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000001", "request": "0", "completion": "0.00000002", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 32768, "max_completion_tokens": null}	null	["frequency_penalty", "max_tokens", "min_p", "presence_penalty", "repetition_penalty", "seed", "stop", "temperature", "top_k", "top_p"]	{"top_p": null, "temperature": null, "frequency_penalty": null}	null
1fc5c32b-941b-4476-8f5a-fb6244027f18	2026-01-16 11:18:55.898017+00	ibm-granite/granite-4.0-h-micro	ibm-granite/granite-4.0-h-micro	ibm-granite/granite-4.0-h-micro	IBM: Granite 4.0 Micro	1760927695	Granite-4.0-H-Micro is a 3B parameter from the Granite 4 family of models. These models are the latest in a series of models released by IBM. They are fine-tuned for long context tool calling. 	131000	{"modality": "text->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.000000017", "request": "0", "completion": "0.00000011", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 131000, "max_completion_tokens": null}	null	["frequency_penalty", "max_tokens", "presence_penalty", "repetition_penalty", "seed", "temperature", "top_k", "top_p"]	{"top_p": null, "temperature": null, "frequency_penalty": null}	null
9fc31290-fcf1-4d53-997f-e7fc1ab4d634	2026-01-16 11:18:55.898017+00	deepcogito/cogito-v2-preview-llama-405b	deepcogito/cogito-v2-preview-llama-405b	deepcogito/cogito-v2-preview-llama-405B	Deep Cogito: Cogito V2 Preview Llama 405B	1760709933	Cogito v2 405B is a dense hybrid reasoning model that combines direct answering capabilities with advanced self-reflection. It represents a significant step toward frontier intelligence with dense architecture delivering performance competitive with leading closed models. This advanced reasoning system combines policy improvement with massive scale for exceptional capabilities.\n	32768	{"modality": "text->text", "tokenizer": "Llama3", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.0000035", "request": "0", "completion": "0.0000035", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 32768, "max_completion_tokens": null}	null	["frequency_penalty", "include_reasoning", "logit_bias", "max_tokens", "min_p", "presence_penalty", "reasoning", "repetition_penalty", "response_format", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_k", "top_p"]	{"top_p": null, "temperature": null, "frequency_penalty": null}	null
fea0e54a-baae-4bb9-bbc7-298dde2662f2	2026-01-16 11:18:55.898017+00	openai/gpt-5-image-mini	openai/gpt-5-image-mini		OpenAI: GPT-5 Image Mini	1760624583	GPT-5 Image Mini combines OpenAI's advanced language capabilities, powered by [GPT-5 Mini](https://openrouter.ai/openai/gpt-5-mini), with GPT Image 1 Mini for efficient image generation. This natively multimodal model features superior instruction following, text rendering, and detailed image editing with reduced latency and cost. It excels at high-quality visual creation while maintaining strong text understanding, making it ideal for applications that require both efficient image generation and text processing at scale.	400000	{"modality": "text+image->text+image", "tokenizer": "GPT", "instruct_type": null, "input_modalities": ["file", "image", "text"], "output_modalities": ["image", "text"]}	{"prompt": "0.0000025", "completion": "0.000002", "web_search": "0.01", "input_cache_read": "0.00000025"}	{"is_moderated": true, "context_length": 400000, "max_completion_tokens": 128000}	null	["frequency_penalty", "include_reasoning", "logit_bias", "logprobs", "max_tokens", "presence_penalty", "reasoning", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_logprobs", "top_p"]	{"top_p": null, "temperature": null, "frequency_penalty": null}	null
2a6b4151-c411-4da8-8461-3d01f3b96819	2026-01-16 11:18:55.898017+00	anthropic/claude-haiku-4.5	anthropic/claude-4.5-haiku-20251001		Anthropic: Claude Haiku 4.5	1760547638	Claude Haiku 4.5 is Anthropic’s fastest and most efficient model, delivering near-frontier intelligence at a fraction of the cost and latency of larger Claude models. Matching Claude Sonnet 4’s performance across reasoning, coding, and computer-use tasks, Haiku 4.5 brings frontier-level capability to real-time and high-volume applications.\n\nIt introduces extended thinking to the Haiku line; enabling controllable reasoning depth, summarized or interleaved thought output, and tool-assisted workflows with full support for coding, bash, web search, and computer-use tools. Scoring >73% on SWE-bench Verified, Haiku 4.5 ranks among the world’s best coding models while maintaining exceptional responsiveness for sub-agents, parallelized execution, and scaled deployment.	200000	{"modality": "text+image->text", "tokenizer": "Claude", "instruct_type": null, "input_modalities": ["image", "text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.000001", "request": "0", "completion": "0.000005", "web_search": "0", "input_cache_read": "0.0000001", "input_cache_write": "0.00000125", "internal_reasoning": "0"}	{"is_moderated": true, "context_length": 200000, "max_completion_tokens": 64000}	null	["include_reasoning", "max_tokens", "reasoning", "stop", "temperature", "tool_choice", "tools", "top_k", "top_p"]	{"top_p": null, "temperature": null, "frequency_penalty": null}	null
4be8bb7c-276c-43c3-90ef-9bfe75640093	2026-01-16 11:18:55.898017+00	qwen/qwen3-vl-8b-instruct	qwen/qwen3-vl-8b-instruct	Qwen/Qwen3-VL-8B-Instruct	Qwen: Qwen3 VL 8B Instruct	1760463308	Qwen3-VL-8B-Instruct is a multimodal vision-language model from the Qwen3-VL series, built for high-fidelity understanding and reasoning across text, images, and video. It features improved multimodal fusion with Interleaved-MRoPE for long-horizon temporal reasoning, DeepStack for fine-grained visual-text alignment, and text-timestamp alignment for precise event localization.\n\nThe model supports a native 256K-token context window, extensible to 1M tokens, and handles both static and dynamic media inputs for tasks like document parsing, visual question answering, spatial reasoning, and GUI control. It achieves text understanding comparable to leading LLMs while expanding OCR coverage to 32 languages and enhancing robustness under varied visual conditions.	131072	{"modality": "text+image->text", "tokenizer": "Qwen3", "instruct_type": null, "input_modalities": ["image", "text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000008", "request": "0", "completion": "0.0000005", "web_search": "0", "input_cache_read": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 131072, "max_completion_tokens": 32768}	null	["frequency_penalty", "logit_bias", "max_tokens", "min_p", "presence_penalty", "repetition_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_k", "top_p"]	{"top_p": 0.8, "temperature": 0.7, "frequency_penalty": null}	null
8407828d-28dc-4b62-b903-63f877d1ece4	2026-01-16 11:18:55.898017+00	openai/gpt-5-image	openai/gpt-5-image		OpenAI: GPT-5 Image	1760447986	[GPT-5](https://openrouter.ai/openai/gpt-5) Image combines OpenAI's GPT-5 model with state-of-the-art image generation capabilities. It offers major improvements in reasoning, code quality, and user experience while incorporating GPT Image 1's superior instruction following, text rendering, and detailed image editing.	400000	{"modality": "text+image->text+image", "tokenizer": "GPT", "instruct_type": null, "input_modalities": ["image", "text", "file"], "output_modalities": ["image", "text"]}	{"prompt": "0.00001", "completion": "0.00001", "web_search": "0.01", "input_cache_read": "0.00000125"}	{"is_moderated": true, "context_length": 400000, "max_completion_tokens": 128000}	null	["frequency_penalty", "include_reasoning", "logit_bias", "logprobs", "max_tokens", "presence_penalty", "reasoning", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_logprobs", "top_p"]	{"top_p": null, "temperature": null, "frequency_penalty": null}	null
d45e4e8a-2a6d-4bf1-830e-31aaee5e5404	2026-01-16 11:18:55.898017+00	openai/o3-deep-research	openai/o3-deep-research-2025-06-26		OpenAI: o3 Deep Research	1760129661	o3-deep-research is OpenAI's advanced model for deep research, designed to tackle complex, multi-step research tasks.\n\nNote: This model always uses the 'web_search' tool which adds additional cost.	200000	{"modality": "text+image->text", "tokenizer": "GPT", "instruct_type": null, "input_modalities": ["image", "text", "file"], "output_modalities": ["text"]}	{"prompt": "0.00001", "completion": "0.00004", "web_search": "0.01", "input_cache_read": "0.0000025"}	{"is_moderated": true, "context_length": 200000, "max_completion_tokens": 100000}	null	["frequency_penalty", "include_reasoning", "logit_bias", "logprobs", "max_tokens", "presence_penalty", "reasoning", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_logprobs", "top_p"]	{"top_p": null, "temperature": null, "frequency_penalty": null}	null
dd8ee3c4-1833-47c8-aab1-b565c9cdf2d4	2026-01-16 11:18:55.898017+00	openai/o4-mini-deep-research	openai/o4-mini-deep-research-2025-06-26		OpenAI: o4 Mini Deep Research	1760129642	o4-mini-deep-research is OpenAI's faster, more affordable deep research model—ideal for tackling complex, multi-step research tasks.\n\nNote: This model always uses the 'web_search' tool which adds additional cost.	200000	{"modality": "text+image->text", "tokenizer": "GPT", "instruct_type": null, "input_modalities": ["file", "image", "text"], "output_modalities": ["text"]}	{"prompt": "0.000002", "completion": "0.000008", "web_search": "0.01", "input_cache_read": "0.0000005"}	{"is_moderated": true, "context_length": 200000, "max_completion_tokens": 100000}	null	["frequency_penalty", "include_reasoning", "logit_bias", "logprobs", "max_tokens", "presence_penalty", "reasoning", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_logprobs", "top_p"]	{"top_p": null, "temperature": null, "frequency_penalty": null}	null
3ca8370e-a2f5-4281-81e8-79790e63f60e	2026-01-16 11:18:55.898017+00	nvidia/llama-3.3-nemotron-super-49b-v1.5	nvidia/llama-3.3-nemotron-super-49b-v1.5	nvidia/Llama-3_3-Nemotron-Super-49B-v1_5	NVIDIA: Llama 3.3 Nemotron Super 49B V1.5	1760101395	Llama-3.3-Nemotron-Super-49B-v1.5 is a 49B-parameter, English-centric reasoning/chat model derived from Meta’s Llama-3.3-70B-Instruct with a 128K context. It’s post-trained for agentic workflows (RAG, tool calling) via SFT across math, code, science, and multi-turn chat, followed by multiple RL stages; Reward-aware Preference Optimization (RPO) for alignment, RL with Verifiable Rewards (RLVR) for step-wise reasoning, and iterative DPO to refine tool-use behavior. A distillation-driven Neural Architecture Search (“Puzzle”) replaces some attention blocks and varies FFN widths to shrink memory footprint and improve throughput, enabling single-GPU (H100/H200) deployment while preserving instruction following and CoT quality.\n\nIn internal evaluations (NeMo-Skills, up to 16 runs, temp = 0.6, top_p = 0.95), the model reports strong reasoning/coding results, e.g., MATH500 pass@1 = 97.4, AIME-2024 = 87.5, AIME-2025 = 82.71, GPQA = 71.97, LiveCodeBench (24.10–25.02) = 73.58, and MMLU-Pro (CoT) = 79.53. The model targets practical inference efficiency (high tokens/s, reduced VRAM) with Transformers/vLLM support and explicit “reasoning on/off” modes (chat-first defaults, greedy recommended when disabled). Suitable for building agents, assistants, and long-context retrieval systems where balanced accuracy-to-cost and reliable tool use matter.\n	131072	{"modality": "text->text", "tokenizer": "Llama3", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.0000001", "request": "0", "completion": "0.0000004", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 131072, "max_completion_tokens": null}	null	["frequency_penalty", "include_reasoning", "max_tokens", "min_p", "presence_penalty", "reasoning", "repetition_penalty", "response_format", "seed", "stop", "temperature", "tool_choice", "tools", "top_k", "top_p"]	null	null
74e15a05-a37b-4e08-a38f-dfd158b9edb5	2026-01-16 11:18:55.898017+00	baidu/ernie-4.5-21b-a3b-thinking	baidu/ernie-4.5-21b-a3b-thinking	baidu/ERNIE-4.5-21B-A3B-Thinking	Baidu: ERNIE 4.5 21B A3B Thinking	1760048887	ERNIE-4.5-21B-A3B-Thinking is Baidu's upgraded lightweight MoE model, refined to boost reasoning depth and quality for top-tier performance in logical puzzles, math, science, coding, text generation, and expert-level academic benchmarks.	131072	{"modality": "text->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000007", "request": "0", "completion": "0.00000028", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 131072, "max_completion_tokens": 65536}	null	["frequency_penalty", "include_reasoning", "max_tokens", "presence_penalty", "reasoning", "repetition_penalty", "seed", "stop", "temperature", "top_k", "top_p"]	{"top_p": 0.95, "temperature": 0.6, "frequency_penalty": null}	null
27df038f-2171-4a5b-bacc-85d10c090893	2026-01-16 11:18:55.898017+00	google/gemini-2.5-flash-image	google/gemini-2.5-flash-image		Google: Gemini 2.5 Flash Image (Nano Banana)	1759870431	Gemini 2.5 Flash Image, a.k.a. "Nano Banana," is now generally available. It is a state of the art image generation model with contextual understanding. It is capable of image generation, edits, and multi-turn conversations. Aspect ratios can be controlled with the [image_config API Parameter](https://openrouter.ai/docs/features/multimodal/image-generation#image-aspect-ratio-configuration)	32768	{"modality": "text+image->text+image", "tokenizer": "Gemini", "instruct_type": null, "input_modalities": ["image", "text"], "output_modalities": ["image", "text"]}	{"image": "0.001238", "prompt": "0.0000003", "request": "0", "completion": "0.0000025", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 32768, "max_completion_tokens": 32768}	null	["max_tokens", "response_format", "seed", "structured_outputs", "temperature", "top_p"]	{"top_p": null, "temperature": null, "frequency_penalty": null}	null
babb83a1-b462-4ec3-862b-666df875a60e	2026-01-16 11:18:55.898017+00	qwen/qwen3-vl-30b-a3b-thinking	qwen/qwen3-vl-30b-a3b-thinking	Qwen/Qwen3-VL-30B-A3B-Thinking	Qwen: Qwen3 VL 30B A3B Thinking	1759794479	Qwen3-VL-30B-A3B-Thinking is a multimodal model that unifies strong text generation with visual understanding for images and videos. Its Thinking variant enhances reasoning in STEM, math, and complex tasks. It excels in perception of real-world/synthetic categories, 2D/3D spatial grounding, and long-form visual comprehension, achieving competitive multimodal benchmark results. For agentic use, it handles multi-image multi-turn instructions, video timeline alignments, GUI automation, and visual coding from sketches to debugged UI. Text performance matches flagship Qwen3 models, suiting document AI, OCR, UI assistance, spatial tasks, and agent research.	131072	{"modality": "text+image->text", "tokenizer": "Qwen3", "instruct_type": null, "input_modalities": ["text", "image"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.0000002", "request": "0", "completion": "0.000001", "web_search": "0", "input_cache_read": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 131072, "max_completion_tokens": 32768}	null	["frequency_penalty", "include_reasoning", "max_tokens", "presence_penalty", "reasoning", "repetition_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_k", "top_p"]	{"top_p": 0.95, "temperature": 0.8}	null
bbad60eb-1104-40ce-880f-3f0ebfe42bd8	2026-01-16 11:18:55.898017+00	qwen/qwen3-vl-30b-a3b-instruct	qwen/qwen3-vl-30b-a3b-instruct	Qwen/Qwen3-VL-30B-A3B-Instruct	Qwen: Qwen3 VL 30B A3B Instruct	1759794476	Qwen3-VL-30B-A3B-Instruct is a multimodal model that unifies strong text generation with visual understanding for images and videos. Its Instruct variant optimizes instruction-following for general multimodal tasks. It excels in perception of real-world/synthetic categories, 2D/3D spatial grounding, and long-form visual comprehension, achieving competitive multimodal benchmark results. For agentic use, it handles multi-image multi-turn instructions, video timeline alignments, GUI automation, and visual coding from sketches to debugged UI. Text performance matches flagship Qwen3 models, suiting document AI, OCR, UI assistance, spatial tasks, and agent research.	262144	{"modality": "text+image->text", "tokenizer": "Qwen3", "instruct_type": null, "input_modalities": ["text", "image"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000015", "request": "0", "completion": "0.0000006", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 262144, "max_completion_tokens": null}	null	["frequency_penalty", "logit_bias", "logprobs", "max_tokens", "min_p", "presence_penalty", "repetition_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_k", "top_logprobs", "top_p"]	{"top_p": 0.8, "temperature": 0.7, "frequency_penalty": null}	null
32825adf-54a8-45f0-a9d1-5c771b090c86	2026-01-16 11:18:55.898017+00	openai/gpt-5-pro	openai/gpt-5-pro-2025-10-06		OpenAI: GPT-5 Pro	1759776663	GPT-5 Pro is OpenAI’s most advanced model, offering major improvements in reasoning, code quality, and user experience. It is optimized for complex tasks that require step-by-step reasoning, instruction following, and accuracy in high-stakes use cases. It supports test-time routing features and advanced prompt understanding, including user-specified intent like "think hard about this." Improvements include reductions in hallucination, sycophancy, and better performance in coding, writing, and health-related tasks.	400000	{"modality": "text+image->text", "tokenizer": "GPT", "instruct_type": null, "input_modalities": ["image", "text", "file"], "output_modalities": ["text"]}	{"prompt": "0.000015", "completion": "0.00012", "web_search": "0.01"}	{"is_moderated": true, "context_length": 400000, "max_completion_tokens": 128000}	null	["include_reasoning", "max_tokens", "reasoning", "response_format", "seed", "structured_outputs", "tool_choice", "tools"]	{"top_p": null, "temperature": null, "frequency_penalty": null}	null
44b44620-19aa-40c5-ab4f-3f0500ab8994	2026-01-16 11:18:55.898017+00	z-ai/glm-4.6	z-ai/glm-4.6		Z.AI: GLM 4.6	1759235576	Compared with GLM-4.5, this generation brings several key improvements:\n\nLonger context window: The context window has been expanded from 128K to 200K tokens, enabling the model to handle more complex agentic tasks.\nSuperior coding performance: The model achieves higher scores on code benchmarks and demonstrates better real-world performance in applications such as Claude Code、Cline、Roo Code and Kilo Code, including improvements in generating visually polished front-end pages.\nAdvanced reasoning: GLM-4.6 shows a clear improvement in reasoning performance and supports tool use during inference, leading to stronger overall capability.\nMore capable agents: GLM-4.6 exhibits stronger performance in tool using and search-based agents, and integrates more effectively within agent frameworks.\nRefined writing: Better aligns with human preferences in style and readability, and performs more naturally in role-playing scenarios.	202752	{"modality": "text->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000035", "request": "0", "completion": "0.0000015", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 202752, "max_completion_tokens": 65536}	null	["frequency_penalty", "include_reasoning", "logit_bias", "logprobs", "max_tokens", "min_p", "presence_penalty", "reasoning", "repetition_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_a", "top_k", "top_logprobs", "top_p"]	{"top_p": null, "temperature": 0.6, "frequency_penalty": null}	null
1df11cea-d86e-46f1-ae0c-541ed8c7ea71	2026-01-16 11:18:55.898017+00	z-ai/glm-4.6:exacto	z-ai/glm-4.6		Z.AI: GLM 4.6 (exacto)	1759235576	Compared with GLM-4.5, this generation brings several key improvements:\n\nLonger context window: The context window has been expanded from 128K to 200K tokens, enabling the model to handle more complex agentic tasks.\nSuperior coding performance: The model achieves higher scores on code benchmarks and demonstrates better real-world performance in applications such as Claude Code、Cline、Roo Code and Kilo Code, including improvements in generating visually polished front-end pages.\nAdvanced reasoning: GLM-4.6 shows a clear improvement in reasoning performance and supports tool use during inference, leading to stronger overall capability.\nMore capable agents: GLM-4.6 exhibits stronger performance in tool using and search-based agents, and integrates more effectively within agent frameworks.\nRefined writing: Better aligns with human preferences in style and readability, and performs more naturally in role-playing scenarios.	204800	{"modality": "text->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000044", "request": "0", "completion": "0.00000176", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 204800, "max_completion_tokens": 131072}	null	["frequency_penalty", "include_reasoning", "max_tokens", "presence_penalty", "reasoning", "repetition_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_k", "top_p"]	{"top_p": null, "temperature": 0.6, "frequency_penalty": null}	null
95b41138-7601-425f-8b5d-3c7f00d1e7d4	2026-01-16 11:18:55.898017+00	anthropic/claude-sonnet-4.5	anthropic/claude-4.5-sonnet-20250929		Anthropic: Claude Sonnet 4.5	1759161676	Claude Sonnet 4.5 is Anthropic’s most advanced Sonnet model to date, optimized for real-world agents and coding workflows. It delivers state-of-the-art performance on coding benchmarks such as SWE-bench Verified, with improvements across system design, code security, and specification adherence. The model is designed for extended autonomous operation, maintaining task continuity across sessions and providing fact-based progress tracking.\n\nSonnet 4.5 also introduces stronger agentic capabilities, including improved tool orchestration, speculative parallel execution, and more efficient context and memory management. With enhanced context tracking and awareness of token usage across tool calls, it is particularly well-suited for multi-context and long-running workflows. Use cases span software engineering, cybersecurity, financial analysis, research agents, and other domains requiring sustained reasoning and tool use.	1000000	{"modality": "text+image->text", "tokenizer": "Claude", "instruct_type": null, "input_modalities": ["text", "image", "file"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.000003", "request": "0", "completion": "0.000015", "web_search": "0", "input_cache_read": "0.0000003", "input_cache_write": "0.00000375", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 1000000, "max_completion_tokens": 64000}	null	["include_reasoning", "max_tokens", "reasoning", "response_format", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_k", "top_p"]	{"top_p": 1, "temperature": 1, "frequency_penalty": null}	null
166267a8-5486-4494-89c6-46818ae1cb4c	2026-01-16 11:18:55.898017+00	nvidia/nemotron-nano-9b-v2	nvidia/nemotron-nano-9b-v2	nvidia/NVIDIA-Nemotron-Nano-9B-v2	NVIDIA: Nemotron Nano 9B V2	1757106807	NVIDIA-Nemotron-Nano-9B-v2 is a large language model (LLM) trained from scratch by NVIDIA, and designed as a unified model for both reasoning and non-reasoning tasks. It responds to user queries and tasks by first generating a reasoning trace and then concluding with a final response. \n\nThe model's reasoning capabilities can be controlled via a system prompt. If the user prefers the model to provide its final answer without intermediate reasoning traces, it can be configured to do so.	131072	{"modality": "text->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000004", "request": "0", "completion": "0.00000016", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 131072, "max_completion_tokens": null}	null	["frequency_penalty", "include_reasoning", "logit_bias", "max_tokens", "min_p", "presence_penalty", "reasoning", "repetition_penalty", "response_format", "seed", "stop", "temperature", "tool_choice", "tools", "top_k", "top_p"]	{}	null
3da9855c-85c9-40e3-835d-9b02034b777a	2026-01-16 11:18:55.898017+00	deepseek/deepseek-v3.2-exp	deepseek/deepseek-v3.2-exp	deepseek-ai/DeepSeek-V3.2-Exp	DeepSeek: DeepSeek V3.2 Exp	1759150481	DeepSeek-V3.2-Exp is an experimental large language model released by DeepSeek as an intermediate step between V3.1 and future architectures. It introduces DeepSeek Sparse Attention (DSA), a fine-grained sparse attention mechanism designed to improve training and inference efficiency in long-context scenarios while maintaining output quality. Users can control the reasoning behaviour with the `reasoning` `enabled` boolean. [Learn more in our docs](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config)\n\nThe model was trained under conditions aligned with V3.1-Terminus to enable direct comparison. Benchmarking shows performance roughly on par with V3.1 across reasoning, coding, and agentic tool-use tasks, with minor tradeoffs and gains depending on the domain. This release focuses on validating architectural optimizations for extended context lengths rather than advancing raw task accuracy, making it primarily a research-oriented model for exploring efficient transformer designs.	163840	{"modality": "text->text", "tokenizer": "DeepSeek", "instruct_type": "deepseek-v3.1", "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000021", "request": "0", "completion": "0.00000032", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 163840, "max_completion_tokens": 65536}	null	["frequency_penalty", "include_reasoning", "max_tokens", "presence_penalty", "reasoning", "repetition_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_k", "top_p"]	{"top_p": 0.95, "temperature": 0.6, "frequency_penalty": null}	null
83903842-baae-443a-b01b-9d6a90fb059d	2026-01-16 11:18:55.898017+00	thedrummer/cydonia-24b-v4.1	thedrummer/cydonia-24b-v4.1	thedrummer/cydonia-24b-v4.1	TheDrummer: Cydonia 24B V4.1	1758931878	Uncensored and creative writing model based on Mistral Small 3.2 24B with good recall, prompt adherence, and intelligence.	131072	{"modality": "text->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.0000003", "request": "0", "completion": "0.0000005", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 131072, "max_completion_tokens": 131072}	null	["frequency_penalty", "logit_bias", "max_tokens", "presence_penalty", "repetition_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "top_k", "top_p"]	{"top_p": null, "temperature": null, "frequency_penalty": null}	null
f89d51c4-bb50-4ad7-b4ce-75081e46e991	2026-01-16 11:18:55.898017+00	relace/relace-apply-3	relace/relace-apply-3		Relace: Relace Apply 3	1758891572	Relace Apply 3 is a specialized code-patching LLM that merges AI-suggested edits straight into your source files. It can apply updates from GPT-4o, Claude, and others into your files at 10,000 tokens/sec on average.\n\nThe model requires the prompt to be in the following format: \n<instruction>{instruction}</instruction>\n<code>{initial_code}</code>\n<update>{edit_snippet}</update>\n\nZero Data Retention is enabled for Relace. Learn more about this model in their [documentation](https://docs.relace.ai/api-reference/instant-apply/apply)	256000	{"modality": "text->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000085", "request": "0", "completion": "0.00000125", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 256000, "max_completion_tokens": 128000}	null	["max_tokens", "seed", "stop"]	{"top_p": null, "temperature": null, "frequency_penalty": null}	null
ff29ba59-540a-4817-a425-4d9f4ac1c73f	2026-01-16 11:18:55.898017+00	google/gemini-2.5-flash-preview-09-2025	google/gemini-2.5-flash-preview-09-2025		Google: Gemini 2.5 Flash Preview 09-2025	1758820178	Gemini 2.5 Flash Preview September 2025 Checkpoint is Google's state-of-the-art workhorse model, specifically designed for advanced reasoning, coding, mathematics, and scientific tasks. It includes built-in "thinking" capabilities, enabling it to provide responses with greater accuracy and nuanced context handling. \n\nAdditionally, Gemini 2.5 Flash is configurable through the "max tokens for reasoning" parameter, as described in the documentation (https://openrouter.ai/docs/use-cases/reasoning-tokens#max-tokens-for-reasoning).	1048576	{"modality": "text+image->text", "tokenizer": "Gemini", "instruct_type": null, "input_modalities": ["image", "file", "text", "audio", "video"], "output_modalities": ["text"]}	{"audio": "0.000001", "image": "0.001238", "prompt": "0.0000003", "request": "0", "completion": "0.0000025", "web_search": "0", "input_cache_read": "0.000000075", "input_cache_write": "0.0000003833", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 1048576, "max_completion_tokens": 65535}	null	["include_reasoning", "max_tokens", "reasoning", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_p"]	{"top_p": null, "temperature": null, "frequency_penalty": null}	2026-02-17
c27836af-9ad2-4ecb-8db6-c72e9b6f288f	2026-01-16 11:18:55.898017+00	google/gemini-2.5-flash-lite-preview-09-2025	google/gemini-2.5-flash-lite-preview-09-2025		Google: Gemini 2.5 Flash Lite Preview 09-2025	1758819686	Gemini 2.5 Flash-Lite is a lightweight reasoning model in the Gemini 2.5 family, optimized for ultra-low latency and cost efficiency. It offers improved throughput, faster token generation, and better performance across common benchmarks compared to earlier Flash models. By default, "thinking" (i.e. multi-pass reasoning) is disabled to prioritize speed, but developers can enable it via the [Reasoning API parameter](https://openrouter.ai/docs/use-cases/reasoning-tokens) to selectively trade off cost for intelligence. 	1048576	{"modality": "text+image->text", "tokenizer": "Gemini", "instruct_type": null, "input_modalities": ["text", "image", "file", "audio", "video"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.0000001", "request": "0", "completion": "0.0000004", "web_search": "0", "input_cache_read": "0.00000001", "input_cache_write": "0.000001", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 1048576, "max_completion_tokens": 65536}	null	["include_reasoning", "max_tokens", "reasoning", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_p"]	{"top_p": null, "temperature": null, "frequency_penalty": null}	null
5277dcef-51df-4d07-9825-7f2358ffc178	2026-01-16 11:18:55.898017+00	openai/gpt-5-mini	openai/gpt-5-mini-2025-08-07		OpenAI: GPT-5 Mini	1754587407	GPT-5 Mini is a compact version of GPT-5, designed to handle lighter-weight reasoning tasks. It provides the same instruction-following and safety-tuning benefits as GPT-5, but with reduced latency and cost. GPT-5 Mini is the successor to OpenAI's o4-mini model.	400000	{"modality": "text+image->text", "tokenizer": "GPT", "instruct_type": null, "input_modalities": ["text", "image", "file"], "output_modalities": ["text"]}	{"prompt": "0.00000025", "completion": "0.000002", "web_search": "0.01", "input_cache_read": "0.000000025"}	{"is_moderated": true, "context_length": 400000, "max_completion_tokens": 128000}	null	["include_reasoning", "max_tokens", "reasoning", "response_format", "seed", "structured_outputs", "tool_choice", "tools"]	{}	null
05424f80-095d-4197-b503-3600e01c7f9d	2026-01-16 11:18:55.898017+00	qwen/qwen3-vl-235b-a22b-thinking	qwen/qwen3-vl-235b-a22b-thinking	Qwen/Qwen3-VL-235B-A22B-Thinking	Qwen: Qwen3 VL 235B A22B Thinking	1758668690	Qwen3-VL-235B-A22B Thinking is a multimodal model that unifies strong text generation with visual understanding across images and video. The Thinking model is optimized for multimodal reasoning in STEM and math. The series emphasizes robust perception (recognition of diverse real-world and synthetic categories), spatial understanding (2D/3D grounding), and long-form visual comprehension, with competitive results on public multimodal benchmarks for both perception and reasoning.\n\nBeyond analysis, Qwen3-VL supports agentic interaction and tool use: it can follow complex instructions over multi-image, multi-turn dialogues; align text to video timelines for precise temporal queries; and operate GUI elements for automation tasks. The models also enable visual coding workflows, turning sketches or mockups into code and assisting with UI debugging, while maintaining strong text-only performance comparable to the flagship Qwen3 language models. This makes Qwen3-VL suitable for production scenarios spanning document AI, multilingual OCR, software/UI assistance, spatial/embodied tasks, and research on vision-language agents.	262144	{"modality": "text+image->text", "tokenizer": "Qwen3", "instruct_type": null, "input_modalities": ["text", "image"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000045", "request": "0", "completion": "0.0000035", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 262144, "max_completion_tokens": 262144}	null	["frequency_penalty", "include_reasoning", "max_tokens", "presence_penalty", "reasoning", "repetition_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_k", "top_p"]	{"top_p": 0.95, "temperature": 0.8, "frequency_penalty": null}	null
fff1ffde-6241-4887-a598-f90f93318118	2026-01-16 11:18:55.898017+00	qwen/qwen3-vl-235b-a22b-instruct	qwen/qwen3-vl-235b-a22b-instruct	Qwen/Qwen3-VL-235B-A22B-Instruct	Qwen: Qwen3 VL 235B A22B Instruct	1758668687	Qwen3-VL-235B-A22B Instruct is an open-weight multimodal model that unifies strong text generation with visual understanding across images and video. The Instruct model targets general vision-language use (VQA, document parsing, chart/table extraction, multilingual OCR). The series emphasizes robust perception (recognition of diverse real-world and synthetic categories), spatial understanding (2D/3D grounding), and long-form visual comprehension, with competitive results on public multimodal benchmarks for both perception and reasoning.\n\nBeyond analysis, Qwen3-VL supports agentic interaction and tool use: it can follow complex instructions over multi-image, multi-turn dialogues; align text to video timelines for precise temporal queries; and operate GUI elements for automation tasks. The models also enable visual coding workflows—turning sketches or mockups into code and assisting with UI debugging—while maintaining strong text-only performance comparable to the flagship Qwen3 language models. This makes Qwen3-VL suitable for production scenarios spanning document AI, multilingual OCR, software/UI assistance, spatial/embodied tasks, and research on vision-language agents.	262144	{"modality": "text+image->text", "tokenizer": "Qwen3", "instruct_type": null, "input_modalities": ["text", "image"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.0000002", "request": "0", "completion": "0.0000012", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 262144, "max_completion_tokens": null}	null	["frequency_penalty", "logit_bias", "logprobs", "max_tokens", "min_p", "presence_penalty", "repetition_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_k", "top_logprobs", "top_p"]	{"top_p": 0.8, "temperature": 0.7, "frequency_penalty": null}	null
69a51c5a-4961-4425-867d-09540b18b095	2026-01-16 11:18:55.898017+00	openai/gpt-5-codex	openai/gpt-5-codex		OpenAI: GPT-5 Codex	1758643403	GPT-5-Codex is a specialized version of GPT-5 optimized for software engineering and coding workflows. It is designed for both interactive development sessions and long, independent execution of complex engineering tasks. The model supports building projects from scratch, feature development, debugging, large-scale refactoring, and code review. Compared to GPT-5, Codex is more steerable, adheres closely to developer instructions, and produces cleaner, higher-quality code outputs. Reasoning effort can be adjusted with the `reasoning.effort` parameter. Read the [docs here](https://openrouter.ai/docs/use-cases/reasoning-tokens#reasoning-effort-level)\n\nCodex integrates into developer environments including the CLI, IDE extensions, GitHub, and cloud tasks. It adapts reasoning effort dynamically—providing fast responses for small tasks while sustaining extended multi-hour runs for large projects. The model is trained to perform structured code reviews, catching critical flaws by reasoning over dependencies and validating behavior against tests. It also supports multimodal inputs such as images or screenshots for UI development and integrates tool use for search, dependency installation, and environment setup. Codex is intended specifically for agentic coding applications.	400000	{"modality": "text+image->text", "tokenizer": "GPT", "instruct_type": null, "input_modalities": ["text", "image"], "output_modalities": ["text"]}	{"prompt": "0.00000125", "completion": "0.00001", "input_cache_read": "0.000000125"}	{"is_moderated": true, "context_length": 400000, "max_completion_tokens": 128000}	null	["include_reasoning", "max_tokens", "reasoning", "response_format", "seed", "structured_outputs", "tool_choice", "tools"]	{"top_p": null, "temperature": null, "frequency_penalty": null}	null
fd7b7841-4f63-447b-bd1a-fd71c83c8501	2026-01-16 11:18:55.898017+00	deepseek/deepseek-v3.1-terminus:exacto	deepseek/deepseek-v3.1-terminus	deepseek-ai/DeepSeek-V3.1-Terminus	DeepSeek: DeepSeek V3.1 Terminus (exacto)	1758548275	DeepSeek-V3.1 Terminus is an update to [DeepSeek V3.1](/deepseek/deepseek-chat-v3.1) that maintains the model's original capabilities while addressing issues reported by users, including language consistency and agent capabilities, further optimizing the model's performance in coding and search agents. It is a large hybrid reasoning model (671B parameters, 37B active) that supports both thinking and non-thinking modes. It extends the DeepSeek-V3 base with a two-phase long-context training process, reaching up to 128K tokens, and uses FP8 microscaling for efficient inference. Users can control the reasoning behaviour with the `reasoning` `enabled` boolean. [Learn more in our docs](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config)\n\nThe model improves tool use, code generation, and reasoning efficiency, achieving performance comparable to DeepSeek-R1 on difficult benchmarks while responding more quickly. It supports structured tool calling, code agents, and search agents, making it suitable for research, coding, and agentic workflows. 	163840	{"modality": "text->text", "tokenizer": "DeepSeek", "instruct_type": "deepseek-v3.1", "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000021", "request": "0", "completion": "0.00000079", "web_search": "0", "input_cache_read": "0.000000168", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 163840, "max_completion_tokens": null}	null	["frequency_penalty", "include_reasoning", "max_tokens", "min_p", "presence_penalty", "reasoning", "repetition_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_k", "top_p"]	{"top_p": null, "temperature": null, "frequency_penalty": null}	null
e8b2a0b2-3d44-4196-a13c-8d28b1a8eb4e	2026-01-16 11:18:55.898017+00	deepseek/deepseek-v3.1-terminus	deepseek/deepseek-v3.1-terminus	deepseek-ai/DeepSeek-V3.1-Terminus	DeepSeek: DeepSeek V3.1 Terminus	1758548275	DeepSeek-V3.1 Terminus is an update to [DeepSeek V3.1](/deepseek/deepseek-chat-v3.1) that maintains the model's original capabilities while addressing issues reported by users, including language consistency and agent capabilities, further optimizing the model's performance in coding and search agents. It is a large hybrid reasoning model (671B parameters, 37B active) that supports both thinking and non-thinking modes. It extends the DeepSeek-V3 base with a two-phase long-context training process, reaching up to 128K tokens, and uses FP8 microscaling for efficient inference. Users can control the reasoning behaviour with the `reasoning` `enabled` boolean. [Learn more in our docs](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config)\n\nThe model improves tool use, code generation, and reasoning efficiency, achieving performance comparable to DeepSeek-R1 on difficult benchmarks while responding more quickly. It supports structured tool calling, code agents, and search agents, making it suitable for research, coding, and agentic workflows. 	163840	{"modality": "text->text", "tokenizer": "DeepSeek", "instruct_type": "deepseek-v3.1", "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000021", "request": "0", "completion": "0.00000079", "web_search": "0", "input_cache_read": "0.000000168", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 163840, "max_completion_tokens": null}	null	["frequency_penalty", "include_reasoning", "max_tokens", "min_p", "presence_penalty", "reasoning", "repetition_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_k", "top_p"]	{"top_p": null, "temperature": null, "frequency_penalty": null}	null
f3b781da-2d5d-46bb-93af-3f4892c28482	2026-01-16 11:18:55.898017+00	x-ai/grok-4-fast	x-ai/grok-4-fast		xAI: Grok 4 Fast	1758240090	Grok 4 Fast is xAI's latest multimodal model with SOTA cost-efficiency and a 2M token context window. It comes in two flavors: non-reasoning and reasoning. Read more about the model on xAI's [news post](http://x.ai/news/grok-4-fast).\n\nReasoning can be enabled/disabled using the `reasoning` `enabled` parameter in the API. [Learn more in our docs](https://openrouter.ai/docs/use-cases/reasoning-tokens#controlling-reasoning-tokens)	2000000	{"modality": "text+image->text", "tokenizer": "Grok", "instruct_type": null, "input_modalities": ["text", "image"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.0000002", "request": "0", "completion": "0.0000005", "web_search": "0", "input_cache_read": "0.00000005", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 2000000, "max_completion_tokens": 30000}	null	["include_reasoning", "logprobs", "max_tokens", "reasoning", "response_format", "seed", "structured_outputs", "temperature", "tool_choice", "tools", "top_logprobs", "top_p"]	{"top_p": null, "temperature": null, "frequency_penalty": null}	null
ae1116a6-6579-451f-936e-f84e8e5e4dec	2026-01-16 11:18:55.898017+00	alibaba/tongyi-deepresearch-30b-a3b	alibaba/tongyi-deepresearch-30b-a3b	Alibaba-NLP/Tongyi-DeepResearch-30B-A3B	Tongyi DeepResearch 30B A3B	1758210804	Tongyi DeepResearch is an agentic large language model developed by Tongyi Lab, with 30 billion total parameters activating only 3 billion per token. It's optimized for long-horizon, deep information-seeking tasks and delivers state-of-the-art performance on benchmarks like Humanity's Last Exam, BrowserComp, BrowserComp-ZH, WebWalkerQA, GAIA, xbench-DeepSearch, and FRAMES. This makes it superior for complex agentic search, reasoning, and multi-step problem-solving compared to prior models.\n\nThe model includes a fully automated synthetic data pipeline for scalable pre-training, fine-tuning, and reinforcement learning. It uses large-scale continual pre-training on diverse agentic data to boost reasoning and stay fresh. It also features end-to-end on-policy RL with a customized Group Relative Policy Optimization, including token-level gradients and negative sample filtering for stable training. The model supports ReAct for core ability checks and an IterResearch-based 'Heavy' mode for max performance through test-time scaling. It's ideal for advanced research agents, tool use, and heavy inference workflows.	131072	{"modality": "text->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000009", "request": "0", "completion": "0.0000004", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 131072, "max_completion_tokens": 131072}	null	["frequency_penalty", "include_reasoning", "max_tokens", "min_p", "presence_penalty", "reasoning", "repetition_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_k", "top_p"]	{"top_p": null, "temperature": null, "frequency_penalty": null}	null
73bd51a0-29f5-43e9-8991-81cf2a48b6ac	2026-01-16 11:18:55.898017+00	opengvlab/internvl3-78b	opengvlab/internvl3-78b	OpenGVLab/InternVL3-78B	OpenGVLab: InternVL3 78B	1757962555	The InternVL3 series is an advanced multimodal large language model (MLLM). Compared to InternVL 2.5, InternVL3 demonstrates stronger multimodal perception and reasoning capabilities. \n\nIn addition, InternVL3 is benchmarked against the Qwen2.5 Chat models, whose pre-trained base models serve as the initialization for its language component. Benefiting from Native Multimodal Pre-Training, the InternVL3 series surpasses the Qwen2.5 series in overall text performance.	32768	{"modality": "text+image->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["image", "text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.0000001", "request": "0", "completion": "0.00000039", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 32768, "max_completion_tokens": 32768}	null	["frequency_penalty", "max_tokens", "presence_penalty", "repetition_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "top_k", "top_p"]	{}	null
2221229e-a7f6-407b-b863-bb3b9cc01a1f	2026-01-16 11:18:55.898017+00	moonshotai/kimi-k2-0905	moonshotai/kimi-k2-0905	moonshotai/Kimi-K2-Instruct-0905	MoonshotAI: Kimi K2 0905	1757021147	Kimi K2 0905 is the September update of [Kimi K2 0711](moonshotai/kimi-k2). It is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active per forward pass. It supports long-context inference up to 256k tokens, extended from the previous 128k.\n\nThis update improves agentic coding with higher accuracy and better generalization across scaffolds, and enhances frontend coding with more aesthetic and functional outputs for web, 3D, and related tasks. Kimi K2 is optimized for agentic capabilities, including advanced tool use, reasoning, and code synthesis. It excels across coding (LiveCodeBench, SWE-bench), reasoning (ZebraLogic, GPQA), and tool-use (Tau2, AceBench) benchmarks. The model is trained with a novel stack incorporating the MuonClip optimizer for stable large-scale MoE training.	262144	{"modality": "text->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000039", "request": "0", "completion": "0.0000019", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 262144, "max_completion_tokens": 262144}	null	["frequency_penalty", "logit_bias", "logprobs", "max_tokens", "min_p", "presence_penalty", "repetition_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_k", "top_logprobs", "top_p"]	{}	null
40b7b880-3611-409a-b6da-229e80eceafb	2026-01-16 11:18:55.898017+00	qwen/qwen3-next-80b-a3b-thinking	qwen/qwen3-next-80b-a3b-thinking-2509	Qwen/Qwen3-Next-80B-A3B-Thinking	Qwen: Qwen3 Next 80B A3B Thinking	1757612284	Qwen3-Next-80B-A3B-Thinking is a reasoning-first chat model in the Qwen3-Next line that outputs structured “thinking” traces by default. It’s designed for hard multi-step problems; math proofs, code synthesis/debugging, logic, and agentic planning, and reports strong results across knowledge, reasoning, coding, alignment, and multilingual evaluations. Compared with prior Qwen3 variants, it emphasizes stability under long chains of thought and efficient scaling during inference, and it is tuned to follow complex instructions while reducing repetitive or off-task behavior.\n\nThe model is suitable for agent frameworks and tool use (function calling), retrieval-heavy workflows, and standardized benchmarking where step-by-step solutions are required. It supports long, detailed completions and leverages throughput-oriented techniques (e.g., multi-token prediction) for faster generation. Note that it operates in thinking-only mode.	262144	{"modality": "text->text", "tokenizer": "Qwen3", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000015", "request": "0", "completion": "0.0000012", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 262144, "max_completion_tokens": 262144}	null	["frequency_penalty", "include_reasoning", "logit_bias", "max_tokens", "min_p", "presence_penalty", "reasoning", "repetition_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_k", "top_p"]	{"top_p": null, "temperature": null, "frequency_penalty": null}	null
35236be2-a2db-4007-b449-14c673d34478	2026-01-16 11:18:55.898017+00	qwen/qwen3-next-80b-a3b-instruct:free	qwen/qwen3-next-80b-a3b-instruct-2509	Qwen/Qwen3-Next-80B-A3B-Instruct	Qwen: Qwen3 Next 80B A3B Instruct (free)	1757612213	Qwen3-Next-80B-A3B-Instruct is an instruction-tuned chat model in the Qwen3-Next series optimized for fast, stable responses without “thinking” traces. It targets complex tasks across reasoning, code generation, knowledge QA, and multilingual use, while remaining robust on alignment and formatting. Compared with prior Qwen3 instruct variants, it focuses on higher throughput and stability on ultra-long inputs and multi-turn dialogues, making it well-suited for RAG, tool use, and agentic workflows that require consistent final answers rather than visible chain-of-thought.\n\nThe model employs scaling-efficient training and decoding to improve parameter efficiency and inference speed, and has been validated on a broad set of public benchmarks where it reaches or approaches larger Qwen3 systems in several categories while outperforming earlier mid-sized baselines. It is best used as a general assistant, code helper, and long-context task solver in production settings where deterministic, instruction-following outputs are preferred.	262144	{"modality": "text->text", "tokenizer": "Qwen3", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0", "request": "0", "completion": "0", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 262144, "max_completion_tokens": null}	null	["frequency_penalty", "max_tokens", "presence_penalty", "response_format", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_k", "top_p"]	{}	null
8ebb9076-aa07-43b2-95c1-cb04877b89f3	2026-01-16 11:18:55.898017+00	qwen/qwen3-next-80b-a3b-instruct	qwen/qwen3-next-80b-a3b-instruct-2509	Qwen/Qwen3-Next-80B-A3B-Instruct	Qwen: Qwen3 Next 80B A3B Instruct	1757612213	Qwen3-Next-80B-A3B-Instruct is an instruction-tuned chat model in the Qwen3-Next series optimized for fast, stable responses without “thinking” traces. It targets complex tasks across reasoning, code generation, knowledge QA, and multilingual use, while remaining robust on alignment and formatting. Compared with prior Qwen3 instruct variants, it focuses on higher throughput and stability on ultra-long inputs and multi-turn dialogues, making it well-suited for RAG, tool use, and agentic workflows that require consistent final answers rather than visible chain-of-thought.\n\nThe model employs scaling-efficient training and decoding to improve parameter efficiency and inference speed, and has been validated on a broad set of public benchmarks where it reaches or approaches larger Qwen3 systems in several categories while outperforming earlier mid-sized baselines. It is best used as a general assistant, code helper, and long-context task solver in production settings where deterministic, instruction-following outputs are preferred.	262144	{"modality": "text->text", "tokenizer": "Qwen3", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000009", "request": "0", "completion": "0.0000011", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 262144, "max_completion_tokens": null}	null	["frequency_penalty", "logit_bias", "max_tokens", "min_p", "presence_penalty", "repetition_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_k", "top_p"]	{}	null
9e5fb7ab-2c29-4170-a5c0-10a3f63721ee	2026-01-16 11:18:55.898017+00	meituan/longcat-flash-chat	meituan/longcat-flash-chat	meituan-longcat/LongCat-Flash-Chat	Meituan: LongCat Flash Chat	1757427658	LongCat-Flash-Chat is a large-scale Mixture-of-Experts (MoE) model with 560B total parameters, of which 18.6B–31.3B (≈27B on average) are dynamically activated per input. It introduces a shortcut-connected MoE design to reduce communication overhead and achieve high throughput while maintaining training stability through advanced scaling strategies such as hyperparameter transfer, deterministic computation, and multi-stage optimization.\n\nThis release, LongCat-Flash-Chat, is a non-thinking foundation model optimized for conversational and agentic tasks. It supports long context windows up to 128K tokens and shows competitive performance across reasoning, coding, instruction following, and domain benchmarks, with particular strengths in tool use and complex multi-step interactions.	131072	{"modality": "text->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.0000002", "request": "0", "completion": "0.0000008", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 131072, "max_completion_tokens": 131072}	null	["max_tokens", "temperature", "top_p"]	{}	null
dd1a43d3-1013-4a77-aaea-019c6053bdf4	2026-01-16 11:18:55.898017+00	nvidia/nemotron-nano-9b-v2:free	nvidia/nemotron-nano-9b-v2	nvidia/NVIDIA-Nemotron-Nano-9B-v2	NVIDIA: Nemotron Nano 9B V2 (free)	1757106807	NVIDIA-Nemotron-Nano-9B-v2 is a large language model (LLM) trained from scratch by NVIDIA, and designed as a unified model for both reasoning and non-reasoning tasks. It responds to user queries and tasks by first generating a reasoning trace and then concluding with a final response. \n\nThe model's reasoning capabilities can be controlled via a system prompt. If the user prefers the model to provide its final answer without intermediate reasoning traces, it can be configured to do so.	128000	{"modality": "text->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0", "request": "0", "completion": "0", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 128000, "max_completion_tokens": null}	null	["include_reasoning", "max_tokens", "reasoning", "response_format", "seed", "structured_outputs", "temperature", "tool_choice", "tools", "top_p"]	{}	null
6504338c-5b55-4aa7-9321-c09492cddbcc	2026-01-16 11:18:55.898017+00	moonshotai/kimi-k2-0905:exacto	moonshotai/kimi-k2-0905	moonshotai/Kimi-K2-Instruct-0905	MoonshotAI: Kimi K2 0905 (exacto)	1757021147	Kimi K2 0905 is the September update of [Kimi K2 0711](moonshotai/kimi-k2). It is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active per forward pass. It supports long-context inference up to 256k tokens, extended from the previous 128k.\n\nThis update improves agentic coding with higher accuracy and better generalization across scaffolds, and enhances frontend coding with more aesthetic and functional outputs for web, 3D, and related tasks. Kimi K2 is optimized for agentic capabilities, including advanced tool use, reasoning, and code synthesis. It excels across coding (LiveCodeBench, SWE-bench), reasoning (ZebraLogic, GPQA), and tool-use (Tau2, AceBench) benchmarks. The model is trained with a novel stack incorporating the MuonClip optimizer for stable large-scale MoE training.	262144	{"modality": "text->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.0000006", "request": "0", "completion": "0.0000025", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 262144, "max_completion_tokens": null}	null	["frequency_penalty", "max_tokens", "presence_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_p"]	{}	null
d7f16246-4d02-4bd3-bb93-22c8d8647ad4	2026-01-16 11:18:55.898017+00	deepcogito/cogito-v2-preview-llama-70b	deepcogito/cogito-v2-preview-llama-70b	deepcogito/cogito-v2-preview-llama-70B	Deep Cogito: Cogito V2 Preview Llama 70B	1756831784	Cogito v2 70B is a dense hybrid reasoning model that combines direct answering capabilities with advanced self-reflection. Built with iterative policy improvement, it delivers strong performance across reasoning tasks while maintaining efficiency through shorter reasoning chains and improved intuition.	32768	{"modality": "text->text", "tokenizer": "Llama3", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000088", "request": "0", "completion": "0.00000088", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 32768, "max_completion_tokens": null}	null	["frequency_penalty", "include_reasoning", "logit_bias", "max_tokens", "min_p", "presence_penalty", "reasoning", "repetition_penalty", "response_format", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_k", "top_p"]	{"top_p": null, "temperature": null, "frequency_penalty": null}	null
c50108ac-880b-4976-b9a4-8ac91f9ecfa8	2026-01-16 11:18:55.898017+00	deepcogito/cogito-v2-preview-llama-109b-moe	deepcogito/cogito-v2-preview-llama-109b-moe	deepcogito/cogito-v2-preview-llama-109B-MoE	Cogito V2 Preview Llama 109B	1756831568	An instruction-tuned, hybrid-reasoning Mixture-of-Experts model built on Llama-4-Scout-17B-16E. Cogito v2 can answer directly or engage an extended “thinking” phase, with alignment guided by Iterated Distillation & Amplification (IDA). It targets coding, STEM, instruction following, and general helpfulness, with stronger multilingual, tool-calling, and reasoning performance than size-equivalent baselines. The model supports long-context use (up to 10M tokens) and standard Transformers workflows. Users can control the reasoning behaviour with the `reasoning` `enabled` boolean. [Learn more in our docs](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config)	32767	{"modality": "text+image->text", "tokenizer": "Llama4", "instruct_type": null, "input_modalities": ["image", "text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000018", "request": "0", "completion": "0.00000059", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 32767, "max_completion_tokens": null}	null	["frequency_penalty", "include_reasoning", "logit_bias", "max_tokens", "min_p", "presence_penalty", "reasoning", "repetition_penalty", "stop", "temperature", "tool_choice", "tools", "top_k", "top_p"]	{}	null
7a72b285-dfb0-454f-9470-81786468845f	2026-01-16 11:18:55.898017+00	stepfun-ai/step3	stepfun-ai/step3	stepfun-ai/step3	StepFun: Step3	1756415375	Step3 is a cutting-edge multimodal reasoning model—built on a Mixture-of-Experts architecture with 321B total parameters and 38B active. It is designed end-to-end to minimize decoding costs while delivering top-tier performance in vision–language reasoning. Through the co-design of Multi-Matrix Factorization Attention (MFA) and Attention-FFN Disaggregation (AFD), Step3 maintains exceptional efficiency across both flagship and low-end accelerators.	65536	{"modality": "text+image->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["image", "text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000057", "request": "0", "completion": "0.00000142", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 65536, "max_completion_tokens": 65536}	null	["frequency_penalty", "include_reasoning", "reasoning", "response_format", "structured_outputs", "temperature", "tool_choice", "tools", "top_k", "top_p"]	{}	null
b5078546-ed09-4063-9f94-5c06a5354bfc	2026-01-16 11:18:55.898017+00	qwen/qwen3-30b-a3b-thinking-2507	qwen/qwen3-30b-a3b-thinking-2507	Qwen/Qwen3-30B-A3B-Thinking-2507	Qwen: Qwen3 30B A3B Thinking 2507	1756399192	Qwen3-30B-A3B-Thinking-2507 is a 30B parameter Mixture-of-Experts reasoning model optimized for complex tasks requiring extended multi-step thinking. The model is designed specifically for “thinking mode,” where internal reasoning traces are separated from final answers.\n\nCompared to earlier Qwen3-30B releases, this version improves performance across logical reasoning, mathematics, science, coding, and multilingual benchmarks. It also demonstrates stronger instruction following, tool use, and alignment with human preferences. With higher reasoning efficiency and extended output budgets, it is best suited for advanced research, competitive problem solving, and agentic applications requiring structured long-context reasoning.	32768	{"modality": "text->text", "tokenizer": "Qwen3", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.000000051", "request": "0", "completion": "0.00000034", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 32768, "max_completion_tokens": null}	null	["frequency_penalty", "include_reasoning", "max_tokens", "presence_penalty", "reasoning", "repetition_penalty", "response_format", "seed", "structured_outputs", "temperature", "tool_choice", "tools", "top_k", "top_p"]	{}	null
cd587e85-4ed5-432d-acb9-f1ddacf5eb05	2026-01-16 11:18:55.898017+00	x-ai/grok-code-fast-1	x-ai/grok-code-fast-1		xAI: Grok Code Fast 1	1756238927	Grok Code Fast 1 is a speedy and economical reasoning model that excels at agentic coding. With reasoning traces visible in the response, developers can steer Grok Code for high-quality work flows.	256000	{"modality": "text->text", "tokenizer": "Grok", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.0000002", "request": "0", "completion": "0.0000015", "web_search": "0", "input_cache_read": "0.00000002", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 256000, "max_completion_tokens": 10000}	null	["include_reasoning", "logprobs", "max_tokens", "reasoning", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_logprobs", "top_p"]	{}	null
1b85eaee-8231-426b-9a2a-57765e12fad3	2026-01-16 11:18:55.898017+00	nousresearch/hermes-4-70b	nousresearch/hermes-4-70b	NousResearch/Hermes-4-70B	Nous: Hermes 4 70B	1756236182	Hermes 4 70B is a hybrid reasoning model from Nous Research, built on Meta-Llama-3.1-70B. It introduces the same hybrid mode as the larger 405B release, allowing the model to either respond directly or generate explicit <think>...</think> reasoning traces before answering. Users can control the reasoning behaviour with the `reasoning` `enabled` boolean. [Learn more in our docs](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config)\n\nThis 70B variant is trained with the expanded post-training corpus (~60B tokens) emphasizing verified reasoning data, leading to improvements in mathematics, coding, STEM, logic, and structured outputs while maintaining general assistant performance. It supports JSON mode, schema adherence, function calling, and tool use, and is designed for greater steerability with reduced refusal rates.	131072	{"modality": "text->text", "tokenizer": "Llama3", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000011", "request": "0", "completion": "0.00000038", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 131072, "max_completion_tokens": 131072}	null	["frequency_penalty", "include_reasoning", "max_tokens", "presence_penalty", "reasoning", "repetition_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_k", "top_p"]	{}	null
de0c8213-d5dd-4ae3-bdae-ac559c13010a	2026-01-16 11:18:55.898017+00	nousresearch/hermes-4-405b	nousresearch/hermes-4-405b	NousResearch/Hermes-4-405B	Nous: Hermes 4 405B	1756235463	Hermes 4 is a large-scale reasoning model built on Meta-Llama-3.1-405B and released by Nous Research. It introduces a hybrid reasoning mode, where the model can choose to deliberate internally with <think>...</think> traces or respond directly, offering flexibility between speed and depth. Users can control the reasoning behaviour with the `reasoning` `enabled` boolean. [Learn more in our docs](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config)\n\nThe model is instruction-tuned with an expanded post-training corpus (~60B tokens) emphasizing reasoning traces, improving performance in math, code, STEM, and logical reasoning, while retaining broad assistant utility. It also supports structured outputs, including JSON mode, schema adherence, function calling, and tool use. Hermes 4 is trained for steerability, lower refusal rates, and alignment toward neutral, user-directed behavior.	131072	{"modality": "text->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.000001", "request": "0", "completion": "0.000003", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 131072, "max_completion_tokens": null}	null	["frequency_penalty", "include_reasoning", "max_tokens", "presence_penalty", "reasoning", "repetition_penalty", "response_format", "temperature", "top_k", "top_p"]	{}	null
b176b391-ad1b-4a53-8ecd-743e7044be20	2026-01-16 11:18:55.898017+00	deepseek/deepseek-chat-v3.1	deepseek/deepseek-chat-v3.1	deepseek-ai/DeepSeek-V3.1	DeepSeek: DeepSeek V3.1	1755779628	DeepSeek-V3.1 is a large hybrid reasoning model (671B parameters, 37B active) that supports both thinking and non-thinking modes via prompt templates. It extends the DeepSeek-V3 base with a two-phase long-context training process, reaching up to 128K tokens, and uses FP8 microscaling for efficient inference. Users can control the reasoning behaviour with the `reasoning` `enabled` boolean. [Learn more in our docs](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config)\n\nThe model improves tool use, code generation, and reasoning efficiency, achieving performance comparable to DeepSeek-R1 on difficult benchmarks while responding more quickly. It supports structured tool calling, code agents, and search agents, making it suitable for research, coding, and agentic workflows. \n\nIt succeeds the [DeepSeek V3-0324](/deepseek/deepseek-chat-v3-0324) model and performs well on a variety of tasks.	32768	{"modality": "text->text", "tokenizer": "DeepSeek", "instruct_type": "deepseek-v3.1", "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000015", "request": "0", "completion": "0.00000075", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 32768, "max_completion_tokens": 7168}	null	["frequency_penalty", "include_reasoning", "logit_bias", "logprobs", "max_tokens", "min_p", "presence_penalty", "reasoning", "repetition_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_k", "top_logprobs", "top_p"]	{}	null
a7fb0be9-48b5-49f8-bd11-81235672e78f	2026-01-16 11:18:55.898017+00	openai/gpt-4o-audio-preview	openai/gpt-4o-audio-preview		OpenAI: GPT-4o Audio	1755233061	The gpt-4o-audio-preview model adds support for audio inputs as prompts. This enhancement allows the model to detect nuances within audio recordings and add depth to generated user experiences. Audio outputs are currently not supported. Audio tokens are priced at $40 per million input audio tokens.	128000	{"modality": "text->text", "tokenizer": "GPT", "instruct_type": null, "input_modalities": ["audio", "text"], "output_modalities": ["text"]}	{"audio": "0.00004", "prompt": "0.0000025", "completion": "0.00001"}	{"is_moderated": true, "context_length": 128000, "max_completion_tokens": 16384}	null	["frequency_penalty", "logit_bias", "logprobs", "max_tokens", "presence_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_logprobs", "top_p"]	{}	null
34886165-8787-4758-893c-0beeef07866d	2026-01-16 11:18:55.898017+00	mistralai/mistral-medium-3.1	mistralai/mistral-medium-3.1		Mistral: Mistral Medium 3.1	1755095639	Mistral Medium 3.1 is an updated version of Mistral Medium 3, which is a high-performance enterprise-grade language model designed to deliver frontier-level capabilities at significantly reduced operational cost. It balances state-of-the-art reasoning and multimodal performance with 8× lower cost compared to traditional large models, making it suitable for scalable deployments across professional and industrial use cases.\n\nThe model excels in domains such as coding, STEM reasoning, and enterprise adaptation. It supports hybrid, on-prem, and in-VPC deployments and is optimized for integration into custom workflows. Mistral Medium 3.1 offers competitive accuracy relative to larger models like Claude Sonnet 3.5/3.7, Llama 4 Maverick, and Command R+, while maintaining broad compatibility across cloud environments.	131072	{"modality": "text+image->text", "tokenizer": "Mistral", "instruct_type": null, "input_modalities": ["text", "image"], "output_modalities": ["text"]}	{"prompt": "0.0000004", "completion": "0.000002"}	{"is_moderated": false, "context_length": 131072, "max_completion_tokens": null}	null	["frequency_penalty", "max_tokens", "presence_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_p"]	{"temperature": 0.3}	null
96e67654-d834-4f40-90dd-7b63b2274401	2026-01-16 11:18:55.898017+00	baidu/ernie-4.5-21b-a3b	baidu/ernie-4.5-21b-a3b	baidu/ERNIE-4.5-21B-A3B-PT	Baidu: ERNIE 4.5 21B A3B	1755034167	A sophisticated text-based Mixture-of-Experts (MoE) model featuring 21B total parameters with 3B activated per token, delivering exceptional multimodal understanding and generation through heterogeneous MoE structures and modality-isolated routing. Supporting an extensive 131K token context length, the model achieves efficient inference via multi-expert parallel collaboration and quantization, while advanced post-training techniques including SFT, DPO, and UPO ensure optimized performance across diverse applications with specialized routing and balancing losses for superior task handling.	120000	{"modality": "text->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000007", "request": "0", "completion": "0.00000028", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 120000, "max_completion_tokens": 8000}	null	["frequency_penalty", "max_tokens", "presence_penalty", "repetition_penalty", "seed", "stop", "temperature", "tool_choice", "tools", "top_k", "top_p"]	{"top_p": 0.8, "temperature": 0.8, "frequency_penalty": null}	null
2ca30c3b-d939-4c15-8eb0-e2bb93c44f65	2026-01-16 11:18:55.898017+00	baidu/ernie-4.5-vl-28b-a3b	baidu/ernie-4.5-vl-28b-a3b	baidu/ERNIE-4.5-VL-28B-A3B-PT	Baidu: ERNIE 4.5 VL 28B A3B	1755032836	A powerful multimodal Mixture-of-Experts chat model featuring 28B total parameters with 3B activated per token, delivering exceptional text and vision understanding through its innovative heterogeneous MoE structure with modality-isolated routing. Built with scaling-efficient infrastructure for high-throughput training and inference, the model leverages advanced post-training techniques including SFT, DPO, and UPO for optimized performance, while supporting an impressive 131K context length and RLVR alignment for superior cross-modal reasoning and generation capabilities.	30000	{"modality": "text+image->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["text", "image"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000014", "request": "0", "completion": "0.00000056", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 30000, "max_completion_tokens": 8000}	null	["frequency_penalty", "include_reasoning", "max_tokens", "presence_penalty", "reasoning", "repetition_penalty", "seed", "stop", "temperature", "tool_choice", "tools", "top_k", "top_p"]	{}	null
8b582565-4d6b-40e8-94c5-1c85e2fbf9c7	2026-01-16 11:18:55.898017+00	z-ai/glm-4.5v	z-ai/glm-4.5v	zai-org/GLM-4.5V	Z.AI: GLM 4.5V	1754922288	GLM-4.5V is a vision-language foundation model for multimodal agent applications. Built on a Mixture-of-Experts (MoE) architecture with 106B parameters and 12B activated parameters, it achieves state-of-the-art results in video understanding, image Q&A, OCR, and document parsing, with strong gains in front-end web coding, grounding, and spatial reasoning. It offers a hybrid inference mode: a "thinking mode" for deep reasoning and a "non-thinking mode" for fast responses. Reasoning behavior can be toggled via the `reasoning` `enabled` boolean. [Learn more in our docs](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config)	65536	{"modality": "text+image->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["text", "image"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.0000006", "request": "0", "completion": "0.0000018", "web_search": "0", "input_cache_read": "0.00000011", "input_cache_write": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 65536, "max_completion_tokens": 16384}	null	["frequency_penalty", "include_reasoning", "max_tokens", "presence_penalty", "reasoning", "repetition_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_k", "top_p"]	{"top_p": null, "temperature": 0.75, "frequency_penalty": null}	null
ad7900bb-8d7a-44a0-93e3-fcc98075a021	2026-01-16 11:18:55.898017+00	ai21/jamba-mini-1.7	ai21/jamba-mini-1.7	ai21labs/AI21-Jamba-Mini-1.7	AI21: Jamba Mini 1.7	1754670601	Jamba Mini 1.7 is a compact and efficient member of the Jamba open model family, incorporating key improvements in grounding and instruction-following while maintaining the benefits of the SSM-Transformer hybrid architecture and 256K context window. Despite its compact size, it delivers accurate, contextually grounded responses and improved steerability.	256000	{"modality": "text->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.0000002", "request": "0", "completion": "0.0000004", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 256000, "max_completion_tokens": 4096}	null	["max_tokens", "response_format", "stop", "temperature", "tool_choice", "tools", "top_p"]	{}	null
6c73e596-7a12-4efd-bb90-e9a3c04368b0	2026-01-16 11:18:55.898017+00	ai21/jamba-large-1.7	ai21/jamba-large-1.7	ai21labs/AI21-Jamba-Large-1.7	AI21: Jamba Large 1.7	1754669020	Jamba Large 1.7 is the latest model in the Jamba open family, offering improvements in grounding, instruction-following, and overall efficiency. Built on a hybrid SSM-Transformer architecture with a 256K context window, it delivers more accurate, contextually grounded responses and better steerability than previous versions.	256000	{"modality": "text->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.000002", "request": "0", "completion": "0.000008", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 256000, "max_completion_tokens": 4096}	null	["max_tokens", "response_format", "stop", "temperature", "tool_choice", "tools", "top_p"]	{}	null
615e48ce-1058-443c-bb5a-209f0825188b	2026-01-16 11:18:55.898017+00	openai/gpt-5-chat	openai/gpt-5-chat-2025-08-07		OpenAI: GPT-5 Chat	1754587837	GPT-5 Chat is designed for advanced, natural, multimodal, and context-aware conversations for enterprise applications.	128000	{"modality": "text+image->text", "tokenizer": "GPT", "instruct_type": null, "input_modalities": ["file", "image", "text"], "output_modalities": ["text"]}	{"prompt": "0.00000125", "completion": "0.00001", "web_search": "0.01", "input_cache_read": "0.000000125"}	{"is_moderated": true, "context_length": 128000, "max_completion_tokens": 16384}	null	["max_tokens", "response_format", "seed", "structured_outputs"]	{}	null
db09e6fc-9a80-4e86-8499-7edcda838bb1	2026-01-16 11:18:55.898017+00	openai/gpt-5	openai/gpt-5-2025-08-07		OpenAI: GPT-5	1754587413	GPT-5 is OpenAI’s most advanced model, offering major improvements in reasoning, code quality, and user experience. It is optimized for complex tasks that require step-by-step reasoning, instruction following, and accuracy in high-stakes use cases. It supports test-time routing features and advanced prompt understanding, including user-specified intent like "think hard about this." Improvements include reductions in hallucination, sycophancy, and better performance in coding, writing, and health-related tasks.	400000	{"modality": "text+image->text", "tokenizer": "GPT", "instruct_type": null, "input_modalities": ["text", "image", "file"], "output_modalities": ["text"]}	{"prompt": "0.00000125", "completion": "0.00001", "web_search": "0.01", "input_cache_read": "0.000000125"}	{"is_moderated": true, "context_length": 400000, "max_completion_tokens": 128000}	null	["include_reasoning", "max_tokens", "reasoning", "response_format", "seed", "structured_outputs", "tool_choice", "tools"]	{"top_p": null, "temperature": null, "frequency_penalty": null}	null
dd1f86da-ad6b-4033-9279-eec5c9380c70	2026-01-16 11:18:55.898017+00	openai/gpt-5-nano	openai/gpt-5-nano-2025-08-07		OpenAI: GPT-5 Nano	1754587402	GPT-5-Nano is the smallest and fastest variant in the GPT-5 system, optimized for developer tools, rapid interactions, and ultra-low latency environments. While limited in reasoning depth compared to its larger counterparts, it retains key instruction-following and safety features. It is the successor to GPT-4.1-nano and offers a lightweight option for cost-sensitive or real-time applications.	400000	{"modality": "text+image->text", "tokenizer": "GPT", "instruct_type": null, "input_modalities": ["text", "image", "file"], "output_modalities": ["text"]}	{"prompt": "0.00000005", "completion": "0.0000004", "web_search": "0.01", "input_cache_read": "0.000000005"}	{"is_moderated": true, "context_length": 400000, "max_completion_tokens": 128000}	null	["include_reasoning", "max_tokens", "reasoning", "response_format", "seed", "structured_outputs", "tool_choice", "tools"]	{}	null
03c2d1f9-0c54-4415-b1dc-384b43203e5d	2026-01-16 11:18:55.898017+00	openai/gpt-oss-120b:free	openai/gpt-oss-120b	openai/gpt-oss-120b	OpenAI: gpt-oss-120b (free)	1754414231	gpt-oss-120b is an open-weight, 117B-parameter Mixture-of-Experts (MoE) language model from OpenAI designed for high-reasoning, agentic, and general-purpose production use cases. It activates 5.1B parameters per forward pass and is optimized to run on a single H100 GPU with native MXFP4 quantization. The model supports configurable reasoning depth, full chain-of-thought access, and native tool use, including function calling, browsing, and structured output generation.	131072	{"modality": "text->text", "tokenizer": "GPT", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0", "request": "0", "completion": "0", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": true, "context_length": 131072, "max_completion_tokens": null}	null	["include_reasoning", "max_tokens", "reasoning", "seed", "stop", "temperature", "tool_choice", "tools"]	{"top_p": null, "temperature": null, "frequency_penalty": null}	null
384e0992-4234-46a0-8d32-7b98de2dcde8	2026-01-16 11:18:55.898017+00	openai/gpt-oss-120b	openai/gpt-oss-120b	openai/gpt-oss-120b	OpenAI: gpt-oss-120b	1754414231	gpt-oss-120b is an open-weight, 117B-parameter Mixture-of-Experts (MoE) language model from OpenAI designed for high-reasoning, agentic, and general-purpose production use cases. It activates 5.1B parameters per forward pass and is optimized to run on a single H100 GPU with native MXFP4 quantization. The model supports configurable reasoning depth, full chain-of-thought access, and native tool use, including function calling, browsing, and structured output generation.	131072	{"modality": "text->text", "tokenizer": "GPT", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.000000039", "request": "0", "completion": "0.00000019", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 131072, "max_completion_tokens": null}	null	["frequency_penalty", "include_reasoning", "logit_bias", "logprobs", "max_tokens", "min_p", "presence_penalty", "reasoning", "reasoning_effort", "repetition_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_k", "top_logprobs", "top_p"]	{"top_p": null, "temperature": null, "frequency_penalty": null}	null
6f6088a7-2fcf-4d71-a74b-63219a1242e8	2026-01-16 11:18:55.898017+00	openai/gpt-oss-120b:exacto	openai/gpt-oss-120b	openai/gpt-oss-120b	OpenAI: gpt-oss-120b (exacto)	1754414231	gpt-oss-120b is an open-weight, 117B-parameter Mixture-of-Experts (MoE) language model from OpenAI designed for high-reasoning, agentic, and general-purpose production use cases. It activates 5.1B parameters per forward pass and is optimized to run on a single H100 GPU with native MXFP4 quantization. The model supports configurable reasoning depth, full chain-of-thought access, and native tool use, including function calling, browsing, and structured output generation.	131072	{"modality": "text->text", "tokenizer": "GPT", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.000000039", "request": "0", "completion": "0.00000019", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 131072, "max_completion_tokens": null}	null	["frequency_penalty", "include_reasoning", "max_tokens", "min_p", "presence_penalty", "reasoning", "repetition_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_k", "top_p"]	{"top_p": null, "temperature": null, "frequency_penalty": null}	null
b41d2c7f-ce89-41b9-8460-eb4b6b10abee	2026-01-16 11:18:55.898017+00	openai/gpt-oss-20b:free	openai/gpt-oss-20b	openai/gpt-oss-20b	OpenAI: gpt-oss-20b (free)	1754414229	gpt-oss-20b is an open-weight 21B parameter model released by OpenAI under the Apache 2.0 license. It uses a Mixture-of-Experts (MoE) architecture with 3.6B active parameters per forward pass, optimized for lower-latency inference and deployability on consumer or single-GPU hardware. The model is trained in OpenAI’s Harmony response format and supports reasoning level configuration, fine-tuning, and agentic capabilities including function calling, tool use, and structured outputs.	131072	{"modality": "text->text", "tokenizer": "GPT", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0", "request": "0", "completion": "0", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": true, "context_length": 131072, "max_completion_tokens": null}	null	["include_reasoning", "max_tokens", "reasoning", "seed", "stop", "temperature", "tool_choice", "tools"]	{"top_p": null, "temperature": null, "frequency_penalty": null}	null
8c3f8c86-73bb-4052-a8d0-ee2ccf70f208	2026-01-16 11:18:55.898017+00	openai/gpt-oss-20b	openai/gpt-oss-20b	openai/gpt-oss-20b	OpenAI: gpt-oss-20b	1754414229	gpt-oss-20b is an open-weight 21B parameter model released by OpenAI under the Apache 2.0 license. It uses a Mixture-of-Experts (MoE) architecture with 3.6B active parameters per forward pass, optimized for lower-latency inference and deployability on consumer or single-GPU hardware. The model is trained in OpenAI’s Harmony response format and supports reasoning level configuration, fine-tuning, and agentic capabilities including function calling, tool use, and structured outputs.	131072	{"modality": "text->text", "tokenizer": "GPT", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000002", "request": "0", "completion": "0.0000001", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 131072, "max_completion_tokens": 131072}	null	["frequency_penalty", "include_reasoning", "logit_bias", "max_tokens", "min_p", "presence_penalty", "reasoning", "reasoning_effort", "repetition_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_k", "top_p"]	{"top_p": null, "temperature": null, "frequency_penalty": null}	null
cc35527d-923c-49a8-8dd3-7b9de5f88cf5	2026-01-16 11:18:55.898017+00	anthropic/claude-opus-4.1	anthropic/claude-4.1-opus-20250805		Anthropic: Claude Opus 4.1	1754411591	Claude Opus 4.1 is an updated version of Anthropic’s flagship model, offering improved performance in coding, reasoning, and agentic tasks. It achieves 74.5% on SWE-bench Verified and shows notable gains in multi-file code refactoring, debugging precision, and detail-oriented reasoning. The model supports extended thinking up to 64K tokens and is optimized for tasks involving research, data analysis, and tool-assisted reasoning.	200000	{"modality": "text+image->text", "tokenizer": "Claude", "instruct_type": null, "input_modalities": ["image", "text", "file"], "output_modalities": ["text"]}	{"image": "0.024", "prompt": "0.000015", "request": "0", "completion": "0.000075", "web_search": "0", "input_cache_read": "0.0000015", "input_cache_write": "0.00001875", "internal_reasoning": "0"}	{"is_moderated": true, "context_length": 200000, "max_completion_tokens": 32000}	null	["include_reasoning", "max_tokens", "reasoning", "response_format", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_k", "top_p"]	{"top_p": null, "temperature": null, "frequency_penalty": null}	null
f906b6d0-e04d-42d5-8614-63ff2348896f	2026-01-16 11:18:55.898017+00	mistralai/codestral-2508	mistralai/codestral-2508		Mistral: Codestral 2508	1754079630	Mistral's cutting-edge language model for coding released end of July 2025. Codestral specializes in low-latency, high-frequency tasks such as fill-in-the-middle (FIM), code correction and test generation.\n\n[Blog Post](https://mistral.ai/news/codestral-25-08)	256000	{"modality": "text->text", "tokenizer": "Mistral", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"prompt": "0.0000003", "completion": "0.0000009"}	{"is_moderated": false, "context_length": 256000, "max_completion_tokens": null}	null	["frequency_penalty", "max_tokens", "presence_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_p"]	{"temperature": 0.3}	null
902d51fd-6c15-47f9-bc8b-45f41dc20b7e	2026-01-16 11:18:55.898017+00	qwen/qwen3-coder-30b-a3b-instruct	qwen/qwen3-coder-30b-a3b-instruct	Qwen/Qwen3-Coder-30B-A3B-Instruct	Qwen: Qwen3 Coder 30B A3B Instruct	1753972379	Qwen3-Coder-30B-A3B-Instruct is a 30.5B parameter Mixture-of-Experts (MoE) model with 128 experts (8 active per forward pass), designed for advanced code generation, repository-scale understanding, and agentic tool use. Built on the Qwen3 architecture, it supports a native context length of 256K tokens (extendable to 1M with Yarn) and performs strongly in tasks involving function calls, browser use, and structured code completion.\n\nThis model is optimized for instruction-following without “thinking mode”, and integrates well with OpenAI-compatible tool-use formats. 	160000	{"modality": "text->text", "tokenizer": "Qwen3", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000007", "request": "0", "completion": "0.00000027", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 160000, "max_completion_tokens": 32768}	null	["frequency_penalty", "max_tokens", "presence_penalty", "repetition_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_k", "top_p"]	{}	null
7f1cb53a-7e9d-4241-9cd3-9ae6021dbee2	2026-01-16 11:18:55.898017+00	qwen/qwen3-30b-a3b-instruct-2507	qwen/qwen3-30b-a3b-instruct-2507	Qwen/Qwen3-30B-A3B-Instruct-2507	Qwen: Qwen3 30B A3B Instruct 2507	1753806965	Qwen3-30B-A3B-Instruct-2507 is a 30.5B-parameter mixture-of-experts language model from Qwen, with 3.3B active parameters per inference. It operates in non-thinking mode and is designed for high-quality instruction following, multilingual understanding, and agentic tool use. Post-trained on instruction data, it demonstrates competitive performance across reasoning (AIME, ZebraLogic), coding (MultiPL-E, LiveCodeBench), and alignment (IFEval, WritingBench) benchmarks. It outperforms its non-instruct variant on subjective and open-ended tasks while retaining strong factual and coding performance.	262144	{"modality": "text->text", "tokenizer": "Qwen3", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000008", "request": "0", "completion": "0.00000033", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 262144, "max_completion_tokens": 262144}	null	["frequency_penalty", "max_tokens", "presence_penalty", "repetition_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_k", "top_p"]	{}	null
5e2d009b-4e4b-4127-a355-2784646c69b1	2026-01-16 11:18:55.898017+00	z-ai/glm-4.5	z-ai/glm-4.5	zai-org/GLM-4.5	Z.AI: GLM 4.5	1753471347	GLM-4.5 is our latest flagship foundation model, purpose-built for agent-based applications. It leverages a Mixture-of-Experts (MoE) architecture and supports a context length of up to 128k tokens. GLM-4.5 delivers significantly enhanced capabilities in reasoning, code generation, and agent alignment. It supports a hybrid inference mode with two options, a "thinking mode" designed for complex reasoning and tool use, and a "non-thinking mode" optimized for instant responses. Users can control the reasoning behaviour with the `reasoning` `enabled` boolean. [Learn more in our docs](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config)	131072	{"modality": "text->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000035", "request": "0", "completion": "0.00000155", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 131072, "max_completion_tokens": 65536}	null	["frequency_penalty", "include_reasoning", "max_tokens", "presence_penalty", "reasoning", "repetition_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_k", "top_p"]	{"top_p": null, "temperature": 0.75, "frequency_penalty": null}	null
7584855a-bba4-4a52-b764-d0a9276befa4	2026-01-16 11:18:55.898017+00	z-ai/glm-4.5-air:free	z-ai/glm-4.5-air	zai-org/GLM-4.5-Air	Z.AI: GLM 4.5 Air (free)	1753471258	GLM-4.5-Air is the lightweight variant of our latest flagship model family, also purpose-built for agent-centric applications. Like GLM-4.5, it adopts the Mixture-of-Experts (MoE) architecture but with a more compact parameter size. GLM-4.5-Air also supports hybrid inference modes, offering a "thinking mode" for advanced reasoning and tool use, and a "non-thinking mode" for real-time interaction. Users can control the reasoning behaviour with the `reasoning` `enabled` boolean. [Learn more in our docs](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config)	131072	{"modality": "text->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0", "request": "0", "completion": "0", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 131072, "max_completion_tokens": 96000}	null	["include_reasoning", "max_tokens", "reasoning", "temperature", "tool_choice", "tools", "top_p"]	{"top_p": null, "temperature": 0.75, "frequency_penalty": null}	null
7c855861-a831-4ce5-9fba-977bd368f7f4	2026-01-16 11:18:55.898017+00	z-ai/glm-4.5-air	z-ai/glm-4.5-air	zai-org/GLM-4.5-Air	Z.AI: GLM 4.5 Air	1753471258	GLM-4.5-Air is the lightweight variant of our latest flagship model family, also purpose-built for agent-centric applications. Like GLM-4.5, it adopts the Mixture-of-Experts (MoE) architecture but with a more compact parameter size. GLM-4.5-Air also supports hybrid inference modes, offering a "thinking mode" for advanced reasoning and tool use, and a "non-thinking mode" for real-time interaction. Users can control the reasoning behaviour with the `reasoning` `enabled` boolean. [Learn more in our docs](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config)	131072	{"modality": "text->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000005", "request": "0", "completion": "0.00000022", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 131072, "max_completion_tokens": 131072}	null	["frequency_penalty", "include_reasoning", "max_tokens", "presence_penalty", "reasoning", "repetition_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_k", "top_p"]	{"top_p": null, "temperature": 0.75, "frequency_penalty": null}	null
b6a49673-1c3f-4718-817b-afe18d3fa522	2026-01-16 11:18:55.898017+00	qwen/qwen3-235b-a22b-thinking-2507	qwen/qwen3-235b-a22b-thinking-2507	Qwen/Qwen3-235B-A22B-Thinking-2507	Qwen: Qwen3 235B A22B Thinking 2507	1753449557	Qwen3-235B-A22B-Thinking-2507 is a high-performance, open-weight Mixture-of-Experts (MoE) language model optimized for complex reasoning tasks. It activates 22B of its 235B parameters per forward pass and natively supports up to 262,144 tokens of context. This "thinking-only" variant enhances structured logical reasoning, mathematics, science, and long-form generation, showing strong benchmark performance across AIME, SuperGPQA, LiveCodeBench, and MMLU-Redux. It enforces a special reasoning mode (</think>) and is designed for high-token outputs (up to 81,920 tokens) in challenging domains.\n\nThe model is instruction-tuned and excels at step-by-step reasoning, tool use, agentic workflows, and multilingual tasks. This release represents the most capable open-source variant in the Qwen3-235B series, surpassing many closed models in structured reasoning use cases.	262144	{"modality": "text->text", "tokenizer": "Qwen3", "instruct_type": "qwen3", "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000011", "request": "0", "completion": "0.0000006", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 262144, "max_completion_tokens": 262144}	null	["frequency_penalty", "include_reasoning", "logit_bias", "max_tokens", "min_p", "presence_penalty", "reasoning", "repetition_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_k", "top_p"]	{"top_p": null, "temperature": null, "frequency_penalty": null}	null
55c273ed-0445-45fb-ae18-8a81eec34198	2026-01-16 11:18:55.898017+00	z-ai/glm-4-32b	z-ai/glm-4-32b-0414		Z.AI: GLM 4 32B 	1753376617	GLM 4 32B is a cost-effective foundation language model.\n\nIt can efficiently perform complex tasks and has significantly enhanced capabilities in tool use, online search, and code-related intelligent tasks.\n\nIt is made by the same lab behind the thudm models.	128000	{"modality": "text->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.0000001", "request": "0", "completion": "0.0000001", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 128000, "max_completion_tokens": null}	null	["max_tokens", "temperature", "tool_choice", "tools", "top_p"]	{"top_p": null, "temperature": 0.75, "frequency_penalty": null}	null
bb138f6e-bdbd-49fa-b0da-27e724cfc335	2026-01-16 11:18:55.898017+00	qwen/qwen3-coder:free	qwen/qwen3-coder-480b-a35b-07-25	Qwen/Qwen3-Coder-480B-A35B-Instruct	Qwen: Qwen3 Coder 480B A35B (free)	1753230546	Qwen3-Coder-480B-A35B-Instruct is a Mixture-of-Experts (MoE) code generation model developed by the Qwen team. It is optimized for agentic coding tasks such as function calling, tool use, and long-context reasoning over repositories. The model features 480 billion total parameters, with 35 billion active per forward pass (8 out of 160 experts).\n\nPricing for the Alibaba endpoints varies by context length. Once a request is greater than 128k input tokens, the higher pricing is used.	262000	{"modality": "text->text", "tokenizer": "Qwen3", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0", "request": "0", "completion": "0", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 262000, "max_completion_tokens": 262000}	null	["frequency_penalty", "max_tokens", "presence_penalty", "seed", "stop", "temperature", "tool_choice", "tools", "top_k", "top_p"]	{}	null
ad623e72-8cd8-4d24-96dc-565c3b69aac1	2026-01-16 11:18:55.898017+00	qwen/qwen3-coder	qwen/qwen3-coder-480b-a35b-07-25	Qwen/Qwen3-Coder-480B-A35B-Instruct	Qwen: Qwen3 Coder 480B A35B	1753230546	Qwen3-Coder-480B-A35B-Instruct is a Mixture-of-Experts (MoE) code generation model developed by the Qwen team. It is optimized for agentic coding tasks such as function calling, tool use, and long-context reasoning over repositories. The model features 480 billion total parameters, with 35 billion active per forward pass (8 out of 160 experts).\n\nPricing for the Alibaba endpoints varies by context length. Once a request is greater than 128k input tokens, the higher pricing is used.	262144	{"modality": "text->text", "tokenizer": "Qwen3", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000022", "request": "0", "completion": "0.00000095", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 262144, "max_completion_tokens": 262144}	null	["frequency_penalty", "logit_bias", "logprobs", "max_tokens", "min_p", "presence_penalty", "reasoning", "repetition_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_k", "top_logprobs", "top_p"]	{}	null
e5bd39e2-a0e9-4082-bf8f-b755bcce3dd3	2026-01-16 11:18:55.898017+00	qwen/qwen3-coder:exacto	qwen/qwen3-coder-480b-a35b-07-25	Qwen/Qwen3-Coder-480B-A35B-Instruct	Qwen: Qwen3 Coder 480B A35B (exacto)	1753230546	Qwen3-Coder-480B-A35B-Instruct is a Mixture-of-Experts (MoE) code generation model developed by the Qwen team. It is optimized for agentic coding tasks such as function calling, tool use, and long-context reasoning over repositories. The model features 480 billion total parameters, with 35 billion active per forward pass (8 out of 160 experts).\n\nPricing for the Alibaba endpoints varies by context length. Once a request is greater than 128k input tokens, the higher pricing is used.	262144	{"modality": "text->text", "tokenizer": "Qwen3", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000022", "request": "0", "completion": "0.0000018", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 262144, "max_completion_tokens": 65536}	null	["frequency_penalty", "max_tokens", "presence_penalty", "reasoning", "repetition_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_k", "top_p"]	{}	null
4fd72b13-0edc-4476-9c21-618e7c824907	2026-01-16 11:18:55.898017+00	bytedance/ui-tars-1.5-7b	bytedance/ui-tars-1.5-7b	ByteDance-Seed/UI-TARS-1.5-7B	ByteDance: UI-TARS 7B 	1753205056	UI-TARS-1.5 is a multimodal vision-language agent optimized for GUI-based environments, including desktop interfaces, web browsers, mobile systems, and games. Built by ByteDance, it builds upon the UI-TARS framework with reinforcement learning-based reasoning, enabling robust action planning and execution across virtual interfaces.\n\nThis model achieves state-of-the-art results on a range of interactive and grounding benchmarks, including OSworld, WebVoyager, AndroidWorld, and ScreenSpot. It also demonstrates perfect task completion across diverse Poki games and outperforms prior models in Minecraft agent tasks. UI-TARS-1.5 supports thought decomposition during inference and shows strong scaling across variants, with the 1.5 version notably exceeding the performance of earlier 72B and 7B checkpoints.	128000	{"modality": "text+image->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["image", "text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.0000001", "request": "0", "completion": "0.0000002", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 128000, "max_completion_tokens": 2048}	null	["frequency_penalty", "logit_bias", "max_tokens", "presence_penalty", "repetition_penalty", "seed", "stop", "temperature", "top_k", "top_p"]	{}	null
9ad17f8c-e621-4e17-975e-1e6006bafde2	2026-01-16 11:18:55.898017+00	google/gemini-2.5-flash-lite	google/gemini-2.5-flash-lite		Google: Gemini 2.5 Flash Lite	1753200276	Gemini 2.5 Flash-Lite is a lightweight reasoning model in the Gemini 2.5 family, optimized for ultra-low latency and cost efficiency. It offers improved throughput, faster token generation, and better performance across common benchmarks compared to earlier Flash models. By default, "thinking" (i.e. multi-pass reasoning) is disabled to prioritize speed, but developers can enable it via the [Reasoning API parameter](https://openrouter.ai/docs/use-cases/reasoning-tokens) to selectively trade off cost for intelligence. 	1048576	{"modality": "text+image->text", "tokenizer": "Gemini", "instruct_type": null, "input_modalities": ["text", "image", "file", "audio", "video"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.0000001", "request": "0", "completion": "0.0000004", "web_search": "0", "input_cache_read": "0.00000001", "input_cache_write": "0.0000001833", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 1048576, "max_completion_tokens": 65535}	null	["include_reasoning", "max_tokens", "reasoning", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_p"]	{"top_p": null, "temperature": null, "frequency_penalty": null}	null
4fba9207-f4ab-49ef-bb90-dff0dae0203c	2026-01-16 11:18:55.898017+00	qwen/qwen3-235b-a22b-2507	qwen/qwen3-235b-a22b-07-25	Qwen/Qwen3-235B-A22B-Instruct-2507	Qwen: Qwen3 235B A22B Instruct 2507	1753119555	Qwen3-235B-A22B-Instruct-2507 is a multilingual, instruction-tuned mixture-of-experts language model based on the Qwen3-235B architecture, with 22B active parameters per forward pass. It is optimized for general-purpose text generation, including instruction following, logical reasoning, math, code, and tool usage. The model supports a native 262K context length and does not implement "thinking mode" (<think> blocks).\n\nCompared to its base variant, this version delivers significant gains in knowledge coverage, long-context reasoning, coding benchmarks, and alignment with open-ended tasks. It is particularly strong on multilingual understanding, math reasoning (e.g., AIME, HMMT), and alignment evaluations like Arena-Hard and WritingBench.	262144	{"modality": "text->text", "tokenizer": "Qwen3", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.000000071", "request": "0", "completion": "0.000000463", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 262144, "max_completion_tokens": null}	null	["frequency_penalty", "include_reasoning", "logit_bias", "logprobs", "max_tokens", "min_p", "presence_penalty", "reasoning", "reasoning_effort", "repetition_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_k", "top_logprobs", "top_p"]	{}	null
3d6b949f-c08f-4faa-81c3-4e1faf74fce1	2026-01-16 11:18:55.898017+00	switchpoint/router	switchpoint/router		Switchpoint Router	1752272899	Switchpoint AI's router instantly analyzes your request and directs it to the optimal AI from an ever-evolving library. \n\nAs the world of LLMs advances, our router gets smarter, ensuring you always benefit from the industry's newest models without changing your workflow.\n\nThis model is configured for a simple, flat rate per response here on OpenRouter. It's powered by the full routing engine from [Switchpoint AI](https://www.switchpoint.dev).	131072	{"modality": "text->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000085", "request": "0", "completion": "0.0000034", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 131072, "max_completion_tokens": null}	null	["include_reasoning", "max_tokens", "reasoning", "seed", "stop", "temperature", "top_k", "top_p"]	{}	null
fe68a2a0-32ce-495b-a902-b3f60a6e2950	2026-01-16 11:18:55.898017+00	moonshotai/kimi-k2:free	moonshotai/kimi-k2	moonshotai/Kimi-K2-Instruct	MoonshotAI: Kimi K2 0711 (free)	1752263252	Kimi K2 Instruct is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active per forward pass. It is optimized for agentic capabilities, including advanced tool use, reasoning, and code synthesis. Kimi K2 excels across a broad range of benchmarks, particularly in coding (LiveCodeBench, SWE-bench), reasoning (ZebraLogic, GPQA), and tool-use (Tau2, AceBench) tasks. It supports long-context inference up to 128K tokens and is designed with a novel training stack that includes the MuonClip optimizer for stable large-scale MoE training.	32768	{"modality": "text->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0", "request": "0", "completion": "0", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": true, "context_length": 32768, "max_completion_tokens": null}	null	["max_tokens", "seed", "stop", "temperature"]	{}	null
e1f2a1a1-10e6-40a8-8278-e5e5a28f5e9a	2026-01-16 11:18:55.898017+00	moonshotai/kimi-k2	moonshotai/kimi-k2	moonshotai/Kimi-K2-Instruct	MoonshotAI: Kimi K2 0711	1752263252	Kimi K2 Instruct is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active per forward pass. It is optimized for agentic capabilities, including advanced tool use, reasoning, and code synthesis. Kimi K2 excels across a broad range of benchmarks, particularly in coding (LiveCodeBench, SWE-bench), reasoning (ZebraLogic, GPQA), and tool-use (Tau2, AceBench) tasks. It supports long-context inference up to 128K tokens and is designed with a novel training stack that includes the MuonClip optimizer for stable large-scale MoE training.	131072	{"modality": "text->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.0000005", "request": "0", "completion": "0.0000024", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 131072, "max_completion_tokens": null}	null	["frequency_penalty", "logprobs", "max_tokens", "min_p", "presence_penalty", "repetition_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_k", "top_logprobs", "top_p"]	{}	null
79d58d41-161a-4c16-9f8b-116010a05d65	2026-01-16 11:18:55.898017+00	mistralai/devstral-medium	mistralai/devstral-medium-2507		Mistral: Devstral Medium	1752161321	Devstral Medium is a high-performance code generation and agentic reasoning model developed jointly by Mistral AI and All Hands AI. Positioned as a step up from Devstral Small, it achieves 61.6% on SWE-Bench Verified, placing it ahead of Gemini 2.5 Pro and GPT-4.1 in code-related tasks, at a fraction of the cost. It is designed for generalization across prompt styles and tool use in code agents and frameworks.\n\nDevstral Medium is available via API only (not open-weight), and supports enterprise deployment on private infrastructure, with optional fine-tuning capabilities.	131072	{"modality": "text->text", "tokenizer": "Mistral", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"prompt": "0.0000004", "completion": "0.000002"}	{"is_moderated": false, "context_length": 131072, "max_completion_tokens": null}	null	["frequency_penalty", "max_tokens", "presence_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_p"]	{"temperature": 0.3}	null
0fa2b9f7-123b-494f-88dd-c13ca843d5fc	2026-01-16 11:18:55.898017+00	mistralai/devstral-small	mistralai/devstral-small-2507	mistralai/Devstral-Small-2507	Mistral: Devstral Small 1.1	1752160751	Devstral Small 1.1 is a 24B parameter open-weight language model for software engineering agents, developed by Mistral AI in collaboration with All Hands AI. Finetuned from Mistral Small 3.1 and released under the Apache 2.0 license, it features a 128k token context window and supports both Mistral-style function calling and XML output formats.\n\nDesigned for agentic coding workflows, Devstral Small 1.1 is optimized for tasks such as codebase exploration, multi-file edits, and integration into autonomous development agents like OpenHands and Cline. It achieves 53.6% on SWE-Bench Verified, surpassing all other open models on this benchmark, while remaining lightweight enough to run on a single 4090 GPU or Apple silicon machine. The model uses a Tekken tokenizer with a 131k vocabulary and is deployable via vLLM, Transformers, Ollama, LM Studio, and other OpenAI-compatible runtimes.\n	131072	{"modality": "text->text", "tokenizer": "Mistral", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"prompt": "0.0000001", "completion": "0.0000003"}	{"is_moderated": false, "context_length": 131072, "max_completion_tokens": null}	null	["frequency_penalty", "max_tokens", "presence_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_p"]	{"temperature": 0.3}	null
54638996-1bc5-4b3d-82d9-db1c1830873a	2026-01-16 11:18:55.898017+00	cognitivecomputations/dolphin-mistral-24b-venice-edition:free	venice/uncensored	cognitivecomputations/Dolphin-Mistral-24B-Venice-Edition	Venice: Uncensored (free)	1752094966	Venice Uncensored Dolphin Mistral 24B Venice Edition is a fine-tuned variant of Mistral-Small-24B-Instruct-2501, developed by dphn.ai in collaboration with Venice.ai. This model is designed as an “uncensored” instruct-tuned LLM, preserving user control over alignment, system prompts, and behavior. Intended for advanced and unrestricted use cases, Venice Uncensored emphasizes steerability and transparent behavior, removing default safety and alignment layers typically found in mainstream assistant models.	32768	{"modality": "text->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0", "request": "0", "completion": "0", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 32768, "max_completion_tokens": null}	null	["frequency_penalty", "max_tokens", "presence_penalty", "response_format", "stop", "structured_outputs", "temperature", "top_k", "top_p"]	{}	null
6006d27d-61da-488b-bb48-d27e287c56a6	2026-01-16 11:18:55.898017+00	x-ai/grok-4	x-ai/grok-4-07-09		xAI: Grok 4	1752087689	Grok 4 is xAI's latest reasoning model with a 256k context window. It supports parallel tool calling, structured outputs, and both image and text inputs. Note that reasoning is not exposed, reasoning cannot be disabled, and the reasoning effort cannot be specified. Pricing increases once the total tokens in a given request is greater than 128k tokens. See more details on the [xAI docs](https://docs.x.ai/docs/models/grok-4-0709)	256000	{"modality": "text+image->text", "tokenizer": "Grok", "instruct_type": null, "input_modalities": ["image", "text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.000003", "request": "0", "completion": "0.000015", "web_search": "0", "input_cache_read": "0.00000075", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 256000, "max_completion_tokens": null}	null	["include_reasoning", "logprobs", "max_tokens", "reasoning", "response_format", "seed", "structured_outputs", "temperature", "tool_choice", "tools", "top_logprobs", "top_p"]	{}	null
ef7c896b-aec8-4909-a3da-cc8691c2214b	2026-01-16 11:18:55.898017+00	google/gemma-3n-e2b-it:free	google/gemma-3n-e2b-it	google/gemma-3n-E2B-it	Google: Gemma 3n 2B (free)	1752074904	Gemma 3n E2B IT is a multimodal, instruction-tuned model developed by Google DeepMind, designed to operate efficiently at an effective parameter size of 2B while leveraging a 6B architecture. Based on the MatFormer architecture, it supports nested submodels and modular composition via the Mix-and-Match framework. Gemma 3n models are optimized for low-resource deployment, offering 32K context length and strong multilingual and reasoning performance across common benchmarks. This variant is trained on a diverse corpus including code, math, web, and multimodal data.	8192	{"modality": "text->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0", "request": "0", "completion": "0", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 8192, "max_completion_tokens": 2048}	null	["frequency_penalty", "max_tokens", "presence_penalty", "response_format", "seed", "stop", "temperature", "top_p"]	{}	null
8ba0815a-2165-40a1-ba8c-2e57c1f97636	2026-01-16 11:18:55.898017+00	tencent/hunyuan-a13b-instruct	tencent/hunyuan-a13b-instruct	tencent/Hunyuan-A13B-Instruct	Tencent: Hunyuan A13B Instruct	1751987664	Hunyuan-A13B is a 13B active parameter Mixture-of-Experts (MoE) language model developed by Tencent, with a total parameter count of 80B and support for reasoning via Chain-of-Thought. It offers competitive benchmark performance across mathematics, science, coding, and multi-turn reasoning tasks, while maintaining high inference efficiency via Grouped Query Attention (GQA) and quantization support (FP8, GPTQ, etc.).	131072	{"modality": "text->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000014", "request": "0", "completion": "0.00000057", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 131072, "max_completion_tokens": 131072}	null	["frequency_penalty", "include_reasoning", "reasoning", "response_format", "structured_outputs", "temperature", "top_k", "top_p"]	{}	null
49beda06-323a-4a27-b3de-b1a1351dd981	2026-01-16 11:18:55.898017+00	tngtech/deepseek-r1t2-chimera:free	tngtech/deepseek-r1t2-chimera	tngtech/DeepSeek-TNG-R1T2-Chimera	TNG: DeepSeek R1T2 Chimera (free)	1751986985	DeepSeek-TNG-R1T2-Chimera is the second-generation Chimera model from TNG Tech. It is a 671 B-parameter mixture-of-experts text-generation model assembled from DeepSeek-AI’s R1-0528, R1, and V3-0324 checkpoints with an Assembly-of-Experts merge. The tri-parent design yields strong reasoning performance while running roughly 20 % faster than the original R1 and more than 2× faster than R1-0528 under vLLM, giving a favorable cost-to-intelligence trade-off. The checkpoint supports contexts up to 60 k tokens in standard use (tested to ~130 k) and maintains consistent <think> token behaviour, making it suitable for long-context analysis, dialogue and other open-ended generation tasks.	163840	{"modality": "text->text", "tokenizer": "DeepSeek", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0", "request": "0", "completion": "0", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 163840, "max_completion_tokens": null}	null	["frequency_penalty", "include_reasoning", "max_tokens", "presence_penalty", "reasoning", "repetition_penalty", "seed", "stop", "temperature", "top_k", "top_p"]	{}	null
b08897c3-bafa-45e1-8b18-514fdc230e3d	2026-01-16 11:18:55.898017+00	tngtech/deepseek-r1t2-chimera	tngtech/deepseek-r1t2-chimera	tngtech/DeepSeek-TNG-R1T2-Chimera	TNG: DeepSeek R1T2 Chimera	1751986985	DeepSeek-TNG-R1T2-Chimera is the second-generation Chimera model from TNG Tech. It is a 671 B-parameter mixture-of-experts text-generation model assembled from DeepSeek-AI’s R1-0528, R1, and V3-0324 checkpoints with an Assembly-of-Experts merge. The tri-parent design yields strong reasoning performance while running roughly 20 % faster than the original R1 and more than 2× faster than R1-0528 under vLLM, giving a favorable cost-to-intelligence trade-off. The checkpoint supports contexts up to 60 k tokens in standard use (tested to ~130 k) and maintains consistent <think> token behaviour, making it suitable for long-context analysis, dialogue and other open-ended generation tasks.	163840	{"modality": "text->text", "tokenizer": "DeepSeek", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000025", "request": "0", "completion": "0.00000085", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 163840, "max_completion_tokens": 163840}	null	["frequency_penalty", "include_reasoning", "max_tokens", "presence_penalty", "reasoning", "repetition_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_k", "top_p"]	{}	null
243640f2-773e-4aac-afa9-f06a91cfee0b	2026-01-16 11:18:55.898017+00	morph/morph-v3-large	morph/morph-v3-large		Morph: Morph V3 Large	1751910858	Morph's high-accuracy apply model for complex code edits. ~4,500 tokens/sec with 98% accuracy for precise code transformations.\n\nThe model requires the prompt to be in the following format: \n<instruction>{instruction}</instruction>\n<code>{initial_code}</code>\n<update>{edit_snippet}</update>\n\nZero Data Retention is enabled for Morph. Learn more about this model in their [documentation](https://docs.morphllm.com/quickstart)	262144	{"modality": "text->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.0000009", "request": "0", "completion": "0.0000019", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 262144, "max_completion_tokens": 131072}	null	["max_tokens", "stop", "temperature"]	{"top_p": null, "temperature": null, "frequency_penalty": null}	null
023ee4e9-07e5-4c7c-86db-b69bfd8a3f4f	2026-01-16 11:18:55.898017+00	morph/morph-v3-fast	morph/morph-v3-fast		Morph: Morph V3 Fast	1751910002	Morph's fastest apply model for code edits. ~10,500 tokens/sec with 96% accuracy for rapid code transformations.\n\nThe model requires the prompt to be in the following format: \n<instruction>{instruction}</instruction>\n<code>{initial_code}</code>\n<update>{edit_snippet}</update>\n\nZero Data Retention is enabled for Morph. Learn more about this model in their [documentation](https://docs.morphllm.com/quickstart)	81920	{"modality": "text->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.0000008", "request": "0", "completion": "0.0000012", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 81920, "max_completion_tokens": 38000}	null	["max_tokens", "stop", "temperature"]	{"top_p": null, "temperature": null, "frequency_penalty": null}	null
8258c6e2-d8bc-4433-a8c3-3f7efa4dc0e5	2026-01-16 11:18:55.898017+00	baidu/ernie-4.5-vl-424b-a47b	baidu/ernie-4.5-vl-424b-a47b	baidu/ERNIE-4.5-VL-424B-A47B-PT	Baidu: ERNIE 4.5 VL 424B A47B 	1751300903	ERNIE-4.5-VL-424B-A47B is a multimodal Mixture-of-Experts (MoE) model from Baidu’s ERNIE 4.5 series, featuring 424B total parameters with 47B active per token. It is trained jointly on text and image data using a heterogeneous MoE architecture and modality-isolated routing to enable high-fidelity cross-modal reasoning, image understanding, and long-context generation (up to 131k tokens). Fine-tuned with techniques like SFT, DPO, UPO, and RLVR, this model supports both “thinking” and non-thinking inference modes. Designed for vision-language tasks in English and Chinese, it is optimized for efficient scaling and can operate under 4-bit/8-bit quantization.	123000	{"modality": "text+image->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["image", "text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000042", "request": "0", "completion": "0.00000125", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 123000, "max_completion_tokens": 16000}	null	["frequency_penalty", "include_reasoning", "max_tokens", "presence_penalty", "reasoning", "repetition_penalty", "seed", "stop", "temperature", "top_k", "top_p"]	{}	null
1867f679-b5f6-41cb-9751-4d6d0a50d671	2026-01-16 11:18:55.898017+00	baidu/ernie-4.5-300b-a47b	baidu/ernie-4.5-300b-a47b	baidu/ERNIE-4.5-300B-A47B-PT	Baidu: ERNIE 4.5 300B A47B 	1751300139	ERNIE-4.5-300B-A47B is a 300B parameter Mixture-of-Experts (MoE) language model developed by Baidu as part of the ERNIE 4.5 series. It activates 47B parameters per token and supports text generation in both English and Chinese. Optimized for high-throughput inference and efficient scaling, it uses a heterogeneous MoE structure with advanced routing and quantization strategies, including FP8 and 2-bit formats. This version is fine-tuned for language-only tasks and supports reasoning, tool parameters, and extended context lengths up to 131k tokens. Suitable for general-purpose LLM applications with high reasoning and throughput demands.	123000	{"modality": "text->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000028", "request": "0", "completion": "0.0000011", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 123000, "max_completion_tokens": 12000}	null	["frequency_penalty", "max_tokens", "presence_penalty", "repetition_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "top_k", "top_p"]	{}	null
668769c7-0c21-4a8a-9941-956ed67f594a	2026-01-16 11:18:55.898017+00	inception/mercury	inception/mercury		Inception: Mercury	1750973026	Mercury is the first diffusion large language model (dLLM). Applying a breakthrough discrete diffusion approach, the model runs 5-10x faster than even speed optimized models like GPT-4.1 Nano and Claude 3.5 Haiku while matching their performance. Mercury's speed enables developers to provide responsive user experiences, including with voice agents, search interfaces, and chatbots. Read more in the [blog post]\n(https://www.inceptionlabs.ai/blog/introducing-mercury) here. 	128000	{"modality": "text->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000025", "request": "0", "completion": "0.000001", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 128000, "max_completion_tokens": 16384}	null	["frequency_penalty", "max_tokens", "presence_penalty", "response_format", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_k", "top_p"]	{"top_p": null, "temperature": 0, "frequency_penalty": null}	null
f91b0ff2-39a6-43a2-a02f-00ce94c9a767	2026-01-16 11:18:55.898017+00	mistralai/mistral-small-3.2-24b-instruct	mistralai/mistral-small-3.2-24b-instruct-2506	mistralai/Mistral-Small-3.2-24B-Instruct-2506	Mistral: Mistral Small 3.2 24B	1750443016	Mistral-Small-3.2-24B-Instruct-2506 is an updated 24B parameter model from Mistral optimized for instruction following, repetition reduction, and improved function calling. Compared to the 3.1 release, version 3.2 significantly improves accuracy on WildBench and Arena Hard, reduces infinite generations, and delivers gains in tool use and structured output tasks.\n\nIt supports image and text inputs with structured outputs, function/tool calling, and strong performance across coding (HumanEval+, MBPP), STEM (MMLU, MATH, GPQA), and vision benchmarks (ChartQA, DocVQA).	131072	{"modality": "text+image->text", "tokenizer": "Mistral", "instruct_type": null, "input_modalities": ["image", "text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000006", "request": "0", "completion": "0.00000018", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 131072, "max_completion_tokens": 131072}	null	["frequency_penalty", "logit_bias", "max_tokens", "min_p", "presence_penalty", "repetition_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_k", "top_p"]	{"temperature": 0.3}	null
fcfba897-58af-4fe5-a702-4434240ca285	2026-01-16 11:18:55.898017+00	minimax/minimax-m1	minimax/minimax-m1		MiniMax: MiniMax M1	1750200414	MiniMax-M1 is a large-scale, open-weight reasoning model designed for extended context and high-efficiency inference. It leverages a hybrid Mixture-of-Experts (MoE) architecture paired with a custom "lightning attention" mechanism, allowing it to process long sequences—up to 1 million tokens—while maintaining competitive FLOP efficiency. With 456 billion total parameters and 45.9B active per token, this variant is optimized for complex, multi-step reasoning tasks.\n\nTrained via a custom reinforcement learning pipeline (CISPO), M1 excels in long-context understanding, software engineering, agentic tool use, and mathematical reasoning. Benchmarks show strong performance across FullStackBench, SWE-bench, MATH, GPQA, and TAU-Bench, often outperforming other open models like DeepSeek R1 and Qwen3-235B.	1000000	{"modality": "text->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.0000004", "request": "0", "completion": "0.0000022", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 1000000, "max_completion_tokens": 40000}	null	["frequency_penalty", "include_reasoning", "max_tokens", "presence_penalty", "reasoning", "repetition_penalty", "seed", "stop", "temperature", "tool_choice", "tools", "top_k", "top_p"]	{}	null
d40e841f-05dc-4c58-80e4-84e2a52c5a9c	2026-01-16 11:18:55.898017+00	google/gemini-2.5-flash	google/gemini-2.5-flash		Google: Gemini 2.5 Flash	1750172488	Gemini 2.5 Flash is Google's state-of-the-art workhorse model, specifically designed for advanced reasoning, coding, mathematics, and scientific tasks. It includes built-in "thinking" capabilities, enabling it to provide responses with greater accuracy and nuanced context handling. \n\nAdditionally, Gemini 2.5 Flash is configurable through the "max tokens for reasoning" parameter, as described in the documentation (https://openrouter.ai/docs/use-cases/reasoning-tokens#max-tokens-for-reasoning).	1048576	{"modality": "text+image->text", "tokenizer": "Gemini", "instruct_type": null, "input_modalities": ["file", "image", "text", "audio", "video"], "output_modalities": ["text"]}	{"audio": "0.000001", "image": "0.001238", "prompt": "0.0000003", "request": "0", "completion": "0.0000025", "web_search": "0", "input_cache_read": "0.00000003", "input_cache_write": "0.0000003833", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 1048576, "max_completion_tokens": 65535}	null	["include_reasoning", "max_tokens", "reasoning", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_p"]	{"top_p": null, "temperature": null, "frequency_penalty": null}	null
8a17d3f7-52ae-474b-8964-c4350da983b7	2026-01-16 11:18:55.898017+00	google/gemini-2.5-pro	google/gemini-2.5-pro		Google: Gemini 2.5 Pro	1750169544	Gemini 2.5 Pro is Google’s state-of-the-art AI model designed for advanced reasoning, coding, mathematics, and scientific tasks. It employs “thinking” capabilities, enabling it to reason through responses with enhanced accuracy and nuanced context handling. Gemini 2.5 Pro achieves top-tier performance on multiple benchmarks, including first-place positioning on the LMArena leaderboard, reflecting superior human-preference alignment and complex problem-solving abilities.	1048576	{"modality": "text+image->text", "tokenizer": "Gemini", "instruct_type": null, "input_modalities": ["text", "image", "file", "audio", "video"], "output_modalities": ["text"]}	{"image": "0.00516", "prompt": "0.00000125", "request": "0", "completion": "0.00001", "web_search": "0", "input_cache_read": "0.000000125", "input_cache_write": "0.000001625", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 1048576, "max_completion_tokens": 65536}	null	["include_reasoning", "max_tokens", "reasoning", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_p"]	{"top_p": null, "temperature": null, "frequency_penalty": null}	null
9dc3bc7a-5828-4a95-8dcf-78f953ba6bd2	2026-01-16 11:18:55.898017+00	moonshotai/kimi-dev-72b	moonshotai/kimi-dev-72b	moonshotai/Kimi-Dev-72B	MoonshotAI: Kimi Dev 72B	1750115909	Kimi-Dev-72B is an open-source large language model fine-tuned for software engineering and issue resolution tasks. Based on Qwen2.5-72B, it is optimized using large-scale reinforcement learning that applies code patches in real repositories and validates them via full test suite execution—rewarding only correct, robust completions. The model achieves 60.4% on SWE-bench Verified, setting a new benchmark among open-source models for software bug fixing and code reasoning.	131072	{"modality": "text->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000029", "request": "0", "completion": "0.00000115", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 131072, "max_completion_tokens": 131072}	null	["frequency_penalty", "include_reasoning", "reasoning", "response_format", "structured_outputs", "temperature", "top_k", "top_p"]	{}	null
662659d5-e2b1-4c46-a241-384afef83324	2026-01-16 11:18:55.898017+00	openai/o3-pro	openai/o3-pro-2025-06-10		OpenAI: o3 Pro	1749598352	The o-series of models are trained with reinforcement learning to think before they answer and perform complex reasoning. The o3-pro model uses more compute to think harder and provide consistently better answers.\n\nNote that BYOK is required for this model. Set up here: https://openrouter.ai/settings/integrations	200000	{"modality": "text+image->text", "tokenizer": "GPT", "instruct_type": null, "input_modalities": ["text", "file", "image"], "output_modalities": ["text"]}	{"prompt": "0.00002", "completion": "0.00008", "web_search": "0.01"}	{"is_moderated": true, "context_length": 200000, "max_completion_tokens": 100000}	null	["include_reasoning", "max_tokens", "reasoning", "response_format", "seed", "structured_outputs", "tool_choice", "tools"]	{"top_p": null, "temperature": null, "frequency_penalty": null}	null
a8f3f79c-7079-4675-aca2-ecd7fad23174	2026-01-16 11:18:55.898017+00	x-ai/grok-3-mini	x-ai/grok-3-mini		xAI: Grok 3 Mini	1749583245	A lightweight model that thinks before responding. Fast, smart, and great for logic-based tasks that do not require deep domain knowledge. The raw thinking traces are accessible.	131072	{"modality": "text->text", "tokenizer": "Grok", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.0000003", "request": "0", "completion": "0.0000005", "web_search": "0", "input_cache_read": "0.000000075", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 131072, "max_completion_tokens": null}	null	["include_reasoning", "logprobs", "max_tokens", "reasoning", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_logprobs", "top_p"]	{"top_p": null, "temperature": null, "frequency_penalty": null}	null
4625de3b-05a5-40cb-bf56-3118cf962f32	2026-01-16 11:18:55.898017+00	x-ai/grok-3	x-ai/grok-3		xAI: Grok 3	1749582908	Grok 3 is the latest model from xAI. It's their flagship model that excels at enterprise use cases like data extraction, coding, and text summarization. Possesses deep domain knowledge in finance, healthcare, law, and science.\n\n	131072	{"modality": "text->text", "tokenizer": "Grok", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.000003", "request": "0", "completion": "0.000015", "web_search": "0", "input_cache_read": "0.00000075", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 131072, "max_completion_tokens": null}	null	["frequency_penalty", "logprobs", "max_tokens", "presence_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_logprobs", "top_p"]	{}	null
b469176c-b8fa-437b-8db3-48981d5b4737	2026-01-16 11:18:55.898017+00	google/gemini-2.5-pro-preview	google/gemini-2.5-pro-preview-06-05		Google: Gemini 2.5 Pro Preview 06-05	1749137257	Gemini 2.5 Pro is Google’s state-of-the-art AI model designed for advanced reasoning, coding, mathematics, and scientific tasks. It employs “thinking” capabilities, enabling it to reason through responses with enhanced accuracy and nuanced context handling. Gemini 2.5 Pro achieves top-tier performance on multiple benchmarks, including first-place positioning on the LMArena leaderboard, reflecting superior human-preference alignment and complex problem-solving abilities.\n	1048576	{"modality": "text+image->text", "tokenizer": "Gemini", "instruct_type": null, "input_modalities": ["file", "image", "text", "audio"], "output_modalities": ["text"]}	{"image": "0.00516", "prompt": "0.00000125", "request": "0", "completion": "0.00001", "web_search": "0", "input_cache_read": "0.00000031", "input_cache_write": "0.000001625", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 1048576, "max_completion_tokens": 65536}	null	["include_reasoning", "max_tokens", "reasoning", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_p"]	{}	null
8e57dcc5-5065-4aee-b52a-4483c17f206e	2026-01-16 11:18:55.898017+00	deepseek/deepseek-r1-0528:free	deepseek/deepseek-r1-0528	deepseek-ai/DeepSeek-R1-0528	DeepSeek: R1 0528 (free)	1748455170	May 28th update to the [original DeepSeek R1](/deepseek/deepseek-r1) Performance on par with [OpenAI o1](/openai/o1), but open-sourced and with fully open reasoning tokens. It's 671B parameters in size, with 37B active in an inference pass.\n\nFully open-source model.	163840	{"modality": "text->text", "tokenizer": "DeepSeek", "instruct_type": "deepseek-r1", "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0", "request": "0", "completion": "0", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 163840, "max_completion_tokens": null}	null	["frequency_penalty", "include_reasoning", "max_tokens", "presence_penalty", "reasoning", "repetition_penalty", "temperature"]	{"top_p": null, "temperature": null, "frequency_penalty": null}	null
c86edf1c-37f7-40b6-b99d-5551a1c2e9bf	2026-01-16 11:18:55.898017+00	deepseek/deepseek-r1-0528	deepseek/deepseek-r1-0528	deepseek-ai/DeepSeek-R1-0528	DeepSeek: R1 0528	1748455170	May 28th update to the [original DeepSeek R1](/deepseek/deepseek-r1) Performance on par with [OpenAI o1](/openai/o1), but open-sourced and with fully open reasoning tokens. It's 671B parameters in size, with 37B active in an inference pass.\n\nFully open-source model.	131072	{"modality": "text->text", "tokenizer": "DeepSeek", "instruct_type": "deepseek-r1", "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000045", "request": "0", "completion": "0.00000215", "web_search": "0", "input_cache_read": "0", "input_cache_write": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 131072, "max_completion_tokens": 32768}	null	["frequency_penalty", "include_reasoning", "logit_bias", "logprobs", "max_tokens", "min_p", "presence_penalty", "reasoning", "repetition_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_k", "top_logprobs", "top_p"]	{"top_p": null, "temperature": null, "frequency_penalty": null}	null
c510b876-0fa2-4995-ad93-8a12c74b61ef	2026-01-16 11:18:55.898017+00	anthropic/claude-opus-4	anthropic/claude-4-opus-20250522		Anthropic: Claude Opus 4	1747931245	Claude Opus 4 is benchmarked as the world’s best coding model, at time of release, bringing sustained performance on complex, long-running tasks and agent workflows. It sets new benchmarks in software engineering, achieving leading results on SWE-bench (72.5%) and Terminal-bench (43.2%). Opus 4 supports extended, agentic workflows, handling thousands of task steps continuously for hours without degradation. \n\nRead more at the [blog post here](https://www.anthropic.com/news/claude-4)	200000	{"modality": "text+image->text", "tokenizer": "Claude", "instruct_type": null, "input_modalities": ["image", "text", "file"], "output_modalities": ["text"]}	{"image": "0.024", "prompt": "0.000015", "request": "0", "completion": "0.000075", "web_search": "0", "input_cache_read": "0.0000015", "input_cache_write": "0.00001875", "internal_reasoning": "0"}	{"is_moderated": true, "context_length": 200000, "max_completion_tokens": 32000}	null	["include_reasoning", "max_tokens", "reasoning", "stop", "temperature", "tool_choice", "tools", "top_k", "top_p"]	{"top_p": null, "temperature": null, "frequency_penalty": null}	null
ec7c7a01-ea43-4237-befe-35a392367cba	2026-01-16 11:18:55.898017+00	anthropic/claude-sonnet-4	anthropic/claude-4-sonnet-20250522		Anthropic: Claude Sonnet 4	1747930371	Claude Sonnet 4 significantly enhances the capabilities of its predecessor, Sonnet 3.7, excelling in both coding and reasoning tasks with improved precision and controllability. Achieving state-of-the-art performance on SWE-bench (72.7%), Sonnet 4 balances capability and computational efficiency, making it suitable for a broad range of applications from routine coding tasks to complex software development projects. Key enhancements include improved autonomous codebase navigation, reduced error rates in agent-driven workflows, and increased reliability in following intricate instructions. Sonnet 4 is optimized for practical everyday use, providing advanced reasoning capabilities while maintaining efficiency and responsiveness in diverse internal and external scenarios.\n\nRead more at the [blog post here](https://www.anthropic.com/news/claude-4)	1000000	{"modality": "text+image->text", "tokenizer": "Claude", "instruct_type": null, "input_modalities": ["image", "text", "file"], "output_modalities": ["text"]}	{"image": "0.0048", "prompt": "0.000003", "request": "0", "completion": "0.000015", "web_search": "0", "input_cache_read": "0.0000003", "input_cache_write": "0.00000375", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 1000000, "max_completion_tokens": 64000}	null	["include_reasoning", "max_tokens", "reasoning", "stop", "temperature", "tool_choice", "tools", "top_k", "top_p"]	{"top_p": null, "temperature": null, "frequency_penalty": null}	null
539f1548-29c0-435d-b683-ee4281f2c5de	2026-01-16 11:18:55.898017+00	google/gemma-3n-e4b-it:free	google/gemma-3n-e4b-it	google/gemma-3n-E4B-it	Google: Gemma 3n 4B (free)	1747776824	Gemma 3n E4B-it is optimized for efficient execution on mobile and low-resource devices, such as phones, laptops, and tablets. It supports multimodal inputs—including text, visual data, and audio—enabling diverse tasks such as text generation, speech recognition, translation, and image analysis. Leveraging innovations like Per-Layer Embedding (PLE) caching and the MatFormer architecture, Gemma 3n dynamically manages memory usage and computational load by selectively activating model parameters, significantly reducing runtime resource requirements.\n\nThis model supports a wide linguistic range (trained in over 140 languages) and features a flexible 32K token context window. Gemma 3n can selectively load parameters, optimizing memory and computational efficiency based on the task or device capabilities, making it well-suited for privacy-focused, offline-capable applications and on-device AI solutions. [Read more in the blog post](https://developers.googleblog.com/en/introducing-gemma-3n/)	8192	{"modality": "text->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0", "request": "0", "completion": "0", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 8192, "max_completion_tokens": 2048}	null	["frequency_penalty", "max_tokens", "presence_penalty", "response_format", "seed", "stop", "temperature", "top_p"]	{}	null
9da1330c-d88f-4031-968f-4ceb8375879a	2026-01-16 11:18:55.898017+00	google/gemma-3n-e4b-it	google/gemma-3n-e4b-it	google/gemma-3n-E4B-it	Google: Gemma 3n 4B	1747776824	Gemma 3n E4B-it is optimized for efficient execution on mobile and low-resource devices, such as phones, laptops, and tablets. It supports multimodal inputs—including text, visual data, and audio—enabling diverse tasks such as text generation, speech recognition, translation, and image analysis. Leveraging innovations like Per-Layer Embedding (PLE) caching and the MatFormer architecture, Gemma 3n dynamically manages memory usage and computational load by selectively activating model parameters, significantly reducing runtime resource requirements.\n\nThis model supports a wide linguistic range (trained in over 140 languages) and features a flexible 32K token context window. Gemma 3n can selectively load parameters, optimizing memory and computational efficiency based on the task or device capabilities, making it well-suited for privacy-focused, offline-capable applications and on-device AI solutions. [Read more in the blog post](https://developers.googleblog.com/en/introducing-gemma-3n/)	32768	{"modality": "text->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000002", "request": "0", "completion": "0.00000004", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 32768, "max_completion_tokens": null}	null	["frequency_penalty", "logit_bias", "max_tokens", "min_p", "presence_penalty", "repetition_penalty", "stop", "temperature", "top_k", "top_p"]	{}	null
fb30a425-3c36-4684-b61d-b34bcd8a6097	2026-01-16 11:18:55.898017+00	nousresearch/deephermes-3-mistral-24b-preview	nousresearch/deephermes-3-mistral-24b-preview	NousResearch/DeepHermes-3-Mistral-24B-Preview	Nous: DeepHermes 3 Mistral 24B Preview	1746830904	DeepHermes 3 (Mistral 24B Preview) is an instruction-tuned language model by Nous Research based on Mistral-Small-24B, designed for chat, function calling, and advanced multi-turn reasoning. It introduces a dual-mode system that toggles between intuitive chat responses and structured “deep reasoning” mode using special system prompts. Fine-tuned via distillation from R1, it supports structured output (JSON mode) and function call syntax for agent-based applications.\n\nDeepHermes 3 supports a **reasoning toggle via system prompt**, allowing users to switch between fast, intuitive responses and deliberate, multi-step reasoning. When activated with the following specific system instruction, the model enters a *"deep thinking"* mode—generating extended chains of thought wrapped in `<think></think>` tags before delivering a final answer. \n\nSystem Prompt: You are a deep thinking AI, you may use extremely long chains of thought to deeply consider the problem and deliberate with yourself via systematic reasoning processes to help come to a correct solution prior to answering. You should enclose your thoughts and internal monologue inside <think> </think> tags, and then provide your solution or response to the problem.\n	32768	{"modality": "text->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000002", "request": "0", "completion": "0.0000001", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 32768, "max_completion_tokens": 32768}	null	["frequency_penalty", "include_reasoning", "max_tokens", "presence_penalty", "reasoning", "repetition_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_k", "top_p"]	{}	null
e7b03776-5644-4323-8dac-43135c39f079	2026-01-16 11:18:55.898017+00	mistralai/mistral-medium-3	mistralai/mistral-medium-3		Mistral: Mistral Medium 3	1746627341	Mistral Medium 3 is a high-performance enterprise-grade language model designed to deliver frontier-level capabilities at significantly reduced operational cost. It balances state-of-the-art reasoning and multimodal performance with 8× lower cost compared to traditional large models, making it suitable for scalable deployments across professional and industrial use cases.\n\nThe model excels in domains such as coding, STEM reasoning, and enterprise adaptation. It supports hybrid, on-prem, and in-VPC deployments and is optimized for integration into custom workflows. Mistral Medium 3 offers competitive accuracy relative to larger models like Claude Sonnet 3.5/3.7, Llama 4 Maverick, and Command R+, while maintaining broad compatibility across cloud environments.	131072	{"modality": "text+image->text", "tokenizer": "Mistral", "instruct_type": null, "input_modalities": ["text", "image"], "output_modalities": ["text"]}	{"prompt": "0.0000004", "completion": "0.000002"}	{"is_moderated": false, "context_length": 131072, "max_completion_tokens": null}	null	["frequency_penalty", "max_tokens", "presence_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_p"]	{"temperature": 0.3}	null
679551be-da64-4673-a5fb-78fae12f4a26	2026-01-16 11:18:55.898017+00	openai/o3	openai/o3-2025-04-16		OpenAI: o3	1744823457	o3 is a well-rounded and powerful model across domains. It sets a new standard for math, science, coding, and visual reasoning tasks. It also excels at technical writing and instruction-following. Use it to think through multi-step problems that involve analysis across text, code, and images. 	200000	{"modality": "text+image->text", "tokenizer": "GPT", "instruct_type": null, "input_modalities": ["image", "text", "file"], "output_modalities": ["text"]}	{"prompt": "0.000002", "completion": "0.000008", "web_search": "0.01", "input_cache_read": "0.0000005"}	{"is_moderated": true, "context_length": 200000, "max_completion_tokens": 100000}	null	["include_reasoning", "max_tokens", "reasoning", "response_format", "seed", "structured_outputs", "tool_choice", "tools"]	{}	null
e20e66c8-78f6-490d-8eb3-1abb4712842f	2026-01-16 11:18:55.898017+00	google/gemini-2.5-pro-preview-05-06	google/gemini-2.5-pro-preview-03-25		Google: Gemini 2.5 Pro Preview 05-06	1746578513	Gemini 2.5 Pro is Google’s state-of-the-art AI model designed for advanced reasoning, coding, mathematics, and scientific tasks. It employs “thinking” capabilities, enabling it to reason through responses with enhanced accuracy and nuanced context handling. Gemini 2.5 Pro achieves top-tier performance on multiple benchmarks, including first-place positioning on the LMArena leaderboard, reflecting superior human-preference alignment and complex problem-solving abilities.	1048576	{"modality": "text+image->text", "tokenizer": "Gemini", "instruct_type": null, "input_modalities": ["text", "image", "file", "audio", "video"], "output_modalities": ["text"]}	{"image": "0.00516", "prompt": "0.00000125", "request": "0", "completion": "0.00001", "web_search": "0", "input_cache_read": "0.00000031", "input_cache_write": "0.000001625", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 1048576, "max_completion_tokens": 65535}	null	["include_reasoning", "max_tokens", "reasoning", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_p"]	{"top_p": null, "temperature": null, "frequency_penalty": null}	null
28bfafa5-5066-4ff4-af2b-9680fdaa248b	2026-01-16 11:18:55.898017+00	arcee-ai/spotlight	arcee-ai/spotlight		Arcee AI: Spotlight	1746481552	Spotlight is a 7‑billion‑parameter vision‑language model derived from Qwen 2.5‑VL and fine‑tuned by Arcee AI for tight image‑text grounding tasks. It offers a 32 k‑token context window, enabling rich multimodal conversations that combine lengthy documents with one or more images. Training emphasized fast inference on consumer GPUs while retaining strong captioning, visual‐question‑answering, and diagram‑analysis accuracy. As a result, Spotlight slots neatly into agent workflows where screenshots, charts or UI mock‑ups need to be interpreted on the fly. Early benchmarks show it matching or out‑scoring larger VLMs such as LLaVA‑1.6 13 B on popular VQA and POPE alignment tests. 	131072	{"modality": "text+image->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["image", "text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000018", "request": "0", "completion": "0.00000018", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 131072, "max_completion_tokens": 65537}	null	["frequency_penalty", "logit_bias", "max_tokens", "min_p", "presence_penalty", "repetition_penalty", "stop", "temperature", "top_k", "top_p"]	{}	null
fa409c13-d302-4e12-a3f6-1525218a8d3c	2026-01-16 11:18:55.898017+00	arcee-ai/maestro-reasoning	arcee-ai/maestro-reasoning		Arcee AI: Maestro Reasoning	1746481269	Maestro Reasoning is Arcee's flagship analysis model: a 32 B‑parameter derivative of Qwen 2.5‑32 B tuned with DPO and chain‑of‑thought RL for step‑by‑step logic. Compared to the earlier 7 B preview, the production 32 B release widens the context window to 128 k tokens and doubles pass‑rate on MATH and GSM‑8K, while also lifting code completion accuracy. Its instruction style encourages structured "thought → answer" traces that can be parsed or hidden according to user preference. That transparency pairs well with audit‑focused industries like finance or healthcare where seeing the reasoning path matters. In Arcee Conductor, Maestro is automatically selected for complex, multi‑constraint queries that smaller SLMs bounce. 	131072	{"modality": "text->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.0000009", "request": "0", "completion": "0.0000033", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 131072, "max_completion_tokens": 32000}	null	["frequency_penalty", "logit_bias", "max_tokens", "min_p", "presence_penalty", "repetition_penalty", "stop", "temperature", "top_k", "top_p"]	{}	null
355cef99-bfdc-4639-bc79-3f92e90fe767	2026-01-16 11:18:55.898017+00	arcee-ai/virtuoso-large	arcee-ai/virtuoso-large		Arcee AI: Virtuoso Large	1746478885	Virtuoso‑Large is Arcee's top‑tier general‑purpose LLM at 72 B parameters, tuned to tackle cross‑domain reasoning, creative writing and enterprise QA. Unlike many 70 B peers, it retains the 128 k context inherited from Qwen 2.5, letting it ingest books, codebases or financial filings wholesale. Training blended DeepSeek R1 distillation, multi‑epoch supervised fine‑tuning and a final DPO/RLHF alignment stage, yielding strong performance on BIG‑Bench‑Hard, GSM‑8K and long‑context Needle‑In‑Haystack tests. Enterprises use Virtuoso‑Large as the "fallback" brain in Conductor pipelines when other SLMs flag low confidence. Despite its size, aggressive KV‑cache optimizations keep first‑token latency in the low‑second range on 8× H100 nodes, making it a practical production‑grade powerhouse.	131072	{"modality": "text->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000075", "request": "0", "completion": "0.0000012", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 131072, "max_completion_tokens": 64000}	null	["frequency_penalty", "logit_bias", "max_tokens", "min_p", "presence_penalty", "repetition_penalty", "stop", "temperature", "tool_choice", "tools", "top_k", "top_p"]	{}	null
bb2dfa21-e80b-4199-8753-8ddfb9e243d9	2026-01-16 11:18:55.898017+00	arcee-ai/coder-large	arcee-ai/coder-large		Arcee AI: Coder Large	1746478663	Coder‑Large is a 32 B‑parameter offspring of Qwen 2.5‑Instruct that has been further trained on permissively‑licensed GitHub, CodeSearchNet and synthetic bug‑fix corpora. It supports a 32k context window, enabling multi‑file refactoring or long diff review in a single call, and understands 30‑plus programming languages with special attention to TypeScript, Go and Terraform. Internal benchmarks show 5–8 pt gains over CodeLlama‑34 B‑Python on HumanEval and competitive BugFix scores thanks to a reinforcement pass that rewards compilable output. The model emits structured explanations alongside code blocks by default, making it suitable for educational tooling as well as production copilot scenarios. Cost‑wise, Together AI prices it well below proprietary incumbents, so teams can scale interactive coding without runaway spend. 	32768	{"modality": "text->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.0000005", "request": "0", "completion": "0.0000008", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 32768, "max_completion_tokens": null}	null	["frequency_penalty", "logit_bias", "max_tokens", "min_p", "presence_penalty", "repetition_penalty", "stop", "temperature", "top_k", "top_p"]	{}	null
4c0413f8-8ea1-4378-bb1d-d182319d9dda	2026-01-16 11:18:55.898017+00	thedrummer/skyfall-36b-v2	thedrummer/skyfall-36b-v2	TheDrummer/Skyfall-36B-v2	TheDrummer: Skyfall 36B V2	1741636566	Skyfall 36B v2 is an enhanced iteration of Mistral Small 2501, specifically fine-tuned for improved creativity, nuanced writing, role-playing, and coherent storytelling.	32768	{"modality": "text->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000055", "request": "0", "completion": "0.0000008", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 32768, "max_completion_tokens": 32768}	null	["frequency_penalty", "logit_bias", "max_tokens", "presence_penalty", "repetition_penalty", "seed", "stop", "temperature", "top_k", "top_p"]	{}	null
316eef40-f1ea-4337-93a9-d568d7585eb6	2026-01-16 11:18:55.898017+00	inception/mercury-coder	inception/mercury-coder-small-beta		Inception: Mercury Coder	1746033880	Mercury Coder is the first diffusion large language model (dLLM). Applying a breakthrough discrete diffusion approach, the model runs 5-10x faster than even speed optimized models like Claude 3.5 Haiku and GPT-4o Mini while matching their performance. Mercury Coder's speed means that developers can stay in the flow while coding, enjoying rapid chat-based iteration and responsive code completion suggestions. On Copilot Arena, Mercury Coder ranks 1st in speed and ties for 2nd in quality. Read more in the [blog post here](https://www.inceptionlabs.ai/blog/introducing-mercury).	128000	{"modality": "text->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000025", "request": "0", "completion": "0.000001", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 128000, "max_completion_tokens": 16384}	null	["frequency_penalty", "max_tokens", "presence_penalty", "response_format", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_k", "top_p"]	{"top_p": null, "temperature": 0, "frequency_penalty": null}	null
c7f73063-86c4-4a5c-a041-6374934979cb	2026-01-16 11:18:55.898017+00	qwen/qwen3-4b:free	qwen/qwen3-4b-04-28	Qwen/Qwen3-4B	Qwen: Qwen3 4B (free)	1746031104	Qwen3-4B is a 4 billion parameter dense language model from the Qwen3 series, designed to support both general-purpose and reasoning-intensive tasks. It introduces a dual-mode architecture—thinking and non-thinking—allowing dynamic switching between high-precision logical reasoning and efficient dialogue generation. This makes it well-suited for multi-turn chat, instruction following, and complex agent workflows.	40960	{"modality": "text->text", "tokenizer": "Qwen3", "instruct_type": "qwen3", "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0", "request": "0", "completion": "0", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 40960, "max_completion_tokens": null}	null	["frequency_penalty", "include_reasoning", "max_tokens", "presence_penalty", "reasoning", "response_format", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_k", "top_p"]	{}	null
0549ad31-7c7f-45f6-91eb-aaf04c0d1284	2026-01-16 11:18:55.898017+00	meta-llama/llama-guard-4-12b	meta-llama/llama-guard-4-12b	meta-llama/Llama-Guard-4-12B	Meta: Llama Guard 4 12B	1745975193	Llama Guard 4 is a Llama 4 Scout-derived multimodal pretrained model, fine-tuned for content safety classification. Similar to previous versions, it can be used to classify content in both LLM inputs (prompt classification) and in LLM responses (response classification). It acts as an LLM—generating text in its output that indicates whether a given prompt or response is safe or unsafe, and if unsafe, it also lists the content categories violated.\n\nLlama Guard 4 was aligned to safeguard against the standardized MLCommons hazards taxonomy and designed to support multimodal Llama 4 capabilities. Specifically, it combines features from previous Llama Guard models, providing content moderation for English and multiple supported languages, along with enhanced capabilities to handle mixed text-and-image prompts, including multiple images. Additionally, Llama Guard 4 is integrated into the Llama Moderations API, extending robust safety classification to text and images.	163840	{"modality": "text+image->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["image", "text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000018", "request": "0", "completion": "0.00000018", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 163840, "max_completion_tokens": null}	null	["frequency_penalty", "logit_bias", "max_tokens", "min_p", "presence_penalty", "repetition_penalty", "response_format", "seed", "stop", "temperature", "top_k", "top_p"]	{}	null
75b742c3-559f-4bc4-b572-98f36608e056	2026-01-16 11:18:55.898017+00	qwen/qwen3-30b-a3b	qwen/qwen3-30b-a3b-04-28	Qwen/Qwen3-30B-A3B	Qwen: Qwen3 30B A3B	1745878604	Qwen3, the latest generation in the Qwen large language model series, features both dense and mixture-of-experts (MoE) architectures to excel in reasoning, multilingual support, and advanced agent tasks. Its unique ability to switch seamlessly between a thinking mode for complex reasoning and a non-thinking mode for efficient dialogue ensures versatile, high-quality performance.\n\nSignificantly outperforming prior models like QwQ and Qwen2.5, Qwen3 delivers superior mathematics, coding, commonsense reasoning, creative writing, and interactive dialogue capabilities. The Qwen3-30B-A3B variant includes 30.5 billion parameters (3.3 billion activated), 48 layers, 128 experts (8 activated per task), and supports up to 131K token contexts with YaRN, setting a new standard among open-source models.	40960	{"modality": "text->text", "tokenizer": "Qwen3", "instruct_type": "qwen3", "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000006", "request": "0", "completion": "0.00000022", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 40960, "max_completion_tokens": 40960}	null	["frequency_penalty", "include_reasoning", "logit_bias", "logprobs", "max_tokens", "min_p", "presence_penalty", "reasoning", "repetition_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_k", "top_logprobs", "top_p"]	{"top_p": null, "temperature": null, "frequency_penalty": null}	null
85a21800-39c4-4338-b910-e769091c1e97	2026-01-16 11:18:55.898017+00	qwen/qwen3-8b	qwen/qwen3-8b-04-28	Qwen/Qwen3-8B	Qwen: Qwen3 8B	1745876632	Qwen3-8B is a dense 8.2B parameter causal language model from the Qwen3 series, designed for both reasoning-heavy tasks and efficient dialogue. It supports seamless switching between "thinking" mode for math, coding, and logical inference, and "non-thinking" mode for general conversation. The model is fine-tuned for instruction-following, agent integration, creative writing, and multilingual use across 100+ languages and dialects. It natively supports a 32K token context window and can extend to 131K tokens with YaRN scaling.	32000	{"modality": "text->text", "tokenizer": "Qwen3", "instruct_type": "qwen3", "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000005", "request": "0", "completion": "0.00000025", "web_search": "0", "input_cache_read": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 32000, "max_completion_tokens": 8192}	null	["frequency_penalty", "include_reasoning", "logit_bias", "logprobs", "max_tokens", "presence_penalty", "reasoning", "repetition_penalty", "response_format", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_k", "top_logprobs", "top_p"]	{"top_p": null, "temperature": null, "frequency_penalty": null}	null
3670715c-03ba-469c-813e-64ef9e26df94	2026-01-16 11:18:55.898017+00	thedrummer/unslopnemo-12b	thedrummer/unslopnemo-12b	TheDrummer/UnslopNemo-12B-v4.1	TheDrummer: UnslopNemo 12B	1731103448	UnslopNemo v4.1 is the latest addition from the creator of Rocinante, designed for adventure writing and role-play scenarios.	32768	{"modality": "text->text", "tokenizer": "Mistral", "instruct_type": "mistral", "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.0000004", "request": "0", "completion": "0.0000004", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 32768, "max_completion_tokens": null}	null	["frequency_penalty", "max_tokens", "presence_penalty", "response_format", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_p"]	{}	null
03910f45-6dcd-429f-9a40-47881215a4cc	2026-01-16 11:18:55.898017+00	qwen/qwen3-14b	qwen/qwen3-14b-04-28	Qwen/Qwen3-14B	Qwen: Qwen3 14B	1745876478	Qwen3-14B is a dense 14.8B parameter causal language model from the Qwen3 series, designed for both complex reasoning and efficient dialogue. It supports seamless switching between a "thinking" mode for tasks like math, programming, and logical inference, and a "non-thinking" mode for general-purpose conversation. The model is fine-tuned for instruction-following, agent tool use, creative writing, and multilingual tasks across 100+ languages and dialects. It natively handles 32K token contexts and can extend to 131K tokens using YaRN-based scaling.	40960	{"modality": "text->text", "tokenizer": "Qwen3", "instruct_type": "qwen3", "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000005", "request": "0", "completion": "0.00000022", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 40960, "max_completion_tokens": 40960}	null	["frequency_penalty", "include_reasoning", "max_tokens", "min_p", "presence_penalty", "reasoning", "repetition_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_k", "top_p"]	{}	null
24a3ec03-f740-42c5-a74d-71ad4d2d1897	2026-01-16 11:18:55.898017+00	qwen/qwen3-32b	qwen/qwen3-32b-04-28	Qwen/Qwen3-32B	Qwen: Qwen3 32B	1745875945	Qwen3-32B is a dense 32.8B parameter causal language model from the Qwen3 series, optimized for both complex reasoning and efficient dialogue. It supports seamless switching between a "thinking" mode for tasks like math, coding, and logical inference, and a "non-thinking" mode for faster, general-purpose conversation. The model demonstrates strong performance in instruction-following, agent tool use, creative writing, and multilingual tasks across 100+ languages and dialects. It natively handles 32K token contexts and can extend to 131K tokens using YaRN-based scaling. 	40960	{"modality": "text->text", "tokenizer": "Qwen3", "instruct_type": "qwen3", "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000008", "request": "0", "completion": "0.00000024", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 40960, "max_completion_tokens": 40960}	null	["frequency_penalty", "include_reasoning", "logprobs", "max_tokens", "min_p", "presence_penalty", "reasoning", "repetition_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_k", "top_logprobs", "top_p"]	{}	null
6d793f14-f748-442b-9eaa-580d71a7d97c	2026-01-16 11:18:55.898017+00	qwen/qwen3-235b-a22b	qwen/qwen3-235b-a22b-04-28	Qwen/Qwen3-235B-A22B	Qwen: Qwen3 235B A22B	1745875757	Qwen3-235B-A22B is a 235B parameter mixture-of-experts (MoE) model developed by Qwen, activating 22B parameters per forward pass. It supports seamless switching between a "thinking" mode for complex reasoning, math, and code tasks, and a "non-thinking" mode for general conversational efficiency. The model demonstrates strong reasoning ability, multilingual support (100+ languages and dialects), advanced instruction-following, and agent tool-calling capabilities. It natively handles a 32K token context window and extends up to 131K tokens using YaRN-based scaling.	40960	{"modality": "text->text", "tokenizer": "Qwen3", "instruct_type": "qwen3", "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.0000002", "request": "0", "completion": "0.0000006", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 40960, "max_completion_tokens": null}	null	["frequency_penalty", "include_reasoning", "logit_bias", "logprobs", "max_tokens", "min_p", "presence_penalty", "reasoning", "repetition_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_k", "top_logprobs", "top_p"]	{}	null
caec36c2-9586-4e9e-85ca-dadfb7a56160	2026-01-16 11:18:55.898017+00	tngtech/deepseek-r1t-chimera:free	tngtech/deepseek-r1t-chimera	tngtech/DeepSeek-R1T-Chimera	TNG: DeepSeek R1T Chimera (free)	1745760875	DeepSeek-R1T-Chimera is created by merging DeepSeek-R1 and DeepSeek-V3 (0324), combining the reasoning capabilities of R1 with the token efficiency improvements of V3. It is based on a DeepSeek-MoE Transformer architecture and is optimized for general text generation tasks.\n\nThe model merges pretrained weights from both source models to balance performance across reasoning, efficiency, and instruction-following tasks. It is released under the MIT license and intended for research and commercial use.	163840	{"modality": "text->text", "tokenizer": "DeepSeek", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0", "request": "0", "completion": "0", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 163840, "max_completion_tokens": null}	null	["frequency_penalty", "include_reasoning", "max_tokens", "presence_penalty", "reasoning", "repetition_penalty", "seed", "stop", "temperature", "top_k", "top_p"]	{}	null
9944456e-3008-4c1b-887d-af0d2e8c4254	2026-01-16 11:18:55.898017+00	tngtech/deepseek-r1t-chimera	tngtech/deepseek-r1t-chimera	tngtech/DeepSeek-R1T-Chimera	TNG: DeepSeek R1T Chimera	1745760875	DeepSeek-R1T-Chimera is created by merging DeepSeek-R1 and DeepSeek-V3 (0324), combining the reasoning capabilities of R1 with the token efficiency improvements of V3. It is based on a DeepSeek-MoE Transformer architecture and is optimized for general text generation tasks.\n\nThe model merges pretrained weights from both source models to balance performance across reasoning, efficiency, and instruction-following tasks. It is released under the MIT license and intended for research and commercial use.	163840	{"modality": "text->text", "tokenizer": "DeepSeek", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.0000003", "request": "0", "completion": "0.0000012", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 163840, "max_completion_tokens": 163840}	null	["frequency_penalty", "include_reasoning", "max_tokens", "presence_penalty", "reasoning", "repetition_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "top_k", "top_p"]	{}	null
16fa1caa-d030-4880-a4dd-5c469b6b46cd	2026-01-16 11:18:55.898017+00	openai/o4-mini-high	openai/o4-mini-high-2025-04-16		OpenAI: o4 Mini High	1744824212	OpenAI o4-mini-high is the same model as [o4-mini](/openai/o4-mini) with reasoning_effort set to high. \n\nOpenAI o4-mini is a compact reasoning model in the o-series, optimized for fast, cost-efficient performance while retaining strong multimodal and agentic capabilities. It supports tool use and demonstrates competitive reasoning and coding performance across benchmarks like AIME (99.5% with Python) and SWE-bench, outperforming its predecessor o3-mini and even approaching o3 in some domains.\n\nDespite its smaller size, o4-mini exhibits high accuracy in STEM tasks, visual problem solving (e.g., MathVista, MMMU), and code editing. It is especially well-suited for high-throughput scenarios where latency or cost is critical. Thanks to its efficient architecture and refined reinforcement learning training, o4-mini can chain tools, generate structured outputs, and solve multi-step tasks with minimal delay—often in under a minute.	200000	{"modality": "text+image->text", "tokenizer": "GPT", "instruct_type": null, "input_modalities": ["image", "text", "file"], "output_modalities": ["text"]}	{"prompt": "0.0000011", "completion": "0.0000044", "web_search": "0.01", "input_cache_read": "0.000000275"}	{"is_moderated": true, "context_length": 200000, "max_completion_tokens": 100000}	null	["include_reasoning", "max_tokens", "reasoning", "response_format", "seed", "structured_outputs", "tool_choice", "tools"]	{"top_p": null, "temperature": null, "frequency_penalty": null}	null
4124debc-348d-4752-b967-4a9c4ffd25e0	2026-01-16 11:18:55.898017+00	openai/o4-mini	openai/o4-mini-2025-04-16		OpenAI: o4 Mini	1744820942	OpenAI o4-mini is a compact reasoning model in the o-series, optimized for fast, cost-efficient performance while retaining strong multimodal and agentic capabilities. It supports tool use and demonstrates competitive reasoning and coding performance across benchmarks like AIME (99.5% with Python) and SWE-bench, outperforming its predecessor o3-mini and even approaching o3 in some domains.\n\nDespite its smaller size, o4-mini exhibits high accuracy in STEM tasks, visual problem solving (e.g., MathVista, MMMU), and code editing. It is especially well-suited for high-throughput scenarios where latency or cost is critical. Thanks to its efficient architecture and refined reinforcement learning training, o4-mini can chain tools, generate structured outputs, and solve multi-step tasks with minimal delay—often in under a minute.	200000	{"modality": "text+image->text", "tokenizer": "GPT", "instruct_type": null, "input_modalities": ["image", "text", "file"], "output_modalities": ["text"]}	{"prompt": "0.0000011", "completion": "0.0000044", "web_search": "0.01", "input_cache_read": "0.000000275"}	{"is_moderated": true, "context_length": 200000, "max_completion_tokens": 100000}	null	["include_reasoning", "max_tokens", "reasoning", "response_format", "seed", "structured_outputs", "tool_choice", "tools"]	{}	null
414be983-3c3f-4aad-91fb-e83c9a684c56	2026-01-16 11:18:55.898017+00	qwen/qwen2.5-coder-7b-instruct	qwen/qwen2.5-coder-7b-instruct	Qwen/Qwen2.5-Coder-7B-Instruct	Qwen: Qwen2.5 Coder 7B Instruct	1744734887	Qwen2.5-Coder-7B-Instruct is a 7B parameter instruction-tuned language model optimized for code-related tasks such as code generation, reasoning, and bug fixing. Based on the Qwen2.5 architecture, it incorporates enhancements like RoPE, SwiGLU, RMSNorm, and GQA attention with support for up to 128K tokens using YaRN-based extrapolation. It is trained on a large corpus of source code, synthetic data, and text-code grounding, providing robust performance across programming languages and agentic coding workflows.\n\nThis model is part of the Qwen2.5-Coder family and offers strong compatibility with tools like vLLM for efficient deployment. Released under the Apache 2.0 license.	32768	{"modality": "text->text", "tokenizer": "Qwen", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000003", "request": "0", "completion": "0.00000009", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 32768, "max_completion_tokens": null}	null	["frequency_penalty", "max_tokens", "presence_penalty", "repetition_penalty", "response_format", "structured_outputs", "temperature", "top_k", "top_p"]	{}	null
365efd10-1ed4-4449-978e-34266dc3b005	2026-01-16 11:18:55.898017+00	openai/gpt-4.1	openai/gpt-4.1-2025-04-14		OpenAI: GPT-4.1	1744651385	GPT-4.1 is a flagship large language model optimized for advanced instruction following, real-world software engineering, and long-context reasoning. It supports a 1 million token context window and outperforms GPT-4o and GPT-4.5 across coding (54.6% SWE-bench Verified), instruction compliance (87.4% IFEval), and multimodal understanding benchmarks. It is tuned for precise code diffs, agent reliability, and high recall in large document contexts, making it ideal for agents, IDE tooling, and enterprise knowledge retrieval.	1047576	{"modality": "text+image->text", "tokenizer": "GPT", "instruct_type": null, "input_modalities": ["image", "text", "file"], "output_modalities": ["text"]}	{"prompt": "0.000002", "completion": "0.000008", "web_search": "0.01", "input_cache_read": "0.0000005"}	{"is_moderated": true, "context_length": 1047576, "max_completion_tokens": 32768}	null	["max_tokens", "response_format", "seed", "structured_outputs", "temperature", "tool_choice", "tools", "top_p"]	{}	null
4ce4b8d3-6430-46f8-9a1f-1fadb2603c15	2026-01-16 11:18:55.898017+00	openai/gpt-4.1-mini	openai/gpt-4.1-mini-2025-04-14		OpenAI: GPT-4.1 Mini	1744651381	GPT-4.1 Mini is a mid-sized model delivering performance competitive with GPT-4o at substantially lower latency and cost. It retains a 1 million token context window and scores 45.1% on hard instruction evals, 35.8% on MultiChallenge, and 84.1% on IFEval. Mini also shows strong coding ability (e.g., 31.6% on Aider’s polyglot diff benchmark) and vision understanding, making it suitable for interactive applications with tight performance constraints.	1047576	{"modality": "text+image->text", "tokenizer": "GPT", "instruct_type": null, "input_modalities": ["image", "text", "file"], "output_modalities": ["text"]}	{"prompt": "0.0000004", "completion": "0.0000016", "web_search": "0.01", "input_cache_read": "0.0000001"}	{"is_moderated": true, "context_length": 1047576, "max_completion_tokens": 32768}	null	["max_tokens", "response_format", "seed", "structured_outputs", "temperature", "tool_choice", "tools", "top_p"]	{}	null
8bccdcaf-7387-49c3-9822-25ee523200e7	2026-01-16 11:18:55.898017+00	openai/gpt-4.1-nano	openai/gpt-4.1-nano-2025-04-14		OpenAI: GPT-4.1 Nano	1744651369	For tasks that demand low latency, GPT‑4.1 nano is the fastest and cheapest model in the GPT-4.1 series. It delivers exceptional performance at a small size with its 1 million token context window, and scores 80.1% on MMLU, 50.3% on GPQA, and 9.8% on Aider polyglot coding – even higher than GPT‑4o mini. It’s ideal for tasks like classification or autocompletion.	1047576	{"modality": "text+image->text", "tokenizer": "GPT", "instruct_type": null, "input_modalities": ["image", "text", "file"], "output_modalities": ["text"]}	{"prompt": "0.0000001", "completion": "0.0000004", "web_search": "0.01", "input_cache_read": "0.000000025"}	{"is_moderated": true, "context_length": 1047576, "max_completion_tokens": 32768}	null	["max_tokens", "response_format", "seed", "structured_outputs", "temperature", "tool_choice", "tools", "top_p"]	{}	null
0f9d849e-9899-4a87-b413-30eeeec15a40	2026-01-16 11:18:55.898017+00	eleutherai/llemma_7b	eleutherai/llemma_7b	EleutherAI/llemma_7b	EleutherAI: Llemma 7b	1744643225	Llemma 7B is a language model for mathematics. It was initialized with Code Llama 7B weights, and trained on the Proof-Pile-2 for 200B tokens. Llemma models are particularly strong at chain-of-thought mathematical reasoning and using computational tools for mathematics, such as Python and formal theorem provers.	4096	{"modality": "text->text", "tokenizer": "Other", "instruct_type": "code-llama", "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.0000008", "request": "0", "completion": "0.0000012", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 4096, "max_completion_tokens": 4096}	null	["frequency_penalty", "max_tokens", "min_p", "presence_penalty", "repetition_penalty", "seed", "stop", "temperature", "top_k", "top_p"]	{}	null
d0203737-9ddc-491e-9871-a4c1f028544c	2026-01-16 11:18:55.898017+00	alfredpros/codellama-7b-instruct-solidity	alfredpros/codellama-7b-instruct-solidity	AlfredPros/CodeLlama-7b-Instruct-Solidity	AlfredPros: CodeLLaMa 7B Instruct Solidity	1744641874	A finetuned 7 billion parameters Code LLaMA - Instruct model to generate Solidity smart contract using 4-bit QLoRA finetuning provided by PEFT library.	4096	{"modality": "text->text", "tokenizer": "Other", "instruct_type": "alpaca", "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.0000008", "request": "0", "completion": "0.0000012", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 4096, "max_completion_tokens": 4096}	null	["frequency_penalty", "max_tokens", "min_p", "presence_penalty", "repetition_penalty", "seed", "stop", "temperature", "top_k", "top_p"]	{}	null
ee1858a5-afaa-42c7-b842-b8eafc07eb34	2026-01-16 11:18:55.898017+00	x-ai/grok-3-mini-beta	x-ai/grok-3-mini-beta		xAI: Grok 3 Mini Beta	1744240195	Grok 3 Mini is a lightweight, smaller thinking model. Unlike traditional models that generate answers immediately, Grok 3 Mini thinks before responding. It’s ideal for reasoning-heavy tasks that don’t demand extensive domain knowledge, and shines in math-specific and quantitative use cases, such as solving challenging puzzles or math problems.\n\nTransparent "thinking" traces accessible. Defaults to low reasoning, can boost with setting `reasoning: { effort: "high" }`\n\nNote: That there are two xAI endpoints for this model. By default when using this model we will always route you to the base endpoint. If you want the fast endpoint you can add `provider: { sort: throughput}`, to sort by throughput instead. \n	131072	{"modality": "text->text", "tokenizer": "Grok", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.0000003", "request": "0", "completion": "0.0000005", "web_search": "0", "input_cache_read": "0.000000075", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 131072, "max_completion_tokens": null}	null	["include_reasoning", "logprobs", "max_tokens", "reasoning", "response_format", "seed", "stop", "temperature", "tool_choice", "tools", "top_logprobs", "top_p"]	{"top_p": null, "temperature": null, "frequency_penalty": null}	null
284e9bf7-787b-4a84-8a1d-80b0696b6d3f	2026-01-16 11:18:55.898017+00	x-ai/grok-3-beta	x-ai/grok-3-beta		xAI: Grok 3 Beta	1744240068	Grok 3 is the latest model from xAI. It's their flagship model that excels at enterprise use cases like data extraction, coding, and text summarization. Possesses deep domain knowledge in finance, healthcare, law, and science.\n\nExcels in structured tasks and benchmarks like GPQA, LCB, and MMLU-Pro where it outperforms Grok 3 Mini even on high thinking. \n\nNote: That there are two xAI endpoints for this model. By default when using this model we will always route you to the base endpoint. If you want the fast endpoint you can add `provider: { sort: throughput}`, to sort by throughput instead. \n	131072	{"modality": "text->text", "tokenizer": "Grok", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.000003", "request": "0", "completion": "0.000015", "web_search": "0", "input_cache_read": "0.00000075", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 131072, "max_completion_tokens": null}	null	["frequency_penalty", "logprobs", "max_tokens", "presence_penalty", "response_format", "seed", "stop", "temperature", "tool_choice", "tools", "top_logprobs", "top_p"]	{}	null
dcd28a36-5a4c-41b3-b025-b13c72a1ad44	2026-01-16 11:18:55.898017+00	nvidia/llama-3.1-nemotron-ultra-253b-v1	nvidia/llama-3.1-nemotron-ultra-253b-v1	nvidia/Llama-3_1-Nemotron-Ultra-253B-v1	NVIDIA: Llama 3.1 Nemotron Ultra 253B v1	1744115059	Llama-3.1-Nemotron-Ultra-253B-v1 is a large language model (LLM) optimized for advanced reasoning, human-interactive chat, retrieval-augmented generation (RAG), and tool-calling tasks. Derived from Meta’s Llama-3.1-405B-Instruct, it has been significantly customized using Neural Architecture Search (NAS), resulting in enhanced efficiency, reduced memory usage, and improved inference latency. The model supports a context length of up to 128K tokens and can operate efficiently on an 8x NVIDIA H100 node.\n\nNote: you must include `detailed thinking on` in the system prompt to enable reasoning. Please see [Usage Recommendations](https://huggingface.co/nvidia/Llama-3_1-Nemotron-Ultra-253B-v1#quick-start-and-usage-recommendations) for more.	131072	{"modality": "text->text", "tokenizer": "Llama3", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.0000006", "request": "0", "completion": "0.0000018", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 131072, "max_completion_tokens": null}	null	["frequency_penalty", "include_reasoning", "max_tokens", "presence_penalty", "reasoning", "repetition_penalty", "response_format", "structured_outputs", "temperature", "top_k", "top_p"]	{}	null
1df65658-73b1-4490-adfb-2a0a2218586c	2026-01-16 11:18:55.898017+00	meta-llama/llama-4-maverick	meta-llama/llama-4-maverick-17b-128e-instruct	meta-llama/Llama-4-Maverick-17B-128E-Instruct	Meta: Llama 4 Maverick	1743881822	Llama 4 Maverick 17B Instruct (128E) is a high-capacity multimodal language model from Meta, built on a mixture-of-experts (MoE) architecture with 128 experts and 17 billion active parameters per forward pass (400B total). It supports multilingual text and image input, and produces multilingual text and code output across 12 supported languages. Optimized for vision-language tasks, Maverick is instruction-tuned for assistant-like behavior, image reasoning, and general-purpose multimodal interaction.\n\nMaverick features early fusion for native multimodality and a 1 million token context window. It was trained on a curated mixture of public, licensed, and Meta-platform data, covering ~22 trillion tokens, with a knowledge cutoff in August 2024. Released on April 5, 2025 under the Llama 4 Community License, Maverick is suited for research and commercial applications requiring advanced multimodal understanding and high model throughput.	1048576	{"modality": "text+image->text", "tokenizer": "Llama4", "instruct_type": null, "input_modalities": ["text", "image"], "output_modalities": ["text"]}	{"image": "0.0006684", "prompt": "0.00000015", "request": "0", "completion": "0.0000006", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 1048576, "max_completion_tokens": 16384}	null	["frequency_penalty", "logit_bias", "logprobs", "max_tokens", "min_p", "presence_penalty", "repetition_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_k", "top_logprobs", "top_p"]	{}	null
457a5178-5d66-4a38-9567-27f6f5311a49	2026-01-16 11:18:55.898017+00	meta-llama/llama-4-scout	meta-llama/llama-4-scout-17b-16e-instruct	meta-llama/Llama-4-Scout-17B-16E-Instruct	Meta: Llama 4 Scout	1743881519	Llama 4 Scout 17B Instruct (16E) is a mixture-of-experts (MoE) language model developed by Meta, activating 17 billion parameters out of a total of 109B. It supports native multimodal input (text and image) and multilingual output (text and code) across 12 supported languages. Designed for assistant-style interaction and visual reasoning, Scout uses 16 experts per forward pass and features a context length of 10 million tokens, with a training corpus of ~40 trillion tokens.\n\nBuilt for high efficiency and local or commercial deployment, Llama 4 Scout incorporates early fusion for seamless modality integration. It is instruction-tuned for use in multilingual chat, captioning, and image understanding tasks. Released under the Llama 4 Community License, it was last trained on data up to August 2024 and launched publicly on April 5, 2025.	327680	{"modality": "text+image->text", "tokenizer": "Llama4", "instruct_type": null, "input_modalities": ["text", "image"], "output_modalities": ["text"]}	{"image": "0.0003342", "prompt": "0.00000008", "request": "0", "completion": "0.0000003", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 327680, "max_completion_tokens": 16384}	null	["frequency_penalty", "logit_bias", "max_tokens", "min_p", "presence_penalty", "repetition_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_k", "top_p"]	{}	null
d12d211e-add0-4ec6-a8c3-34639eda86e3	2026-01-16 11:18:55.898017+00	qwen/qwen2.5-vl-32b-instruct	qwen/qwen2.5-vl-32b-instruct	Qwen/Qwen2.5-VL-32B-Instruct	Qwen: Qwen2.5 VL 32B Instruct	1742839838	Qwen2.5-VL-32B is a multimodal vision-language model fine-tuned through reinforcement learning for enhanced mathematical reasoning, structured outputs, and visual problem-solving capabilities. It excels at visual analysis tasks, including object recognition, textual interpretation within images, and precise event localization in extended videos. Qwen2.5-VL-32B demonstrates state-of-the-art performance across multimodal benchmarks such as MMMU, MathVista, and VideoMME, while maintaining strong reasoning and clarity in text-based tasks like MMLU, mathematical problem-solving, and code generation.	16384	{"modality": "text+image->text", "tokenizer": "Qwen", "instruct_type": null, "input_modalities": ["text", "image"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000005", "request": "0", "completion": "0.00000022", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 16384, "max_completion_tokens": 16384}	null	["frequency_penalty", "logit_bias", "logprobs", "max_tokens", "min_p", "presence_penalty", "repetition_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "top_k", "top_logprobs", "top_p"]	{}	null
169d2a01-1411-4d1c-89b0-9012b68bc28a	2026-01-16 11:18:55.898017+00	deepseek/deepseek-chat-v3-0324	deepseek/deepseek-chat-v3-0324	deepseek-ai/DeepSeek-V3-0324	DeepSeek: DeepSeek V3 0324	1742824755	DeepSeek V3, a 685B-parameter, mixture-of-experts model, is the latest iteration of the flagship chat model family from the DeepSeek team.\n\nIt succeeds the [DeepSeek V3](/deepseek/deepseek-chat-v3) model and performs really well on a variety of tasks.	163840	{"modality": "text->text", "tokenizer": "DeepSeek", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000019", "request": "0", "completion": "0.00000087", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 163840, "max_completion_tokens": 65536}	null	["frequency_penalty", "logit_bias", "logprobs", "max_tokens", "min_p", "presence_penalty", "reasoning", "repetition_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_k", "top_logprobs", "top_p"]	{}	null
6a8ffc94-e5ba-4075-9448-64e8825b256a	2026-01-16 11:18:55.898017+00	openai/o1-pro	openai/o1-pro		OpenAI: o1-pro	1742423211	The o1 series of models are trained with reinforcement learning to think before they answer and perform complex reasoning. The o1-pro model uses more compute to think harder and provide consistently better answers.	200000	{"modality": "text+image->text", "tokenizer": "GPT", "instruct_type": null, "input_modalities": ["text", "image", "file"], "output_modalities": ["text"]}	{"prompt": "0.00015", "completion": "0.0006"}	{"is_moderated": true, "context_length": 200000, "max_completion_tokens": 100000}	null	["include_reasoning", "max_tokens", "reasoning", "response_format", "seed", "structured_outputs"]	{}	null
df72d4e9-9196-4493-b5ad-e3df7cf19e15	2026-01-16 11:18:55.898017+00	mistralai/mistral-small-3.1-24b-instruct:free	mistralai/mistral-small-3.1-24b-instruct-2503	mistralai/Mistral-Small-3.1-24B-Instruct-2503	Mistral: Mistral Small 3.1 24B (free)	1742238937	Mistral Small 3.1 24B Instruct is an upgraded variant of Mistral Small 3 (2501), featuring 24 billion parameters with advanced multimodal capabilities. It provides state-of-the-art performance in text-based reasoning and vision tasks, including image analysis, programming, mathematical reasoning, and multilingual support across dozens of languages. Equipped with an extensive 128k token context window and optimized for efficient local inference, it supports use cases such as conversational agents, function calling, long-document comprehension, and privacy-sensitive deployments. The updated version is [Mistral Small 3.2](mistralai/mistral-small-3.2-24b-instruct)	128000	{"modality": "text+image->text", "tokenizer": "Mistral", "instruct_type": null, "input_modalities": ["text", "image"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0", "request": "0", "completion": "0", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 128000, "max_completion_tokens": null}	null	["frequency_penalty", "max_tokens", "presence_penalty", "response_format", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_k", "top_p"]	{"temperature": 0.3}	null
4d51d2b3-64a7-4434-8e6d-5d691cdffce1	2026-01-16 11:18:55.898017+00	mistralai/mistral-small-3.1-24b-instruct	mistralai/mistral-small-3.1-24b-instruct-2503	mistralai/Mistral-Small-3.1-24B-Instruct-2503	Mistral: Mistral Small 3.1 24B	1742238937	Mistral Small 3.1 24B Instruct is an upgraded variant of Mistral Small 3 (2501), featuring 24 billion parameters with advanced multimodal capabilities. It provides state-of-the-art performance in text-based reasoning and vision tasks, including image analysis, programming, mathematical reasoning, and multilingual support across dozens of languages. Equipped with an extensive 128k token context window and optimized for efficient local inference, it supports use cases such as conversational agents, function calling, long-document comprehension, and privacy-sensitive deployments. The updated version is [Mistral Small 3.2](mistralai/mistral-small-3.2-24b-instruct)	131072	{"modality": "text+image->text", "tokenizer": "Mistral", "instruct_type": null, "input_modalities": ["text", "image"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000003", "request": "0", "completion": "0.00000011", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 131072, "max_completion_tokens": 131072}	null	["frequency_penalty", "max_tokens", "presence_penalty", "repetition_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_k", "top_p"]	{"temperature": 0.3}	null
23b45d7c-3d07-4201-9970-137d877097f0	2026-01-16 11:18:55.898017+00	allenai/olmo-2-0325-32b-instruct	allenai/olmo-2-0325-32b-instruct	allenai/OLMo-2-0325-32B-Instruct	AllenAI: Olmo 2 32B Instruct	1741988556	OLMo-2 32B Instruct is a supervised instruction-finetuned variant of the OLMo-2 32B March 2025 base model. It excels in complex reasoning and instruction-following tasks across diverse benchmarks such as GSM8K, MATH, IFEval, and general NLP evaluation. Developed by AI2, OLMo-2 32B is part of an open, research-oriented initiative, trained primarily on English-language datasets to advance the understanding and development of open-source language models.	128000	{"modality": "text->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000005", "request": "0", "completion": "0.0000002", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 128000, "max_completion_tokens": null}	null	[]	{}	null
ad6679d5-901e-4715-9742-3d81e303554e	2026-01-16 11:18:55.898017+00	google/gemma-3-4b-it:free	google/gemma-3-4b-it	google/gemma-3-4b-it	Google: Gemma 3 4B (free)	1741905510	Gemma 3 introduces multimodality, supporting vision-language input and text outputs. It handles context windows up to 128k tokens, understands over 140 languages, and offers improved math, reasoning, and chat capabilities, including structured outputs and function calling.	32768	{"modality": "text+image->text", "tokenizer": "Gemini", "instruct_type": "gemma", "input_modalities": ["text", "image"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0", "request": "0", "completion": "0", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 32768, "max_completion_tokens": 8192}	null	["max_tokens", "response_format", "seed", "stop", "temperature", "top_p"]	{}	null
0155bff7-ebd9-4524-8341-1bf3a1ea746d	2026-01-16 11:18:55.898017+00	google/gemma-3-4b-it	google/gemma-3-4b-it	google/gemma-3-4b-it	Google: Gemma 3 4B	1741905510	Gemma 3 introduces multimodality, supporting vision-language input and text outputs. It handles context windows up to 128k tokens, understands over 140 languages, and offers improved math, reasoning, and chat capabilities, including structured outputs and function calling.	96000	{"modality": "text+image->text", "tokenizer": "Gemini", "instruct_type": "gemma", "input_modalities": ["text", "image"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000001703012", "request": "0", "completion": "0.0000000681536", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 96000, "max_completion_tokens": null}	null	["frequency_penalty", "max_tokens", "min_p", "presence_penalty", "repetition_penalty", "response_format", "seed", "stop", "temperature", "top_k", "top_p"]	{}	null
5f2fa5eb-7304-4eb4-976b-fdd57e4d8fa9	2026-01-16 11:18:55.898017+00	google/gemma-3-12b-it:free	google/gemma-3-12b-it	google/gemma-3-12b-it	Google: Gemma 3 12B (free)	1741902625	Gemma 3 introduces multimodality, supporting vision-language input and text outputs. It handles context windows up to 128k tokens, understands over 140 languages, and offers improved math, reasoning, and chat capabilities, including structured outputs and function calling. Gemma 3 12B is the second largest in the family of Gemma 3 models after [Gemma 3 27B](google/gemma-3-27b-it)	32768	{"modality": "text+image->text", "tokenizer": "Gemini", "instruct_type": "gemma", "input_modalities": ["text", "image"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0", "request": "0", "completion": "0", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 32768, "max_completion_tokens": 8192}	null	["max_tokens", "seed", "stop", "temperature", "top_p"]	{}	null
f8be78c4-9c19-410f-8c19-70f7fe346b8b	2026-01-16 11:18:55.898017+00	google/gemma-3-12b-it	google/gemma-3-12b-it	google/gemma-3-12b-it	Google: Gemma 3 12B	1741902625	Gemma 3 introduces multimodality, supporting vision-language input and text outputs. It handles context windows up to 128k tokens, understands over 140 languages, and offers improved math, reasoning, and chat capabilities, including structured outputs and function calling. Gemma 3 12B is the second largest in the family of Gemma 3 models after [Gemma 3 27B](google/gemma-3-27b-it)	131072	{"modality": "text+image->text", "tokenizer": "Gemini", "instruct_type": "gemma", "input_modalities": ["text", "image"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000003", "request": "0", "completion": "0.0000001", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 131072, "max_completion_tokens": 131072}	null	["frequency_penalty", "logit_bias", "max_tokens", "min_p", "presence_penalty", "repetition_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "top_k", "top_p"]	{}	null
fb8405e7-0f57-4a41-b357-328eb0152d39	2026-01-16 11:18:55.898017+00	cohere/command-a	cohere/command-a-03-2025	CohereForAI/c4ai-command-a-03-2025	Cohere: Command A	1741894342	Command A is an open-weights 111B parameter model with a 256k context window focused on delivering great performance across agentic, multilingual, and coding use cases.\nCompared to other leading proprietary and open-weights models Command A delivers maximum performance with minimum hardware costs, excelling on business-critical agentic and multilingual tasks.	256000	{"modality": "text->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.0000025", "request": "0", "completion": "0.00001", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": true, "context_length": 256000, "max_completion_tokens": 8192}	null	["frequency_penalty", "max_tokens", "presence_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "top_k", "top_p"]	{}	null
0ba11a95-7f82-477e-a755-9cf87414c92f	2026-01-16 11:18:55.898017+00	openai/gpt-4o-mini-search-preview	openai/gpt-4o-mini-search-preview-2025-03-11		OpenAI: GPT-4o-mini Search Preview	1741818122	GPT-4o mini Search Preview is a specialized model for web search in Chat Completions. It is trained to understand and execute web search queries.	128000	{"modality": "text->text", "tokenizer": "GPT", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"prompt": "0.00000015", "completion": "0.0000006", "web_search": "0.0275"}	{"is_moderated": true, "context_length": 128000, "max_completion_tokens": 16384}	null	["max_tokens", "response_format", "structured_outputs", "web_search_options"]	{}	null
38129614-8726-47f3-a5be-bd724ba32711	2026-01-16 11:18:55.898017+00	openai/gpt-4o-search-preview	openai/gpt-4o-search-preview-2025-03-11		OpenAI: GPT-4o Search Preview	1741817949	GPT-4o Search Previewis a specialized model for web search in Chat Completions. It is trained to understand and execute web search queries.	128000	{"modality": "text->text", "tokenizer": "GPT", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"prompt": "0.0000025", "completion": "0.00001", "web_search": "0.035"}	{"is_moderated": true, "context_length": 128000, "max_completion_tokens": 16384}	null	["max_tokens", "response_format", "structured_outputs", "web_search_options"]	{}	null
8e3a7ace-cf93-4a01-8105-aae544fdece6	2026-01-16 11:18:55.898017+00	google/gemma-3-27b-it:free	google/gemma-3-27b-it	google/gemma-3-27b-it	Google: Gemma 3 27B (free)	1741756359	Gemma 3 introduces multimodality, supporting vision-language input and text outputs. It handles context windows up to 128k tokens, understands over 140 languages, and offers improved math, reasoning, and chat capabilities, including structured outputs and function calling. Gemma 3 27B is Google's latest open source model, successor to [Gemma 2](google/gemma-2-27b-it)	131072	{"modality": "text+image->text", "tokenizer": "Gemini", "instruct_type": "gemma", "input_modalities": ["text", "image"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0", "request": "0", "completion": "0", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 131072, "max_completion_tokens": null}	null	["frequency_penalty", "max_tokens", "presence_penalty", "repetition_penalty", "response_format", "seed", "stop", "temperature", "tool_choice", "tools", "top_p"]	{"top_p": null, "temperature": null, "frequency_penalty": null}	null
08e05db3-228c-4115-852b-862a42e9aa27	2026-01-16 11:18:55.898017+00	google/gemma-3-27b-it	google/gemma-3-27b-it	google/gemma-3-27b-it	Google: Gemma 3 27B	1741756359	Gemma 3 introduces multimodality, supporting vision-language input and text outputs. It handles context windows up to 128k tokens, understands over 140 languages, and offers improved math, reasoning, and chat capabilities, including structured outputs and function calling. Gemma 3 27B is Google's latest open source model, successor to [Gemma 2](google/gemma-2-27b-it)	96000	{"modality": "text+image->text", "tokenizer": "Gemini", "instruct_type": "gemma", "input_modalities": ["text", "image"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000004", "request": "0", "completion": "0.00000015", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 96000, "max_completion_tokens": 96000}	null	["frequency_penalty", "logit_bias", "max_tokens", "min_p", "presence_penalty", "repetition_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_k", "top_p"]	{"top_p": null, "temperature": null, "frequency_penalty": null}	null
8db37b53-dd9b-4a8b-8165-dffca66a76ba	2026-01-16 11:18:55.898017+00	perplexity/sonar-reasoning-pro	perplexity/sonar-reasoning-pro		Perplexity: Sonar Reasoning Pro	1741313308	Note: Sonar Pro pricing includes Perplexity search pricing. See [details here](https://docs.perplexity.ai/guides/pricing#detailed-pricing-breakdown-for-sonar-reasoning-pro-and-sonar-pro)\n\nSonar Reasoning Pro is a premier reasoning model powered by DeepSeek R1 with Chain of Thought (CoT). Designed for advanced use cases, it supports in-depth, multi-step queries with a larger context window and can surface more citations per search, enabling more comprehensive and extensible responses.	128000	{"modality": "text+image->text", "tokenizer": "Other", "instruct_type": "deepseek-r1", "input_modalities": ["text", "image"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.000002", "request": "0", "completion": "0.000008", "web_search": "0.005", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 128000, "max_completion_tokens": null}	null	["frequency_penalty", "include_reasoning", "max_tokens", "presence_penalty", "reasoning", "temperature", "top_k", "top_p", "web_search_options"]	{}	null
3e42085e-49dd-475b-a812-8d1045b4ef39	2026-01-16 11:18:55.898017+00	perplexity/sonar-pro	perplexity/sonar-pro		Perplexity: Sonar Pro	1741312423	Note: Sonar Pro pricing includes Perplexity search pricing. See [details here](https://docs.perplexity.ai/guides/pricing#detailed-pricing-breakdown-for-sonar-reasoning-pro-and-sonar-pro)\n\nFor enterprises seeking more advanced capabilities, the Sonar Pro API can handle in-depth, multi-step queries with added extensibility, like double the number of citations per search as Sonar on average. Plus, with a larger context window, it can handle longer and more nuanced searches and follow-up questions. 	200000	{"modality": "text+image->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["text", "image"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.000003", "request": "0", "completion": "0.000015", "web_search": "0.005", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 200000, "max_completion_tokens": 8000}	null	["frequency_penalty", "max_tokens", "presence_penalty", "temperature", "top_k", "top_p", "web_search_options"]	{}	null
bdfd7d81-ef1b-4907-9645-efd0e46b197f	2026-01-16 11:18:55.898017+00	perplexity/sonar-deep-research	perplexity/sonar-deep-research		Perplexity: Sonar Deep Research	1741311246	Sonar Deep Research is a research-focused model designed for multi-step retrieval, synthesis, and reasoning across complex topics. It autonomously searches, reads, and evaluates sources, refining its approach as it gathers information. This enables comprehensive report generation across domains like finance, technology, health, and current events.\n\nNotes on Pricing ([Source](https://docs.perplexity.ai/guides/pricing#detailed-pricing-breakdown-for-sonar-deep-research)) \n- Input tokens comprise of Prompt tokens (user prompt) + Citation tokens (these are processed tokens from running searches)\n- Deep Research runs multiple searches to conduct exhaustive research. Searches are priced at $5/1000 searches. A request that does 30 searches will cost $0.15 in this step.\n- Reasoning is a distinct step in Deep Research since it does extensive automated reasoning through all the material it gathers during its research phase. Reasoning tokens here are a bit different than the CoTs in the answer - these are tokens that we use to reason through the research material prior to generating the outputs via the CoTs. Reasoning tokens are priced at $3/1M tokens	128000	{"modality": "text->text", "tokenizer": "Other", "instruct_type": "deepseek-r1", "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.000002", "request": "0", "completion": "0.000008", "web_search": "0.005", "internal_reasoning": "0.000003"}	{"is_moderated": false, "context_length": 128000, "max_completion_tokens": null}	null	["frequency_penalty", "include_reasoning", "max_tokens", "presence_penalty", "reasoning", "temperature", "top_k", "top_p", "web_search_options"]	{}	null
67f611f6-1551-4a8d-b9a7-eecaba4451f8	2026-01-16 11:18:55.898017+00	qwen/qwq-32b	qwen/qwq-32b	Qwen/QwQ-32B	Qwen: QwQ 32B	1741208814	QwQ is the reasoning model of the Qwen series. Compared with conventional instruction-tuned models, QwQ, which is capable of thinking and reasoning, can achieve significantly enhanced performance in downstream tasks, especially hard problems. QwQ-32B is the medium-sized reasoning model, which is capable of achieving competitive performance against state-of-the-art reasoning models, e.g., DeepSeek-R1, o1-mini.	32768	{"modality": "text->text", "tokenizer": "Qwen", "instruct_type": "qwq", "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000015", "request": "0", "completion": "0.0000004", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 32768, "max_completion_tokens": null}	null	["frequency_penalty", "include_reasoning", "logit_bias", "max_tokens", "min_p", "presence_penalty", "reasoning", "repetition_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_k", "top_p"]	{}	null
7b6fc15e-dbd6-449f-b9c8-c986c5ec8ff1	2026-01-16 11:18:55.898017+00	google/gemini-2.0-flash-lite-001	google/gemini-2.0-flash-lite-001		Google: Gemini 2.0 Flash Lite	1740506212	Gemini 2.0 Flash Lite offers a significantly faster time to first token (TTFT) compared to [Gemini Flash 1.5](/google/gemini-flash-1.5), while maintaining quality on par with larger models like [Gemini Pro 1.5](/google/gemini-pro-1.5), all at extremely economical token prices.	1048576	{"modality": "text+image->text", "tokenizer": "Gemini", "instruct_type": null, "input_modalities": ["text", "image", "file", "audio", "video"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.000000075", "request": "0", "completion": "0.0000003", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 1048576, "max_completion_tokens": 8192}	null	["max_tokens", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_p"]	{"top_p": null, "temperature": null, "frequency_penalty": null}	2026-02-06
b84b2874-7128-4065-b1dc-08a9404a527c	2026-01-16 11:18:55.898017+00	anthropic/claude-3.7-sonnet:thinking	anthropic/claude-3-7-sonnet-20250219		Anthropic: Claude 3.7 Sonnet (thinking)	1740422110	Claude 3.7 Sonnet is an advanced large language model with improved reasoning, coding, and problem-solving capabilities. It introduces a hybrid reasoning approach, allowing users to choose between rapid responses and extended, step-by-step processing for complex tasks. The model demonstrates notable improvements in coding, particularly in front-end development and full-stack updates, and excels in agentic workflows, where it can autonomously navigate multi-step processes. \n\nClaude 3.7 Sonnet maintains performance parity with its predecessor in standard mode while offering an extended reasoning mode for enhanced accuracy in math, coding, and instruction-following tasks.\n\nRead more at the [blog post here](https://www.anthropic.com/news/claude-3-7-sonnet)	200000	{"modality": "text+image->text", "tokenizer": "Claude", "instruct_type": null, "input_modalities": ["text", "image", "file"], "output_modalities": ["text"]}	{"image": "0.0048", "prompt": "0.000003", "request": "0", "completion": "0.000015", "web_search": "0", "input_cache_read": "0.0000003", "input_cache_write": "0.00000375", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 200000, "max_completion_tokens": 64000}	null	["include_reasoning", "max_tokens", "reasoning", "stop", "temperature", "tool_choice", "tools", "top_p"]	{"top_p": null, "temperature": null, "frequency_penalty": null}	null
6bced271-1060-4951-ba69-5c7729bcf2fa	2026-01-16 11:18:55.898017+00	anthropic/claude-3.7-sonnet	anthropic/claude-3-7-sonnet-20250219		Anthropic: Claude 3.7 Sonnet	1740422110	Claude 3.7 Sonnet is an advanced large language model with improved reasoning, coding, and problem-solving capabilities. It introduces a hybrid reasoning approach, allowing users to choose between rapid responses and extended, step-by-step processing for complex tasks. The model demonstrates notable improvements in coding, particularly in front-end development and full-stack updates, and excels in agentic workflows, where it can autonomously navigate multi-step processes. \n\nClaude 3.7 Sonnet maintains performance parity with its predecessor in standard mode while offering an extended reasoning mode for enhanced accuracy in math, coding, and instruction-following tasks.\n\nRead more at the [blog post here](https://www.anthropic.com/news/claude-3-7-sonnet)	200000	{"modality": "text+image->text", "tokenizer": "Claude", "instruct_type": null, "input_modalities": ["text", "image", "file"], "output_modalities": ["text"]}	{"image": "0.0048", "prompt": "0.000003", "request": "0", "completion": "0.000015", "web_search": "0", "input_cache_read": "0.0000003", "input_cache_write": "0.00000375", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 200000, "max_completion_tokens": 64000}	null	["include_reasoning", "max_tokens", "reasoning", "stop", "temperature", "tool_choice", "tools", "top_k", "top_p"]	{"top_p": null, "temperature": null, "frequency_penalty": null}	null
8f9540e0-41b6-48be-a174-20c1679c3ed7	2026-01-16 11:18:55.898017+00	mistralai/mistral-saba	mistralai/mistral-saba-2502		Mistral: Saba	1739803239	Mistral Saba is a 24B-parameter language model specifically designed for the Middle East and South Asia, delivering accurate and contextually relevant responses while maintaining efficient performance. Trained on curated regional datasets, it supports multiple Indian-origin languages—including Tamil and Malayalam—alongside Arabic. This makes it a versatile option for a range of regional and multilingual applications. Read more at the blog post [here](https://mistral.ai/en/news/mistral-saba)	32768	{"modality": "text->text", "tokenizer": "Mistral", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"prompt": "0.0000002", "completion": "0.0000006"}	{"is_moderated": false, "context_length": 32768, "max_completion_tokens": null}	null	["frequency_penalty", "max_tokens", "presence_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_p"]	{"temperature": 0.3}	null
6a778452-3a97-4942-b179-1b05a3354ab8	2026-01-16 11:18:55.898017+00	meta-llama/llama-guard-3-8b	meta-llama/llama-guard-3-8b	meta-llama/Llama-Guard-3-8B	Llama Guard 3 8B	1739401318	Llama Guard 3 is a Llama-3.1-8B pretrained model, fine-tuned for content safety classification. Similar to previous versions, it can be used to classify content in both LLM inputs (prompt classification) and in LLM responses (response classification). It acts as an LLM – it generates text in its output that indicates whether a given prompt or response is safe or unsafe, and if unsafe, it also lists the content categories violated.\n\nLlama Guard 3 was aligned to safeguard against the MLCommons standardized hazards taxonomy and designed to support Llama 3.1 capabilities. Specifically, it provides content moderation in 8 languages, and was optimized to support safety and security for search and code interpreter tool calls.\n	131072	{"modality": "text->text", "tokenizer": "Llama3", "instruct_type": "none", "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000002", "request": "0", "completion": "0.00000006", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 131072, "max_completion_tokens": null}	null	["frequency_penalty", "max_tokens", "presence_penalty", "repetition_penalty", "seed", "temperature", "top_k", "top_p"]	{}	null
57398273-94b2-4d23-b844-2f0fc33f0417	2026-01-16 11:18:55.898017+00	openai/o3-mini-high	openai/o3-mini-high-2025-01-31		OpenAI: o3 Mini High	1739372611	OpenAI o3-mini-high is the same model as [o3-mini](/openai/o3-mini) with reasoning_effort set to high. \n\no3-mini is a cost-efficient language model optimized for STEM reasoning tasks, particularly excelling in science, mathematics, and coding. The model features three adjustable reasoning effort levels and supports key developer capabilities including function calling, structured outputs, and streaming, though it does not include vision processing capabilities.\n\nThe model demonstrates significant improvements over its predecessor, with expert testers preferring its responses 56% of the time and noting a 39% reduction in major errors on complex questions. With medium reasoning effort settings, o3-mini matches the performance of the larger o1 model on challenging reasoning evaluations like AIME and GPQA, while maintaining lower latency and cost.	200000	{"modality": "text->text", "tokenizer": "GPT", "instruct_type": null, "input_modalities": ["text", "file"], "output_modalities": ["text"]}	{"prompt": "0.0000011", "completion": "0.0000044", "input_cache_read": "0.00000055"}	{"is_moderated": true, "context_length": 200000, "max_completion_tokens": 100000}	null	["max_tokens", "response_format", "seed", "structured_outputs", "tool_choice", "tools"]	{"top_p": null, "temperature": null, "frequency_penalty": null}	null
3d73b7cb-b06b-486d-b2a6-3b8c0605afb6	2026-01-16 11:18:55.898017+00	google/gemini-2.0-flash-001	google/gemini-2.0-flash-001		Google: Gemini 2.0 Flash	1738769413	Gemini Flash 2.0 offers a significantly faster time to first token (TTFT) compared to [Gemini Flash 1.5](/google/gemini-flash-1.5), while maintaining quality on par with larger models like [Gemini Pro 1.5](/google/gemini-pro-1.5). It introduces notable enhancements in multimodal understanding, coding capabilities, complex instruction following, and function calling. These advancements come together to deliver more seamless and robust agentic experiences.	1048576	{"modality": "text+image->text", "tokenizer": "Gemini", "instruct_type": null, "input_modalities": ["text", "image", "file", "audio", "video"], "output_modalities": ["text"]}	{"audio": "0.0000007", "image": "0.0000258", "prompt": "0.0000001", "request": "0", "completion": "0.0000004", "web_search": "0", "input_cache_read": "0.000000025", "input_cache_write": "0.0000001833", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 1048576, "max_completion_tokens": 8192}	null	["max_tokens", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_p"]	{"top_p": null, "temperature": null, "frequency_penalty": null}	null
a9ac2301-ddbc-4b2e-aa01-38abe2235c01	2026-01-16 11:18:55.898017+00	aion-labs/aion-1.0	aion-labs/aion-1.0		AionLabs: Aion-1.0	1738697557	Aion-1.0 is a multi-model system designed for high performance across various tasks, including reasoning and coding. It is built on DeepSeek-R1, augmented with additional models and techniques such as Tree of Thoughts (ToT) and Mixture of Experts (MoE). It is Aion Lab's most powerful reasoning model.	131072	{"modality": "text->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.000004", "request": "0", "completion": "0.000008", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 131072, "max_completion_tokens": 32768}	null	["include_reasoning", "max_tokens", "reasoning", "temperature", "top_p"]	{}	null
be446ff6-f99f-42c2-a62b-186823baeefa	2026-01-16 11:18:55.898017+00	aion-labs/aion-1.0-mini	aion-labs/aion-1.0-mini	FuseAI/FuseO1-DeepSeekR1-QwQ-SkyT1-32B-Preview	AionLabs: Aion-1.0-Mini	1738697107	Aion-1.0-Mini 32B parameter model is a distilled version of the DeepSeek-R1 model, designed for strong performance in reasoning domains such as mathematics, coding, and logic. It is a modified variant of a FuseAI model that outperforms R1-Distill-Qwen-32B and R1-Distill-Llama-70B, with benchmark results available on its [Hugging Face page](https://huggingface.co/FuseAI/FuseO1-DeepSeekR1-QwQ-SkyT1-32B-Preview), independently replicated for verification.	131072	{"modality": "text->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.0000007", "request": "0", "completion": "0.0000014", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 131072, "max_completion_tokens": 32768}	null	["include_reasoning", "max_tokens", "reasoning", "temperature", "top_p"]	{}	null
41ea2271-3bd6-4d0d-83e3-995b9b28c8f7	2026-01-16 11:18:55.898017+00	aion-labs/aion-rp-llama-3.1-8b	aion-labs/aion-rp-llama-3.1-8b		AionLabs: Aion-RP 1.0 (8B)	1738696718	Aion-RP-Llama-3.1-8B ranks the highest in the character evaluation portion of the RPBench-Auto benchmark, a roleplaying-specific variant of Arena-Hard-Auto, where LLMs evaluate each other’s responses. It is a fine-tuned base model rather than an instruct model, designed to produce more natural and varied writing.	32768	{"modality": "text->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.0000008", "request": "0", "completion": "0.0000016", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 32768, "max_completion_tokens": 32768}	null	["max_tokens", "temperature", "top_p"]	{}	null
83298af2-10bf-4b05-95db-9286c1ff52f6	2026-01-16 11:18:55.898017+00	qwen/qwen2.5-vl-72b-instruct	qwen/qwen2.5-vl-72b-instruct	Qwen/Qwen2.5-VL-72B-Instruct	Qwen: Qwen2.5 VL 72B Instruct	1738410311	Qwen2.5-VL is proficient in recognizing common objects such as flowers, birds, fish, and insects. It is also highly capable of analyzing texts, charts, icons, graphics, and layouts within images.	32768	{"modality": "text+image->text", "tokenizer": "Qwen", "instruct_type": null, "input_modalities": ["text", "image"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000015", "request": "0", "completion": "0.0000006", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 32768, "max_completion_tokens": 32768}	null	["frequency_penalty", "logit_bias", "max_tokens", "min_p", "presence_penalty", "repetition_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "top_k", "top_p"]	{}	null
95d73920-193d-4b13-9c66-cc662ed800a6	2026-01-16 11:18:55.898017+00	openai/o3-mini	openai/o3-mini-2025-01-31		OpenAI: o3 Mini	1738351721	OpenAI o3-mini is a cost-efficient language model optimized for STEM reasoning tasks, particularly excelling in science, mathematics, and coding.\n\nThis model supports the `reasoning_effort` parameter, which can be set to "high", "medium", or "low" to control the thinking time of the model. The default is "medium". OpenRouter also offers the model slug `openai/o3-mini-high` to default the parameter to "high".\n\nThe model features three adjustable reasoning effort levels and supports key developer capabilities including function calling, structured outputs, and streaming, though it does not include vision processing capabilities.\n\nThe model demonstrates significant improvements over its predecessor, with expert testers preferring its responses 56% of the time and noting a 39% reduction in major errors on complex questions. With medium reasoning effort settings, o3-mini matches the performance of the larger o1 model on challenging reasoning evaluations like AIME and GPQA, while maintaining lower latency and cost.	200000	{"modality": "text->text", "tokenizer": "GPT", "instruct_type": null, "input_modalities": ["text", "file"], "output_modalities": ["text"]}	{"prompt": "0.0000011", "completion": "0.0000044", "input_cache_read": "0.00000055"}	{"is_moderated": true, "context_length": 200000, "max_completion_tokens": 100000}	null	["max_tokens", "response_format", "seed", "structured_outputs", "tool_choice", "tools"]	{}	null
eda6ca39-49d2-474b-b73d-433ce9d152f0	2026-01-16 11:18:55.898017+00	mistralai/mistral-small-24b-instruct-2501	mistralai/mistral-small-24b-instruct-2501	mistralai/Mistral-Small-24B-Instruct-2501	Mistral: Mistral Small 3	1738255409	Mistral Small 3 is a 24B-parameter language model optimized for low-latency performance across common AI tasks. Released under the Apache 2.0 license, it features both pre-trained and instruction-tuned versions designed for efficient local deployment.\n\nThe model achieves 81% accuracy on the MMLU benchmark and performs competitively with larger models like Llama 3.3 70B and Qwen 32B, while operating at three times the speed on equivalent hardware. [Read the blog post about the model here.](https://mistral.ai/news/mistral-small-3/)	32768	{"modality": "text->text", "tokenizer": "Mistral", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000003", "request": "0", "completion": "0.00000011", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 32768, "max_completion_tokens": 32768}	null	["frequency_penalty", "logit_bias", "max_tokens", "min_p", "presence_penalty", "repetition_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_k", "top_p"]	{"top_p": null, "temperature": 0.3, "frequency_penalty": null}	null
ca416d4f-6143-429b-bfa3-91de27d7cdf8	2026-01-16 11:18:55.898017+00	deepseek/deepseek-r1-distill-qwen-32b	deepseek/deepseek-r1-distill-qwen-32b	deepseek-ai/DeepSeek-R1-Distill-Qwen-32B	DeepSeek: R1 Distill Qwen 32B	1738194830	DeepSeek R1 Distill Qwen 32B is a distilled large language model based on [Qwen 2.5 32B](https://huggingface.co/Qwen/Qwen2.5-32B), using outputs from [DeepSeek R1](/deepseek/deepseek-r1). It outperforms OpenAI's o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.\\n\\nOther benchmark results include:\\n\\n- AIME 2024 pass@1: 72.6\\n- MATH-500 pass@1: 94.3\\n- CodeForces Rating: 1691\\n\\nThe model leverages fine-tuning from DeepSeek R1's outputs, enabling competitive performance comparable to larger frontier models.	32768	{"modality": "text->text", "tokenizer": "Qwen", "instruct_type": "deepseek-r1", "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000029", "request": "0", "completion": "0.00000029", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 32768, "max_completion_tokens": null}	null	["frequency_penalty", "include_reasoning", "max_tokens", "presence_penalty", "reasoning", "repetition_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "top_k", "top_p"]	{}	null
3530ede3-dea5-41f8-9c7c-e380e1daab86	2026-01-16 11:18:55.898017+00	perplexity/sonar	perplexity/sonar		Perplexity: Sonar	1738013808	Sonar is lightweight, affordable, fast, and simple to use — now featuring citations and the ability to customize sources. It is designed for companies seeking to integrate lightweight question-and-answer features optimized for speed.	127072	{"modality": "text+image->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["text", "image"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.000001", "request": "0.005", "completion": "0.000001", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 127072, "max_completion_tokens": null}	null	["frequency_penalty", "max_tokens", "presence_penalty", "temperature", "top_k", "top_p", "web_search_options"]	{}	null
7116f233-7a43-42bc-b594-0ec3ddeb6193	2026-01-16 11:18:55.898017+00	deepseek/deepseek-r1-distill-llama-70b	deepseek/deepseek-r1-distill-llama-70b	deepseek-ai/DeepSeek-R1-Distill-Llama-70B	DeepSeek: R1 Distill Llama 70B	1737663169	DeepSeek R1 Distill Llama 70B is a distilled large language model based on [Llama-3.3-70B-Instruct](/meta-llama/llama-3.3-70b-instruct), using outputs from [DeepSeek R1](/deepseek/deepseek-r1). The model combines advanced distillation techniques to achieve high performance across multiple benchmarks, including:\n\n- AIME 2024 pass@1: 70.0\n- MATH-500 pass@1: 94.5\n- CodeForces Rating: 1633\n\nThe model leverages fine-tuning from DeepSeek R1's outputs, enabling competitive performance comparable to larger frontier models.	131072	{"modality": "text->text", "tokenizer": "Llama3", "instruct_type": "deepseek-r1", "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000003", "request": "0", "completion": "0.00000011", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 131072, "max_completion_tokens": 131072}	null	["frequency_penalty", "include_reasoning", "logit_bias", "max_tokens", "min_p", "presence_penalty", "reasoning", "repetition_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_k", "top_p"]	{}	null
9cb91cab-4365-4efd-a60f-38f47a6065de	2026-01-16 11:18:55.898017+00	deepseek/deepseek-r1	deepseek/deepseek-r1	deepseek-ai/DeepSeek-R1	DeepSeek: R1	1737381095	DeepSeek R1 is here: Performance on par with [OpenAI o1](/openai/o1), but open-sourced and with fully open reasoning tokens. It's 671B parameters in size, with 37B active in an inference pass.\n\nFully open-source model & [technical report](https://api-docs.deepseek.com/news/news250120).\n\nMIT licensed: Distill & commercialize freely!	64000	{"modality": "text->text", "tokenizer": "DeepSeek", "instruct_type": "deepseek-r1", "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.0000007", "request": "0", "completion": "0.0000025", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 64000, "max_completion_tokens": 16000}	null	["frequency_penalty", "include_reasoning", "max_tokens", "presence_penalty", "reasoning", "repetition_penalty", "seed", "stop", "temperature", "tool_choice", "tools", "top_k", "top_p"]	{}	null
008c6430-e0e3-4051-92dc-04b8da52a0f2	2026-01-16 11:18:55.898017+00	minimax/minimax-01	minimax/minimax-01	MiniMaxAI/MiniMax-Text-01	MiniMax: MiniMax-01	1736915462	MiniMax-01 is a combines MiniMax-Text-01 for text generation and MiniMax-VL-01 for image understanding. It has 456 billion parameters, with 45.9 billion parameters activated per inference, and can handle a context of up to 4 million tokens.\n\nThe text model adopts a hybrid architecture that combines Lightning Attention, Softmax Attention, and Mixture-of-Experts (MoE). The image model adopts the “ViT-MLP-LLM” framework and is trained on top of the text model.\n\nTo read more about the release, see: https://www.minimaxi.com/en/news/minimax-01-series-2	1000192	{"modality": "text+image->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["text", "image"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.0000002", "request": "0", "completion": "0.0000011", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 1000192, "max_completion_tokens": 1000192}	null	["max_tokens", "temperature", "top_p"]	{}	null
59e795c9-c063-4b47-8704-9353ef3c7216	2026-01-16 11:18:55.898017+00	microsoft/phi-4	microsoft/phi-4	microsoft/phi-4	Microsoft: Phi 4	1736489872	[Microsoft Research](/microsoft) Phi-4 is designed to perform well in complex reasoning tasks and can operate efficiently in situations with limited memory or where quick responses are needed. \n\nAt 14 billion parameters, it was trained on a mix of high-quality synthetic datasets, data from curated websites, and academic materials. It has undergone careful improvement to follow instructions accurately and maintain strong safety standards. It works best with English language inputs.\n\nFor more information, please see [Phi-4 Technical Report](https://arxiv.org/pdf/2412.08905)\n	16384	{"modality": "text->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000006", "request": "0", "completion": "0.00000014", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 16384, "max_completion_tokens": null}	null	["frequency_penalty", "max_tokens", "min_p", "presence_penalty", "repetition_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "top_k", "top_p"]	{}	null
6401995b-fea1-46bc-9cf1-cad74134e68c	2026-01-16 11:18:55.898017+00	sao10k/l3.1-70b-hanami-x1	sao10k/l3.1-70b-hanami-x1	Sao10K/L3.1-70B-Hanami-x1	Sao10K: Llama 3.1 70B Hanami x1	1736302854	This is [Sao10K](/sao10k)'s experiment over [Euryale v2.2](/sao10k/l3.1-euryale-70b).	16000	{"modality": "text->text", "tokenizer": "Llama3", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.000003", "request": "0", "completion": "0.000003", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 16000, "max_completion_tokens": null}	null	["frequency_penalty", "logit_bias", "max_tokens", "min_p", "presence_penalty", "repetition_penalty", "seed", "stop", "temperature", "top_k", "top_p"]	{}	null
b4c2711c-3bef-442a-90f8-5210a5557ce3	2026-01-16 11:18:55.898017+00	deepseek/deepseek-chat	deepseek/deepseek-chat-v3	deepseek-ai/DeepSeek-V3	DeepSeek: DeepSeek V3	1735241320	DeepSeek-V3 is the latest model from the DeepSeek team, building upon the instruction following and coding abilities of the previous versions. Pre-trained on nearly 15 trillion tokens, the reported evaluations reveal that the model outperforms other open-source models and rivals leading closed-source models.\n\nFor model details, please visit [the DeepSeek-V3 repo](https://github.com/deepseek-ai/DeepSeek-V3) for more information, or see the [launch announcement](https://api-docs.deepseek.com/news/news1226).	163840	{"modality": "text->text", "tokenizer": "DeepSeek", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.0000003", "request": "0", "completion": "0.0000012", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 163840, "max_completion_tokens": 163840}	null	["frequency_penalty", "max_tokens", "min_p", "presence_penalty", "repetition_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_k", "top_p"]	{}	null
249b8e9c-1ff4-41fe-a5de-3e0f93f05035	2026-01-16 11:18:55.898017+00	sao10k/l3.3-euryale-70b	sao10k/l3.3-euryale-70b-v2.3	Sao10K/L3.3-70B-Euryale-v2.3	Sao10K: Llama 3.3 Euryale 70B	1734535928	Euryale L3.3 70B is a model focused on creative roleplay from [Sao10k](https://ko-fi.com/sao10k). It is the successor of [Euryale L3 70B v2.2](/models/sao10k/l3-euryale-70b).	131072	{"modality": "text->text", "tokenizer": "Llama3", "instruct_type": "llama3", "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000065", "request": "0", "completion": "0.00000075", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 131072, "max_completion_tokens": 16384}	null	["frequency_penalty", "max_tokens", "min_p", "presence_penalty", "repetition_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "top_k", "top_p"]	{}	null
4aff8dec-c8ce-418d-a30e-89cff65dcbee	2026-01-16 11:18:55.898017+00	openai/o1	openai/o1-2024-12-17		OpenAI: o1	1734459999	The latest and strongest model family from OpenAI, o1 is designed to spend more time thinking before responding. The o1 model series is trained with large-scale reinforcement learning to reason using chain of thought. \n\nThe o1 models are optimized for math, science, programming, and other STEM-related tasks. They consistently exhibit PhD-level accuracy on benchmarks in physics, chemistry, and biology. Learn more in the [launch announcement](https://openai.com/o1).\n	200000	{"modality": "text+image->text", "tokenizer": "GPT", "instruct_type": null, "input_modalities": ["text", "image", "file"], "output_modalities": ["text"]}	{"prompt": "0.000015", "completion": "0.00006", "input_cache_read": "0.0000075"}	{"is_moderated": true, "context_length": 200000, "max_completion_tokens": 100000}	null	["max_tokens", "response_format", "seed", "structured_outputs", "tool_choice", "tools"]	{}	null
73b0da03-b89d-48ac-a522-deb5ab6d1e08	2026-01-16 11:18:55.898017+00	cohere/command-r7b-12-2024	cohere/command-r7b-12-2024		Cohere: Command R7B (12-2024)	1734158152	Command R7B (12-2024) is a small, fast update of the Command R+ model, delivered in December 2024. It excels at RAG, tool use, agents, and similar tasks requiring complex reasoning and multiple steps.\n\nUse of this model is subject to Cohere's [Usage Policy](https://docs.cohere.com/docs/usage-policy) and [SaaS Agreement](https://cohere.com/saas-agreement).	128000	{"modality": "text->text", "tokenizer": "Cohere", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.0000000375", "request": "0", "completion": "0.00000015", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": true, "context_length": 128000, "max_completion_tokens": 4000}	null	["frequency_penalty", "max_tokens", "presence_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "top_k", "top_p"]	{}	null
56a816e8-caff-4669-9ae5-7af8d795b4d4	2026-01-16 11:18:55.898017+00	google/gemini-2.0-flash-exp:free	google/gemini-2.0-flash-exp		Google: Gemini 2.0 Flash Experimental (free)	1733937523	Gemini Flash 2.0 offers a significantly faster time to first token (TTFT) compared to [Gemini Flash 1.5](/google/gemini-flash-1.5), while maintaining quality on par with larger models like [Gemini Pro 1.5](/google/gemini-pro-1.5). It introduces notable enhancements in multimodal understanding, coding capabilities, complex instruction following, and function calling. These advancements come together to deliver more seamless and robust agentic experiences.	1048576	{"modality": "text+image->text", "tokenizer": "Gemini", "instruct_type": null, "input_modalities": ["text", "image"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0", "request": "0", "completion": "0", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 1048576, "max_completion_tokens": 8192}	null	["max_tokens", "response_format", "seed", "stop", "temperature", "tool_choice", "tools", "top_p"]	{}	2026-02-06
12f07ba8-6746-454d-a9e2-dca2365c2f62	2026-01-16 11:18:55.898017+00	meta-llama/llama-3.3-70b-instruct:free	meta-llama/llama-3.3-70b-instruct	meta-llama/Llama-3.3-70B-Instruct	Meta: Llama 3.3 70B Instruct (free)	1733506137	The Meta Llama 3.3 multilingual large language model (LLM) is a pretrained and instruction tuned generative model in 70B (text in/text out). The Llama 3.3 instruction tuned text only model is optimized for multilingual dialogue use cases and outperforms many of the available open source and closed chat models on common industry benchmarks.\n\nSupported languages: English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai.\n\n[Model Card](https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/MODEL_CARD.md)	131072	{"modality": "text->text", "tokenizer": "Llama3", "instruct_type": "llama3", "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0", "request": "0", "completion": "0", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 131072, "max_completion_tokens": null}	null	["frequency_penalty", "max_tokens", "presence_penalty", "repetition_penalty", "seed", "stop", "temperature", "tool_choice", "tools", "top_k", "top_p"]	{}	null
1abce99b-a627-45e0-9b8b-c084c7c8eeb5	2026-01-16 11:18:55.898017+00	meta-llama/llama-3.3-70b-instruct	meta-llama/llama-3.3-70b-instruct	meta-llama/Llama-3.3-70B-Instruct	Meta: Llama 3.3 70B Instruct	1733506137	The Meta Llama 3.3 multilingual large language model (LLM) is a pretrained and instruction tuned generative model in 70B (text in/text out). The Llama 3.3 instruction tuned text only model is optimized for multilingual dialogue use cases and outperforms many of the available open source and closed chat models on common industry benchmarks.\n\nSupported languages: English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai.\n\n[Model Card](https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/MODEL_CARD.md)	131072	{"modality": "text->text", "tokenizer": "Llama3", "instruct_type": "llama3", "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.0000001", "request": "0", "completion": "0.00000032", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 131072, "max_completion_tokens": 16384}	null	["frequency_penalty", "logit_bias", "logprobs", "max_tokens", "min_p", "presence_penalty", "repetition_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_k", "top_logprobs", "top_p"]	{}	null
8951af51-1e69-4e3d-9524-f4d1e09bdb05	2026-01-16 11:18:55.898017+00	amazon/nova-lite-v1	amazon/nova-lite-v1		Amazon: Nova Lite 1.0	1733437363	Amazon Nova Lite 1.0 is a very low-cost multimodal model from Amazon that focused on fast processing of image, video, and text inputs to generate text output. Amazon Nova Lite can handle real-time customer interactions, document analysis, and visual question-answering tasks with high accuracy.\n\nWith an input context of 300K tokens, it can analyze multiple images or up to 30 minutes of video in a single input.	300000	{"modality": "text+image->text", "tokenizer": "Nova", "instruct_type": null, "input_modalities": ["text", "image"], "output_modalities": ["text"]}	{"image": "0.00009", "prompt": "0.00000006", "request": "0", "completion": "0.00000024", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": true, "context_length": 300000, "max_completion_tokens": 5120}	null	["max_tokens", "stop", "temperature", "tools", "top_k", "top_p"]	{}	null
7ab9df3c-49f0-4923-af90-ce9067a692c5	2026-01-16 11:18:55.898017+00	amazon/nova-micro-v1	amazon/nova-micro-v1		Amazon: Nova Micro 1.0	1733437237	Amazon Nova Micro 1.0 is a text-only model that delivers the lowest latency responses in the Amazon Nova family of models at a very low cost. With a context length of 128K tokens and optimized for speed and cost, Amazon Nova Micro excels at tasks such as text summarization, translation, content classification, interactive chat, and brainstorming. It has  simple mathematical reasoning and coding abilities.	128000	{"modality": "text->text", "tokenizer": "Nova", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.000000035", "request": "0", "completion": "0.00000014", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": true, "context_length": 128000, "max_completion_tokens": 5120}	null	["max_tokens", "stop", "temperature", "tools", "top_k", "top_p"]	{}	null
8fb46690-71c0-4832-9ab6-818ddd492149	2026-01-16 11:18:55.898017+00	amazon/nova-pro-v1	amazon/nova-pro-v1		Amazon: Nova Pro 1.0	1733436303	Amazon Nova Pro 1.0 is a capable multimodal model from Amazon focused on providing a combination of accuracy, speed, and cost for a wide range of tasks. As of December 2024, it achieves state-of-the-art performance on key benchmarks including visual question answering (TextVQA) and video understanding (VATEX).\n\nAmazon Nova Pro demonstrates strong capabilities in processing both visual and textual information and at analyzing financial documents.\n\n**NOTE**: Video input is not supported at this time.	300000	{"modality": "text+image->text", "tokenizer": "Nova", "instruct_type": null, "input_modalities": ["text", "image"], "output_modalities": ["text"]}	{"image": "0.0012", "prompt": "0.0000008", "request": "0", "completion": "0.0000032", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": true, "context_length": 300000, "max_completion_tokens": 5120}	null	["max_tokens", "stop", "temperature", "tools", "top_k", "top_p"]	{}	null
4f8f5b60-d101-41f4-a565-d2757c849122	2026-01-16 11:18:55.898017+00	openai/gpt-4o-2024-11-20	openai/gpt-4o-2024-11-20		OpenAI: GPT-4o (2024-11-20)	1732127594	The 2024-11-20 version of GPT-4o offers a leveled-up creative writing ability with more natural, engaging, and tailored writing to improve relevance & readability. It’s also better at working with uploaded files, providing deeper insights & more thorough responses.\n\nGPT-4o ("o" for "omni") is OpenAI's latest AI model, supporting both text and image inputs with text outputs. It maintains the intelligence level of [GPT-4 Turbo](/models/openai/gpt-4-turbo) while being twice as fast and 50% more cost-effective. GPT-4o also offers improved performance in processing non-English languages and enhanced visual capabilities.	128000	{"modality": "text+image->text", "tokenizer": "GPT", "instruct_type": null, "input_modalities": ["text", "image", "file"], "output_modalities": ["text"]}	{"prompt": "0.0000025", "completion": "0.00001", "input_cache_read": "0.00000125"}	{"is_moderated": true, "context_length": 128000, "max_completion_tokens": 16384}	null	["frequency_penalty", "logit_bias", "logprobs", "max_tokens", "presence_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_logprobs", "top_p", "web_search_options"]	{}	null
9a4dbc8e-8416-42c8-8ef9-8e4d2c849b7d	2026-01-16 11:18:55.898017+00	mistralai/mistral-large-2411	mistralai/mistral-large-2411		Mistral Large 2411	1731978685	Mistral Large 2 2411 is an update of [Mistral Large 2](/mistralai/mistral-large) released together with [Pixtral Large 2411](/mistralai/pixtral-large-2411)\n\nIt provides a significant upgrade on the previous [Mistral Large 24.07](/mistralai/mistral-large-2407), with notable improvements in long context understanding, a new system prompt, and more accurate function calling.	131072	{"modality": "text->text", "tokenizer": "Mistral", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"prompt": "0.000002", "completion": "0.000006"}	{"is_moderated": false, "context_length": 131072, "max_completion_tokens": null}	null	["frequency_penalty", "max_tokens", "presence_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_p"]	{"temperature": 0.3}	null
06710a88-5aef-4cba-932a-99d795a474a9	2026-01-16 11:18:55.898017+00	mistralai/mistral-large-2407	mistralai/mistral-large-2407		Mistral Large 2407	1731978415	This is Mistral AI's flagship model, Mistral Large 2 (version mistral-large-2407). It's a proprietary weights-available model and excels at reasoning, code, JSON, chat, and more. Read the launch announcement [here](https://mistral.ai/news/mistral-large-2407/).\n\nIt supports dozens of languages including French, German, Spanish, Italian, Portuguese, Arabic, Hindi, Russian, Chinese, Japanese, and Korean, along with 80+ coding languages including Python, Java, C, C++, JavaScript, and Bash. Its long context window allows precise information recall from large documents.\n	131072	{"modality": "text->text", "tokenizer": "Mistral", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"prompt": "0.000002", "completion": "0.000006"}	{"is_moderated": false, "context_length": 131072, "max_completion_tokens": null}	null	["frequency_penalty", "max_tokens", "presence_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_p"]	{"temperature": 0.3}	null
9fe25871-0c33-48e1-8060-db1971384bb5	2026-01-16 11:18:55.898017+00	mistralai/pixtral-large-2411	mistralai/pixtral-large-2411		Mistral: Pixtral Large 2411	1731977388	Pixtral Large is a 124B parameter, open-weight, multimodal model built on top of [Mistral Large 2](/mistralai/mistral-large-2411). The model is able to understand documents, charts and natural images.\n\nThe model is available under the Mistral Research License (MRL) for research and educational use, and the Mistral Commercial License for experimentation, testing, and production for commercial purposes.\n\n	131072	{"modality": "text+image->text", "tokenizer": "Mistral", "instruct_type": null, "input_modalities": ["text", "image"], "output_modalities": ["text"]}	{"prompt": "0.000002", "completion": "0.000006"}	{"is_moderated": false, "context_length": 131072, "max_completion_tokens": null}	null	["frequency_penalty", "max_tokens", "presence_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_p"]	{"temperature": 0.3}	null
0c3c0a07-8f08-42bd-a493-904a6adbe688	2026-01-16 11:18:55.898017+00	qwen/qwen-2.5-coder-32b-instruct	qwen/qwen-2.5-coder-32b-instruct	Qwen/Qwen2.5-Coder-32B-Instruct	Qwen2.5 Coder 32B Instruct	1731368400	Qwen2.5-Coder is the latest series of Code-Specific Qwen large language models (formerly known as CodeQwen). Qwen2.5-Coder brings the following improvements upon CodeQwen1.5:\n\n- Significantly improvements in **code generation**, **code reasoning** and **code fixing**. \n- A more comprehensive foundation for real-world applications such as **Code Agents**. Not only enhancing coding capabilities but also maintaining its strengths in mathematics and general competencies.\n\nTo read more about its evaluation results, check out [Qwen 2.5 Coder's blog](https://qwenlm.github.io/blog/qwen2.5-coder-family/).	32768	{"modality": "text->text", "tokenizer": "Qwen", "instruct_type": "chatml", "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000003", "request": "0", "completion": "0.00000011", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 32768, "max_completion_tokens": 32768}	null	["frequency_penalty", "logit_bias", "max_tokens", "min_p", "presence_penalty", "repetition_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "top_k", "top_p"]	{}	null
1d8cdeee-f02c-4762-a55b-7a6d3d20f2d0	2026-01-16 11:18:55.898017+00	raifle/sorcererlm-8x22b	raifle/sorcererlm-8x22b	rAIfle/SorcererLM-8x22b-bf16	SorcererLM 8x22B	1731105083	SorcererLM is an advanced RP and storytelling model, built as a Low-rank 16-bit LoRA fine-tuned on [WizardLM-2 8x22B](/microsoft/wizardlm-2-8x22b).\n\n- Advanced reasoning and emotional intelligence for engaging and immersive interactions\n- Vivid writing capabilities enriched with spatial and contextual awareness\n- Enhanced narrative depth, promoting creative and dynamic storytelling	16000	{"modality": "text->text", "tokenizer": "Mistral", "instruct_type": "vicuna", "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.0000045", "request": "0", "completion": "0.0000045", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 16000, "max_completion_tokens": null}	null	["frequency_penalty", "logit_bias", "max_tokens", "min_p", "presence_penalty", "repetition_penalty", "seed", "stop", "temperature", "top_k", "top_p"]	{}	null
f7fb936e-1cba-41c8-9407-9d3461307320	2026-01-16 11:18:55.898017+00	anthropic/claude-3.5-haiku	anthropic/claude-3-5-haiku	null	Anthropic: Claude 3.5 Haiku	1730678400	Claude 3.5 Haiku features offers enhanced capabilities in speed, coding accuracy, and tool use. Engineered to excel in real-time applications, it delivers quick response times that are essential for dynamic tasks such as chat interactions and immediate coding suggestions.\n\nThis makes it highly suitable for environments that demand both speed and precision, such as software development, customer service bots, and data management systems.\n\nThis model is currently pointing to [Claude 3.5 Haiku (2024-10-22)](/anthropic/claude-3-5-haiku-20241022).	200000	{"modality": "text+image->text", "tokenizer": "Claude", "instruct_type": null, "input_modalities": ["text", "image"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.0000008", "request": "0", "completion": "0.000004", "web_search": "0", "input_cache_read": "0.00000008", "input_cache_write": "0.000001", "internal_reasoning": "0"}	{"is_moderated": true, "context_length": 200000, "max_completion_tokens": 8192}	null	["max_tokens", "stop", "temperature", "tool_choice", "tools", "top_k", "top_p"]	{"top_p": null, "temperature": null, "frequency_penalty": null}	null
907a475a-49e2-4f26-a877-5c342e9a092b	2026-01-16 11:18:55.898017+00	anthracite-org/magnum-v4-72b	anthracite-org/magnum-v4-72b	anthracite-org/magnum-v4-72b	Magnum v4 72B	1729555200	This is a series of models designed to replicate the prose quality of the Claude 3 models, specifically Sonnet(https://openrouter.ai/anthropic/claude-3.5-sonnet) and Opus(https://openrouter.ai/anthropic/claude-3-opus).\n\nThe model is fine-tuned on top of [Qwen2.5 72B](https://openrouter.ai/qwen/qwen-2.5-72b-instruct).	16384	{"modality": "text->text", "tokenizer": "Qwen", "instruct_type": "chatml", "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.000003", "request": "0", "completion": "0.000005", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 16384, "max_completion_tokens": 2048}	null	["frequency_penalty", "logit_bias", "logprobs", "max_tokens", "min_p", "presence_penalty", "repetition_penalty", "response_format", "seed", "stop", "temperature", "top_a", "top_k", "top_logprobs", "top_p"]	{}	null
eecead10-20e6-4f51-b8c1-c691de4febbe	2026-01-16 11:18:55.898017+00	anthropic/claude-3.5-sonnet	anthropic/claude-3.5-sonnet	null	Anthropic: Claude 3.5 Sonnet	1729555200	New Claude 3.5 Sonnet delivers better-than-Opus capabilities, faster-than-Sonnet speeds, at the same Sonnet prices. Sonnet is particularly good at:\n\n- Coding: Scores ~49% on SWE-Bench Verified, higher than the last best score, and without any fancy prompt scaffolding\n- Data science: Augments human data science expertise; navigates unstructured data while using multiple tools for insights\n- Visual processing: excelling at interpreting charts, graphs, and images, accurately transcribing text to derive insights beyond just the text alone\n- Agentic tasks: exceptional tool use, making it great at agentic tasks (i.e. complex, multi-step problem solving tasks that require engaging with other systems)\n\n#multimodal	200000	{"modality": "text+image->text", "tokenizer": "Claude", "instruct_type": null, "input_modalities": ["text", "image", "file"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.000006", "request": "0", "completion": "0.00003", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": true, "context_length": 200000, "max_completion_tokens": 8192}	null	["max_tokens", "stop", "temperature", "tool_choice", "tools", "top_k", "top_p"]	{}	null
4ac925c7-cb88-442d-b3fe-0c77b7d5ea15	2026-01-16 11:18:55.898017+00	mistralai/ministral-8b	mistralai/ministral-8b	null	Mistral: Ministral 8B	1729123200	Ministral 8B is an 8B parameter model featuring a unique interleaved sliding-window attention pattern for faster, memory-efficient inference. Designed for edge use cases, it supports up to 128k context length and excels in knowledge and reasoning tasks. It outperforms peers in the sub-10B category, making it perfect for low-latency, privacy-first applications.	131072	{"modality": "text->text", "tokenizer": "Mistral", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"prompt": "0.0000001", "completion": "0.0000001"}	{"is_moderated": false, "context_length": 131072, "max_completion_tokens": null}	null	["frequency_penalty", "max_tokens", "presence_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_p"]	{"temperature": 0.3}	null
30e9d2e2-cc1e-4047-b7f2-230fa7f7e624	2026-01-16 11:18:55.898017+00	mistralai/ministral-3b	mistralai/ministral-3b	null	Mistral: Ministral 3B	1729123200	Ministral 3B is a 3B parameter model optimized for on-device and edge computing. It excels in knowledge, commonsense reasoning, and function-calling, outperforming larger models like Mistral 7B on most benchmarks. Supporting up to 128k context length, it’s ideal for orchestrating agentic workflows and specialist tasks with efficient inference.	131072	{"modality": "text->text", "tokenizer": "Mistral", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"prompt": "0.00000004", "completion": "0.00000004"}	{"is_moderated": false, "context_length": 131072, "max_completion_tokens": null}	null	["frequency_penalty", "max_tokens", "presence_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_p"]	{"temperature": 0.3}	null
12e79dd0-7ae6-469f-9afe-3d28c9f0615a	2026-01-16 11:18:55.898017+00	qwen/qwen-2.5-7b-instruct	qwen/qwen-2.5-7b-instruct	Qwen/Qwen2.5-7B-Instruct	Qwen: Qwen2.5 7B Instruct	1729036800	Qwen2.5 7B is the latest series of Qwen large language models. Qwen2.5 brings the following improvements upon Qwen2:\n\n- Significantly more knowledge and has greatly improved capabilities in coding and mathematics, thanks to our specialized expert models in these domains.\n\n- Significant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON. More resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots.\n\n- Long-context Support up to 128K tokens and can generate up to 8K tokens.\n\n- Multilingual support for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more.\n\nUsage of this model is subject to [Tongyi Qianwen LICENSE AGREEMENT](https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE).	32768	{"modality": "text->text", "tokenizer": "Qwen", "instruct_type": "chatml", "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000004", "request": "0", "completion": "0.0000001", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 32768, "max_completion_tokens": null}	null	["frequency_penalty", "logit_bias", "max_tokens", "min_p", "presence_penalty", "repetition_penalty", "seed", "stop", "temperature", "tool_choice", "tools", "top_k", "top_p"]	{"top_p": null, "temperature": null, "frequency_penalty": null}	null
6e0ea475-3ca0-4c12-975a-10cef45ca97d	2026-01-16 11:18:55.898017+00	nvidia/llama-3.1-nemotron-70b-instruct	nvidia/llama-3.1-nemotron-70b-instruct	nvidia/Llama-3.1-Nemotron-70B-Instruct-HF	NVIDIA: Llama 3.1 Nemotron 70B Instruct	1728950400	NVIDIA's Llama 3.1 Nemotron 70B is a language model designed for generating precise and useful responses. Leveraging [Llama 3.1 70B](/models/meta-llama/llama-3.1-70b-instruct) architecture and Reinforcement Learning from Human Feedback (RLHF), it excels in automatic alignment benchmarks. This model is tailored for applications requiring high accuracy in helpfulness and response generation, suitable for diverse user queries across multiple domains.\n\nUsage of this model is subject to [Meta's Acceptable Use Policy](https://www.llama.com/llama3/use-policy/).	131072	{"modality": "text->text", "tokenizer": "Llama3", "instruct_type": "llama3", "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.0000012", "request": "0", "completion": "0.0000012", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 131072, "max_completion_tokens": 16384}	null	["frequency_penalty", "max_tokens", "min_p", "presence_penalty", "repetition_penalty", "response_format", "seed", "stop", "temperature", "tool_choice", "tools", "top_k", "top_p"]	{}	null
85faf710-1fab-40ec-a881-3c41b7c8da61	2026-01-16 11:18:55.898017+00	inflection/inflection-3-pi	inflection/inflection-3-pi	null	Inflection: Inflection 3 Pi	1728604800	Inflection 3 Pi powers Inflection's [Pi](https://pi.ai) chatbot, including backstory, emotional intelligence, productivity, and safety. It has access to recent news, and excels in scenarios like customer support and roleplay.\n\nPi has been trained to mirror your tone and style, if you use more emojis, so will Pi! Try experimenting with various prompts and conversation styles.	8000	{"modality": "text->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.0000025", "request": "0", "completion": "0.00001", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 8000, "max_completion_tokens": 1024}	null	["max_tokens", "stop", "temperature", "top_p"]	{}	null
fd1a01d8-9cf6-45be-b673-f6166aef9945	2026-01-16 11:18:55.898017+00	inflection/inflection-3-productivity	inflection/inflection-3-productivity	null	Inflection: Inflection 3 Productivity	1728604800	Inflection 3 Productivity is optimized for following instructions. It is better for tasks requiring JSON output or precise adherence to provided guidelines. It has access to recent news.\n\nFor emotional intelligence similar to Pi, see [Inflect 3 Pi](/inflection/inflection-3-pi)\n\nSee [Inflection's announcement](https://inflection.ai/blog/enterprise) for more details.	8000	{"modality": "text->text", "tokenizer": "Other", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.0000025", "request": "0", "completion": "0.00001", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 8000, "max_completion_tokens": 1024}	null	["max_tokens", "stop", "temperature", "top_p"]	{}	null
07d142be-3d9d-41af-b921-105741ec3029	2026-01-16 11:18:55.898017+00	thedrummer/rocinante-12b	thedrummer/rocinante-12b	TheDrummer/Rocinante-12B-v1.1	TheDrummer: Rocinante 12B	1727654400	Rocinante 12B is designed for engaging storytelling and rich prose.\n\nEarly testers have reported:\n- Expanded vocabulary with unique and expressive word choices\n- Enhanced creativity for vivid narratives\n- Adventure-filled and captivating stories	32768	{"modality": "text->text", "tokenizer": "Qwen", "instruct_type": "chatml", "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000017", "request": "0", "completion": "0.00000043", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 32768, "max_completion_tokens": null}	null	["frequency_penalty", "logit_bias", "max_tokens", "min_p", "presence_penalty", "repetition_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_k", "top_p"]	{}	null
9cb4b72b-216d-406b-9658-7754c4278327	2026-01-16 11:18:55.898017+00	meta-llama/llama-3.2-3b-instruct:free	meta-llama/llama-3.2-3b-instruct	meta-llama/Llama-3.2-3B-Instruct	Meta: Llama 3.2 3B Instruct (free)	1727222400	Llama 3.2 3B is a 3-billion-parameter multilingual large language model, optimized for advanced natural language processing tasks like dialogue generation, reasoning, and summarization. Designed with the latest transformer architecture, it supports eight languages, including English, Spanish, and Hindi, and is adaptable for additional languages.\n\nTrained on 9 trillion tokens, the Llama 3.2 3B model excels in instruction-following, complex reasoning, and tool use. Its balanced performance makes it ideal for applications needing accuracy and efficiency in text generation across multilingual settings.\n\nClick here for the [original model card](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD.md).\n\nUsage of this model is subject to [Meta's Acceptable Use Policy](https://www.llama.com/llama3/use-policy/).	131072	{"modality": "text->text", "tokenizer": "Llama3", "instruct_type": "llama3", "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0", "request": "0", "completion": "0", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 131072, "max_completion_tokens": null}	null	["frequency_penalty", "max_tokens", "presence_penalty", "stop", "temperature", "top_k", "top_p"]	{}	null
dde102bf-876e-4bef-a9bb-caef40264341	2026-01-16 11:18:55.898017+00	meta-llama/llama-3.2-3b-instruct	meta-llama/llama-3.2-3b-instruct	meta-llama/Llama-3.2-3B-Instruct	Meta: Llama 3.2 3B Instruct	1727222400	Llama 3.2 3B is a 3-billion-parameter multilingual large language model, optimized for advanced natural language processing tasks like dialogue generation, reasoning, and summarization. Designed with the latest transformer architecture, it supports eight languages, including English, Spanish, and Hindi, and is adaptable for additional languages.\n\nTrained on 9 trillion tokens, the Llama 3.2 3B model excels in instruction-following, complex reasoning, and tool use. Its balanced performance makes it ideal for applications needing accuracy and efficiency in text generation across multilingual settings.\n\nClick here for the [original model card](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD.md).\n\nUsage of this model is subject to [Meta's Acceptable Use Policy](https://www.llama.com/llama3/use-policy/).	131072	{"modality": "text->text", "tokenizer": "Llama3", "instruct_type": "llama3", "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000002", "request": "0", "completion": "0.00000002", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 131072, "max_completion_tokens": 16384}	null	["frequency_penalty", "logit_bias", "max_tokens", "min_p", "presence_penalty", "repetition_penalty", "response_format", "seed", "stop", "temperature", "top_k", "top_p"]	{}	null
318e4829-a008-428a-a605-7708a2254f70	2026-01-16 11:18:55.898017+00	meta-llama/llama-3.2-1b-instruct	meta-llama/llama-3.2-1b-instruct	meta-llama/Llama-3.2-1B-Instruct	Meta: Llama 3.2 1B Instruct	1727222400	Llama 3.2 1B is a 1-billion-parameter language model focused on efficiently performing natural language tasks, such as summarization, dialogue, and multilingual text analysis. Its smaller size allows it to operate efficiently in low-resource environments while maintaining strong task performance.\n\nSupporting eight core languages and fine-tunable for more, Llama 1.3B is ideal for businesses or developers seeking lightweight yet powerful AI solutions that can operate in diverse multilingual settings without the high computational demand of larger models.\n\nClick here for the [original model card](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD.md).\n\nUsage of this model is subject to [Meta's Acceptable Use Policy](https://www.llama.com/llama3/use-policy/).	60000	{"modality": "text->text", "tokenizer": "Llama3", "instruct_type": "llama3", "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.000000027", "request": "0", "completion": "0.0000002", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 60000, "max_completion_tokens": null}	null	["frequency_penalty", "max_tokens", "presence_penalty", "repetition_penalty", "seed", "temperature", "top_k", "top_p"]	{}	null
e47a9e76-7f05-4eab-998a-8006b577a605	2026-01-16 11:18:55.898017+00	meta-llama/llama-3.2-11b-vision-instruct	meta-llama/llama-3.2-11b-vision-instruct	meta-llama/Llama-3.2-11B-Vision-Instruct	Meta: Llama 3.2 11B Vision Instruct	1727222400	Llama 3.2 11B Vision is a multimodal model with 11 billion parameters, designed to handle tasks combining visual and textual data. It excels in tasks such as image captioning and visual question answering, bridging the gap between language generation and visual reasoning. Pre-trained on a massive dataset of image-text pairs, it performs well in complex, high-accuracy image analysis.\n\nIts ability to integrate visual understanding with language processing makes it an ideal solution for industries requiring comprehensive visual-linguistic AI applications, such as content creation, AI-driven customer service, and research.\n\nClick here for the [original model card](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD_VISION.md).\n\nUsage of this model is subject to [Meta's Acceptable Use Policy](https://www.llama.com/llama3/use-policy/).	131072	{"modality": "text+image->text", "tokenizer": "Llama3", "instruct_type": "llama3", "input_modalities": ["text", "image"], "output_modalities": ["text"]}	{"image": "0.00007948", "prompt": "0.000000049", "request": "0", "completion": "0.000000049", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 131072, "max_completion_tokens": 16384}	null	["frequency_penalty", "logit_bias", "max_tokens", "min_p", "presence_penalty", "repetition_penalty", "response_format", "seed", "stop", "temperature", "top_k", "top_p"]	{}	null
4a1767ab-c36f-4138-8fb1-4218a4c9e3a2	2026-01-16 11:18:55.898017+00	qwen/qwen-2.5-72b-instruct	qwen/qwen-2.5-72b-instruct	Qwen/Qwen2.5-72B-Instruct	Qwen2.5 72B Instruct	1726704000	Qwen2.5 72B is the latest series of Qwen large language models. Qwen2.5 brings the following improvements upon Qwen2:\n\n- Significantly more knowledge and has greatly improved capabilities in coding and mathematics, thanks to our specialized expert models in these domains.\n\n- Significant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON. More resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots.\n\n- Long-context Support up to 128K tokens and can generate up to 8K tokens.\n\n- Multilingual support for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more.\n\nUsage of this model is subject to [Tongyi Qianwen LICENSE AGREEMENT](https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE).	32768	{"modality": "text->text", "tokenizer": "Qwen", "instruct_type": "chatml", "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000012", "request": "0", "completion": "0.00000039", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 32768, "max_completion_tokens": 16384}	null	["frequency_penalty", "logit_bias", "max_tokens", "min_p", "presence_penalty", "repetition_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_k", "top_p"]	{}	null
1bdac9ae-93b5-4fea-b6d2-e912ba42d4fc	2026-01-16 11:18:55.898017+00	neversleep/llama-3.1-lumimaid-8b	neversleep/llama-3.1-lumimaid-8b	NeverSleep/Lumimaid-v0.2-8B	NeverSleep: Lumimaid v0.2 8B	1726358400	Lumimaid v0.2 8B is a finetune of [Llama 3.1 8B](/models/meta-llama/llama-3.1-8b-instruct) with a "HUGE step up dataset wise" compared to Lumimaid v0.1. Sloppy chats output were purged.\n\nUsage of this model is subject to [Meta's Acceptable Use Policy](https://llama.meta.com/llama3/use-policy/).	32768	{"modality": "text->text", "tokenizer": "Llama3", "instruct_type": "llama3", "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000009", "request": "0", "completion": "0.0000006", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 32768, "max_completion_tokens": null}	null	["frequency_penalty", "max_tokens", "presence_penalty", "response_format", "stop", "structured_outputs", "temperature", "top_p"]	{}	null
d234b1d2-0a71-4b9b-bc13-ad3261660bb4	2026-01-16 11:18:55.898017+00	mistralai/pixtral-12b	mistralai/pixtral-12b	mistralai/Pixtral-12B-2409	Mistral: Pixtral 12B	1725926400	The first multi-modal, text+image-to-text model from Mistral AI. Its weights were launched via torrent: https://x.com/mistralai/status/1833758285167722836.	32768	{"modality": "text+image->text", "tokenizer": "Mistral", "instruct_type": null, "input_modalities": ["text", "image"], "output_modalities": ["text"]}	{"image": "0.0001445", "prompt": "0.0000001", "request": "0", "completion": "0.0000001", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 32768, "max_completion_tokens": null}	null	["frequency_penalty", "logit_bias", "max_tokens", "min_p", "presence_penalty", "repetition_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_k", "top_p"]	{"temperature": 0.3}	null
ec5d2a26-9996-4466-a355-e9acd7a092f3	2026-01-16 11:18:55.898017+00	cohere/command-r-08-2024	cohere/command-r-08-2024	null	Cohere: Command R (08-2024)	1724976000	command-r-08-2024 is an update of the [Command R](/models/cohere/command-r) with improved performance for multilingual retrieval-augmented generation (RAG) and tool use. More broadly, it is better at math, code and reasoning and is competitive with the previous version of the larger Command R+ model.\n\nRead the launch post [here](https://docs.cohere.com/changelog/command-gets-refreshed).\n\nUse of this model is subject to Cohere's [Usage Policy](https://docs.cohere.com/docs/usage-policy) and [SaaS Agreement](https://cohere.com/saas-agreement).	128000	{"modality": "text->text", "tokenizer": "Cohere", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000015", "request": "0", "completion": "0.0000006", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": true, "context_length": 128000, "max_completion_tokens": 4000}	null	["frequency_penalty", "max_tokens", "presence_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_k", "top_p"]	{}	null
5bf0cd5d-f108-4dac-abd8-aaf8a78cb74b	2026-01-16 11:18:55.898017+00	cohere/command-r-plus-08-2024	cohere/command-r-plus-08-2024	null	Cohere: Command R+ (08-2024)	1724976000	command-r-plus-08-2024 is an update of the [Command R+](/models/cohere/command-r-plus) with roughly 50% higher throughput and 25% lower latencies as compared to the previous Command R+ version, while keeping the hardware footprint the same.\n\nRead the launch post [here](https://docs.cohere.com/changelog/command-gets-refreshed).\n\nUse of this model is subject to Cohere's [Usage Policy](https://docs.cohere.com/docs/usage-policy) and [SaaS Agreement](https://cohere.com/saas-agreement).	128000	{"modality": "text->text", "tokenizer": "Cohere", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.0000025", "request": "0", "completion": "0.00001", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": true, "context_length": 128000, "max_completion_tokens": 4000}	null	["frequency_penalty", "max_tokens", "presence_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_k", "top_p"]	{}	null
c98760f3-05d8-4139-bad4-eced92604587	2026-01-16 11:18:55.898017+00	sao10k/l3.1-euryale-70b	sao10k/l3.1-euryale-70b	Sao10K/L3.1-70B-Euryale-v2.2	Sao10K: Llama 3.1 Euryale 70B v2.2	1724803200	Euryale L3.1 70B v2.2 is a model focused on creative roleplay from [Sao10k](https://ko-fi.com/sao10k). It is the successor of [Euryale L3 70B v2.1](/models/sao10k/l3-euryale-70b).	32768	{"modality": "text->text", "tokenizer": "Llama3", "instruct_type": "llama3", "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000065", "request": "0", "completion": "0.00000075", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 32768, "max_completion_tokens": null}	null	["frequency_penalty", "max_tokens", "min_p", "presence_penalty", "repetition_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_k", "top_p"]	{}	null
88cac4cd-7209-4af8-b3ee-2e2fb3c12bc4	2026-01-16 11:18:55.898017+00	qwen/qwen-2.5-vl-7b-instruct:free	qwen/qwen-2-vl-7b-instruct	Qwen/Qwen2.5-VL-7B-Instruct	Qwen: Qwen2.5-VL 7B Instruct (free)	1724803200	Qwen2.5 VL 7B is a multimodal LLM from the Qwen Team with the following key enhancements:\n\n- SoTA understanding of images of various resolution & ratio: Qwen2.5-VL achieves state-of-the-art performance on visual understanding benchmarks, including MathVista, DocVQA, RealWorldQA, MTVQA, etc.\n\n- Understanding videos of 20min+: Qwen2.5-VL can understand videos over 20 minutes for high-quality video-based question answering, dialog, content creation, etc.\n\n- Agent that can operate your mobiles, robots, etc.: with the abilities of complex reasoning and decision making, Qwen2.5-VL can be integrated with devices like mobile phones, robots, etc., for automatic operation based on visual environment and text instructions.\n\n- Multilingual Support: to serve global users, besides English and Chinese, Qwen2.5-VL now supports the understanding of texts in different languages inside images, including most European languages, Japanese, Korean, Arabic, Vietnamese, etc.\n\nFor more details, see this [blog post](https://qwenlm.github.io/blog/qwen2-vl/) and [GitHub repo](https://github.com/QwenLM/Qwen2-VL).\n\nUsage of this model is subject to [Tongyi Qianwen LICENSE AGREEMENT](https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE).	32768	{"modality": "text+image->text", "tokenizer": "Qwen", "instruct_type": null, "input_modalities": ["text", "image"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0", "request": "0", "completion": "0", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 32768, "max_completion_tokens": null}	null	["frequency_penalty", "max_tokens", "presence_penalty", "repetition_penalty", "temperature"]	{}	null
f9941375-f9fc-4645-8763-b9c5efdc9a3d	2026-01-16 11:18:55.898017+00	qwen/qwen-2.5-vl-7b-instruct	qwen/qwen-2-vl-7b-instruct	Qwen/Qwen2.5-VL-7B-Instruct	Qwen: Qwen2.5-VL 7B Instruct	1724803200	Qwen2.5 VL 7B is a multimodal LLM from the Qwen Team with the following key enhancements:\n\n- SoTA understanding of images of various resolution & ratio: Qwen2.5-VL achieves state-of-the-art performance on visual understanding benchmarks, including MathVista, DocVQA, RealWorldQA, MTVQA, etc.\n\n- Understanding videos of 20min+: Qwen2.5-VL can understand videos over 20 minutes for high-quality video-based question answering, dialog, content creation, etc.\n\n- Agent that can operate your mobiles, robots, etc.: with the abilities of complex reasoning and decision making, Qwen2.5-VL can be integrated with devices like mobile phones, robots, etc., for automatic operation based on visual environment and text instructions.\n\n- Multilingual Support: to serve global users, besides English and Chinese, Qwen2.5-VL now supports the understanding of texts in different languages inside images, including most European languages, Japanese, Korean, Arabic, Vietnamese, etc.\n\nFor more details, see this [blog post](https://qwenlm.github.io/blog/qwen2-vl/) and [GitHub repo](https://github.com/QwenLM/Qwen2-VL).\n\nUsage of this model is subject to [Tongyi Qianwen LICENSE AGREEMENT](https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE).	32768	{"modality": "text+image->text", "tokenizer": "Qwen", "instruct_type": null, "input_modalities": ["text", "image"], "output_modalities": ["text"]}	{"image": "0.0001445", "prompt": "0.0000002", "request": "0", "completion": "0.0000002", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 32768, "max_completion_tokens": null}	null	["frequency_penalty", "logit_bias", "max_tokens", "min_p", "presence_penalty", "repetition_penalty", "seed", "stop", "temperature", "top_k", "top_p"]	{}	null
115623e0-9c11-4f70-9bb5-f4676d4e5419	2026-01-16 11:18:55.898017+00	nousresearch/hermes-3-llama-3.1-70b	nousresearch/hermes-3-llama-3.1-70b	NousResearch/Hermes-3-Llama-3.1-70B	Nous: Hermes 3 70B Instruct	1723939200	Hermes 3 is a generalist language model with many improvements over [Hermes 2](/models/nousresearch/nous-hermes-2-mistral-7b-dpo), including advanced agentic capabilities, much better roleplaying, reasoning, multi-turn conversation, long context coherence, and improvements across the board.\n\nHermes 3 70B is a competitive, if not superior finetune of the [Llama-3.1 70B foundation model](/models/meta-llama/llama-3.1-70b-instruct), focused on aligning LLMs to the user, with powerful steering capabilities and control given to the end user.\n\nThe Hermes 3 series builds and expands on the Hermes 2 set of capabilities, including more powerful and reliable function calling and structured output capabilities, generalist assistant capabilities, and improved code generation skills.	65536	{"modality": "text->text", "tokenizer": "Llama3", "instruct_type": "chatml", "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.0000003", "request": "0", "completion": "0.0000003", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 65536, "max_completion_tokens": null}	null	["frequency_penalty", "max_tokens", "min_p", "presence_penalty", "repetition_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "top_k", "top_p"]	{}	null
dc8fdb79-629b-4fc8-8283-087a2975ecea	2026-01-16 11:18:55.898017+00	nousresearch/hermes-3-llama-3.1-405b:free	nousresearch/hermes-3-llama-3.1-405b	NousResearch/Hermes-3-Llama-3.1-405B	Nous: Hermes 3 405B Instruct (free)	1723766400	Hermes 3 is a generalist language model with many improvements over Hermes 2, including advanced agentic capabilities, much better roleplaying, reasoning, multi-turn conversation, long context coherence, and improvements across the board.\n\nHermes 3 405B is a frontier-level, full-parameter finetune of the Llama-3.1 405B foundation model, focused on aligning LLMs to the user, with powerful steering capabilities and control given to the end user.\n\nThe Hermes 3 series builds and expands on the Hermes 2 set of capabilities, including more powerful and reliable function calling and structured output capabilities, generalist assistant capabilities, and improved code generation skills.\n\nHermes 3 is competitive, if not superior, to Llama-3.1 Instruct models at general capabilities, with varying strengths and weaknesses attributable between the two.	131072	{"modality": "text->text", "tokenizer": "Llama3", "instruct_type": "chatml", "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0", "request": "0", "completion": "0", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 131072, "max_completion_tokens": null}	null	["frequency_penalty", "max_tokens", "presence_penalty", "stop", "temperature", "top_k", "top_p"]	{}	null
b689e46a-2dc9-4801-8317-e96124bcb56c	2026-01-16 11:18:55.898017+00	nousresearch/hermes-3-llama-3.1-405b	nousresearch/hermes-3-llama-3.1-405b	NousResearch/Hermes-3-Llama-3.1-405B	Nous: Hermes 3 405B Instruct	1723766400	Hermes 3 is a generalist language model with many improvements over Hermes 2, including advanced agentic capabilities, much better roleplaying, reasoning, multi-turn conversation, long context coherence, and improvements across the board.\n\nHermes 3 405B is a frontier-level, full-parameter finetune of the Llama-3.1 405B foundation model, focused on aligning LLMs to the user, with powerful steering capabilities and control given to the end user.\n\nThe Hermes 3 series builds and expands on the Hermes 2 set of capabilities, including more powerful and reliable function calling and structured output capabilities, generalist assistant capabilities, and improved code generation skills.\n\nHermes 3 is competitive, if not superior, to Llama-3.1 Instruct models at general capabilities, with varying strengths and weaknesses attributable between the two.	131072	{"modality": "text->text", "tokenizer": "Llama3", "instruct_type": "chatml", "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.000001", "request": "0", "completion": "0.000001", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 131072, "max_completion_tokens": 16384}	null	["frequency_penalty", "max_tokens", "min_p", "presence_penalty", "repetition_penalty", "response_format", "seed", "stop", "temperature", "top_k", "top_p"]	{}	null
21d5eda4-0490-489f-af65-9ca6be292f25	2026-01-16 11:18:55.898017+00	openai/chatgpt-4o-latest	openai/chatgpt-4o-latest	null	OpenAI: ChatGPT-4o	1723593600	OpenAI ChatGPT 4o is continually updated by OpenAI to point to the current version of GPT-4o used by ChatGPT. It therefore differs slightly from the API version of [GPT-4o](/models/openai/gpt-4o) in that it has additional RLHF. It is intended for research and evaluation.\n\nOpenAI notes that this model is not suited for production use-cases as it may be removed or redirected to another model in the future.	128000	{"modality": "text+image->text", "tokenizer": "GPT", "instruct_type": null, "input_modalities": ["text", "image"], "output_modalities": ["text"]}	{"prompt": "0.000005", "completion": "0.000015"}	{"is_moderated": true, "context_length": 128000, "max_completion_tokens": 16384}	null	["frequency_penalty", "logit_bias", "logprobs", "max_tokens", "presence_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "top_logprobs", "top_p"]	{}	null
83d8d788-4a22-43e8-9277-a99c67fe2502	2026-01-16 11:18:55.898017+00	sao10k/l3-lunaris-8b	sao10k/l3-lunaris-8b	Sao10K/L3-8B-Lunaris-v1	Sao10K: Llama 3 8B Lunaris	1723507200	Lunaris 8B is a versatile generalist and roleplaying model based on Llama 3. It's a strategic merge of multiple models, designed to balance creativity with improved logic and general knowledge.\n\nCreated by [Sao10k](https://huggingface.co/Sao10k), this model aims to offer an improved experience over Stheno v3.2, with enhanced creativity and logical reasoning.\n\nFor best results, use with Llama 3 Instruct context template, temperature 1.4, and min_p 0.1.	8192	{"modality": "text->text", "tokenizer": "Llama3", "instruct_type": "llama3", "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000004", "request": "0", "completion": "0.00000005", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 8192, "max_completion_tokens": null}	null	["frequency_penalty", "max_tokens", "min_p", "presence_penalty", "repetition_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "top_k", "top_p"]	{}	null
02decaff-e57a-4231-9e15-4c204accf525	2026-01-16 11:18:55.898017+00	openai/gpt-4o-2024-08-06	openai/gpt-4o-2024-08-06	null	OpenAI: GPT-4o (2024-08-06)	1722902400	The 2024-08-06 version of GPT-4o offers improved performance in structured outputs, with the ability to supply a JSON schema in the respone_format. Read more [here](https://openai.com/index/introducing-structured-outputs-in-the-api/).\n\nGPT-4o ("o" for "omni") is OpenAI's latest AI model, supporting both text and image inputs with text outputs. It maintains the intelligence level of [GPT-4 Turbo](/models/openai/gpt-4-turbo) while being twice as fast and 50% more cost-effective. GPT-4o also offers improved performance in processing non-English languages and enhanced visual capabilities.\n\nFor benchmarking against other models, it was briefly called ["im-also-a-good-gpt2-chatbot"](https://twitter.com/LiamFedus/status/1790064963966370209)	128000	{"modality": "text+image->text", "tokenizer": "GPT", "instruct_type": null, "input_modalities": ["text", "image", "file"], "output_modalities": ["text"]}	{"image": "0.003613", "prompt": "0.0000025", "request": "0", "completion": "0.00001", "web_search": "0", "input_cache_read": "0.00000125", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 128000, "max_completion_tokens": 16384}	null	["frequency_penalty", "logit_bias", "logprobs", "max_tokens", "presence_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_logprobs", "top_p", "web_search_options"]	{}	null
01b9ab14-2a73-4238-9ffb-7237fa26df0b	2026-01-16 11:18:55.898017+00	meta-llama/llama-3.1-405b	meta-llama/llama-3.1-405b	meta-llama/llama-3.1-405B	Meta: Llama 3.1 405B (base)	1722556800	Meta's latest class of model (Llama 3.1) launched with a variety of sizes & flavors. This is the base 405B pre-trained version.\n\nIt has demonstrated strong performance compared to leading closed-source models in human evaluations.\n\nTo read more about the model release, [click here](https://ai.meta.com/blog/meta-llama-3/). Usage of this model is subject to [Meta's Acceptable Use Policy](https://llama.meta.com/llama3/use-policy/).	32768	{"modality": "text->text", "tokenizer": "Llama3", "instruct_type": "none", "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.000004", "request": "0", "completion": "0.000004", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 32768, "max_completion_tokens": 32768}	null	["frequency_penalty", "logit_bias", "max_tokens", "min_p", "presence_penalty", "repetition_penalty", "seed", "stop", "temperature", "top_k", "top_p"]	{}	null
885ceef3-3fbf-4ec6-83b4-c5e238845fa7	2026-01-16 11:18:55.898017+00	meta-llama/llama-3.1-8b-instruct	meta-llama/llama-3.1-8b-instruct	meta-llama/Meta-Llama-3.1-8B-Instruct	Meta: Llama 3.1 8B Instruct	1721692800	Meta's latest class of model (Llama 3.1) launched with a variety of sizes & flavors. This 8B instruct-tuned version is fast and efficient.\n\nIt has demonstrated strong performance compared to leading closed-source models in human evaluations.\n\nTo read more about the model release, [click here](https://ai.meta.com/blog/meta-llama-3-1/). Usage of this model is subject to [Meta's Acceptable Use Policy](https://llama.meta.com/llama3/use-policy/).	16384	{"modality": "text->text", "tokenizer": "Llama3", "instruct_type": "llama3", "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000002", "request": "0", "completion": "0.00000005", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 16384, "max_completion_tokens": 16384}	null	["frequency_penalty", "logit_bias", "logprobs", "max_tokens", "min_p", "presence_penalty", "repetition_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_k", "top_logprobs", "top_p"]	{}	null
fae3ba1e-e6d6-46b8-94be-cf3f2b1110c7	2026-01-16 11:18:55.898017+00	meta-llama/llama-3.1-405b-instruct:free	meta-llama/llama-3.1-405b-instruct	meta-llama/Meta-Llama-3.1-405B-Instruct	Meta: Llama 3.1 405B Instruct (free)	1721692800	The highly anticipated 400B class of Llama3 is here! Clocking in at 128k context with impressive eval scores, the Meta AI team continues to push the frontier of open-source LLMs.\n\nMeta's latest class of model (Llama 3.1) launched with a variety of sizes & flavors. This 405B instruct-tuned version is optimized for high quality dialogue usecases.\n\nIt has demonstrated strong performance compared to leading closed-source models including GPT-4o and Claude 3.5 Sonnet in evaluations.\n\nTo read more about the model release, [click here](https://ai.meta.com/blog/meta-llama-3-1/). Usage of this model is subject to [Meta's Acceptable Use Policy](https://llama.meta.com/llama3/use-policy/).	131072	{"modality": "text->text", "tokenizer": "Llama3", "instruct_type": "llama3", "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0", "request": "0", "completion": "0", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 131072, "max_completion_tokens": null}	null	["frequency_penalty", "max_tokens", "presence_penalty", "repetition_penalty", "temperature"]	{}	null
7bca72f2-7e13-45c1-91be-6194753fdb51	2026-01-16 11:18:55.898017+00	meta-llama/llama-3.1-405b-instruct	meta-llama/llama-3.1-405b-instruct	meta-llama/Meta-Llama-3.1-405B-Instruct	Meta: Llama 3.1 405B Instruct	1721692800	The highly anticipated 400B class of Llama3 is here! Clocking in at 128k context with impressive eval scores, the Meta AI team continues to push the frontier of open-source LLMs.\n\nMeta's latest class of model (Llama 3.1) launched with a variety of sizes & flavors. This 405B instruct-tuned version is optimized for high quality dialogue usecases.\n\nIt has demonstrated strong performance compared to leading closed-source models including GPT-4o and Claude 3.5 Sonnet in evaluations.\n\nTo read more about the model release, [click here](https://ai.meta.com/blog/meta-llama-3-1/). Usage of this model is subject to [Meta's Acceptable Use Policy](https://llama.meta.com/llama3/use-policy/).	10000	{"modality": "text->text", "tokenizer": "Llama3", "instruct_type": "llama3", "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.0000035", "request": "0", "completion": "0.0000035", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 10000, "max_completion_tokens": null}	null	["frequency_penalty", "logit_bias", "max_tokens", "min_p", "presence_penalty", "repetition_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_k", "top_p"]	{}	null
60638689-dfa5-495e-9da5-7ba53e7f856a	2026-01-16 11:18:55.898017+00	meta-llama/llama-3.1-70b-instruct	meta-llama/llama-3.1-70b-instruct	meta-llama/Meta-Llama-3.1-70B-Instruct	Meta: Llama 3.1 70B Instruct	1721692800	Meta's latest class of model (Llama 3.1) launched with a variety of sizes & flavors. This 70B instruct-tuned version is optimized for high quality dialogue usecases.\n\nIt has demonstrated strong performance compared to leading closed-source models in human evaluations.\n\nTo read more about the model release, [click here](https://ai.meta.com/blog/meta-llama-3-1/). Usage of this model is subject to [Meta's Acceptable Use Policy](https://llama.meta.com/llama3/use-policy/).	131072	{"modality": "text->text", "tokenizer": "Llama3", "instruct_type": "llama3", "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.0000004", "request": "0", "completion": "0.0000004", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 131072, "max_completion_tokens": null}	null	["frequency_penalty", "logit_bias", "max_tokens", "min_p", "presence_penalty", "repetition_penalty", "response_format", "seed", "stop", "temperature", "tool_choice", "tools", "top_k", "top_p"]	{}	null
967985f1-5d70-455f-b0de-8cfa51433417	2026-01-16 11:18:55.898017+00	mistralai/mistral-nemo	mistralai/mistral-nemo	mistralai/Mistral-Nemo-Instruct-2407	Mistral: Mistral Nemo	1721347200	A 12B parameter model with a 128k token context length built by Mistral in collaboration with NVIDIA.\n\nThe model is multilingual, supporting English, French, German, Spanish, Italian, Portuguese, Chinese, Japanese, Korean, Arabic, and Hindi.\n\nIt supports function calling and is released under the Apache 2.0 license.	131072	{"modality": "text->text", "tokenizer": "Mistral", "instruct_type": "mistral", "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000002", "request": "0", "completion": "0.00000004", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 131072, "max_completion_tokens": 131072}	null	["frequency_penalty", "max_tokens", "presence_penalty", "repetition_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_k", "top_p"]	{"temperature": 0.3}	null
e1c3c1f3-d014-4d36-9e3d-34de4bd452b0	2026-01-16 11:18:55.898017+00	openai/gpt-4o-mini-2024-07-18	openai/gpt-4o-mini-2024-07-18	null	OpenAI: GPT-4o-mini (2024-07-18)	1721260800	GPT-4o mini is OpenAI's newest model after [GPT-4 Omni](/models/openai/gpt-4o), supporting both text and image inputs with text outputs.\n\nAs their most advanced small model, it is many multiples more affordable than other recent frontier models, and more than 60% cheaper than [GPT-3.5 Turbo](/models/openai/gpt-3.5-turbo). It maintains SOTA intelligence, while being significantly more cost-effective.\n\nGPT-4o mini achieves an 82% score on MMLU and presently ranks higher than GPT-4 on chat preferences [common leaderboards](https://arena.lmsys.org/).\n\nCheck out the [launch announcement](https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/) to learn more.\n\n#multimodal	128000	{"modality": "text+image->text", "tokenizer": "GPT", "instruct_type": null, "input_modalities": ["text", "image", "file"], "output_modalities": ["text"]}	{"prompt": "0.00000015", "completion": "0.0000006", "input_cache_read": "0.000000075"}	{"is_moderated": true, "context_length": 128000, "max_completion_tokens": 16384}	null	["frequency_penalty", "logit_bias", "logprobs", "max_tokens", "presence_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_logprobs", "top_p", "web_search_options"]	{}	null
d1400a59-f9f8-4804-9f7f-cf5f4052d6c3	2026-01-16 11:18:55.898017+00	openai/gpt-4o-mini	openai/gpt-4o-mini	null	OpenAI: GPT-4o-mini	1721260800	GPT-4o mini is OpenAI's newest model after [GPT-4 Omni](/models/openai/gpt-4o), supporting both text and image inputs with text outputs.\n\nAs their most advanced small model, it is many multiples more affordable than other recent frontier models, and more than 60% cheaper than [GPT-3.5 Turbo](/models/openai/gpt-3.5-turbo). It maintains SOTA intelligence, while being significantly more cost-effective.\n\nGPT-4o mini achieves an 82% score on MMLU and presently ranks higher than GPT-4 on chat preferences [common leaderboards](https://arena.lmsys.org/).\n\nCheck out the [launch announcement](https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/) to learn more.\n\n#multimodal	128000	{"modality": "text+image->text", "tokenizer": "GPT", "instruct_type": null, "input_modalities": ["text", "image", "file"], "output_modalities": ["text"]}	{"prompt": "0.00000015", "completion": "0.0000006", "input_cache_read": "0.000000075"}	{"is_moderated": true, "context_length": 128000, "max_completion_tokens": 16384}	null	["frequency_penalty", "logit_bias", "logprobs", "max_tokens", "presence_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_logprobs", "top_p", "web_search_options"]	{}	null
d8caae08-9734-4ed6-b594-e0ae8c2b9477	2026-01-16 11:18:55.898017+00	google/gemma-2-27b-it	google/gemma-2-27b-it	google/gemma-2-27b-it	Google: Gemma 2 27B	1720828800	Gemma 2 27B by Google is an open model built from the same research and technology used to create the [Gemini models](/models?q=gemini).\n\nGemma models are well-suited for a variety of text generation tasks, including question answering, summarization, and reasoning.\n\nSee the [launch announcement](https://blog.google/technology/developers/google-gemma-2/) for more details. Usage of Gemma is subject to Google's [Gemma Terms of Use](https://ai.google.dev/gemma/terms).	8192	{"modality": "text->text", "tokenizer": "Gemini", "instruct_type": "gemma", "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000065", "request": "0", "completion": "0.00000065", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 8192, "max_completion_tokens": null}	null	["frequency_penalty", "max_tokens", "presence_penalty", "response_format", "stop", "structured_outputs", "temperature", "top_p"]	{}	null
3b6be7ce-fbdf-4678-8e2c-1fe0a15a8f44	2026-01-16 11:18:55.898017+00	google/gemma-2-9b-it	google/gemma-2-9b-it	google/gemma-2-9b-it	Google: Gemma 2 9B	1719532800	Gemma 2 9B by Google is an advanced, open-source language model that sets a new standard for efficiency and performance in its size class.\n\nDesigned for a wide variety of tasks, it empowers developers and researchers to build innovative applications, while maintaining accessibility, safety, and cost-effectiveness.\n\nSee the [launch announcement](https://blog.google/technology/developers/google-gemma-2/) for more details. Usage of Gemma is subject to Google's [Gemma Terms of Use](https://ai.google.dev/gemma/terms).	8192	{"modality": "text->text", "tokenizer": "Gemini", "instruct_type": "gemma", "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000003", "request": "0", "completion": "0.00000009", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 8192, "max_completion_tokens": null}	null	["frequency_penalty", "max_tokens", "presence_penalty", "repetition_penalty", "temperature", "top_k", "top_p"]	{}	null
0ffa56a7-747e-4857-8d85-04c5b411cce1	2026-01-16 11:18:55.898017+00	sao10k/l3-euryale-70b	sao10k/l3-euryale-70b	Sao10K/L3-70B-Euryale-v2.1	Sao10k: Llama 3 Euryale 70B v2.1	1718668800	Euryale 70B v2.1 is a model focused on creative roleplay from [Sao10k](https://ko-fi.com/sao10k).\n\n- Better prompt adherence.\n- Better anatomy / spatial awareness.\n- Adapts much better to unique and custom formatting / reply formats.\n- Very creative, lots of unique swipes.\n- Is not restrictive during roleplays.	8192	{"modality": "text->text", "tokenizer": "Llama3", "instruct_type": "llama3", "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000148", "request": "0", "completion": "0.00000148", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 8192, "max_completion_tokens": 8192}	null	["frequency_penalty", "max_tokens", "presence_penalty", "repetition_penalty", "seed", "stop", "temperature", "tool_choice", "tools", "top_k", "top_p"]	{}	null
9719c629-fda7-4278-a016-39936d6c25cc	2026-01-16 11:18:55.898017+00	nousresearch/hermes-2-pro-llama-3-8b	nousresearch/hermes-2-pro-llama-3-8b	NousResearch/Hermes-2-Pro-Llama-3-8B	NousResearch: Hermes 2 Pro - Llama-3 8B	1716768000	Hermes 2 Pro is an upgraded, retrained version of Nous Hermes 2, consisting of an updated and cleaned version of the OpenHermes 2.5 Dataset, as well as a newly introduced Function Calling and JSON Mode dataset developed in-house.	8192	{"modality": "text->text", "tokenizer": "Llama3", "instruct_type": "chatml", "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.000000025", "request": "0", "completion": "0.00000008", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 8192, "max_completion_tokens": 2048}	null	["frequency_penalty", "max_tokens", "presence_penalty", "repetition_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "top_k", "top_p"]	{}	null
38fa6c69-669f-43d5-b058-9fac21e95918	2026-01-16 11:18:55.898017+00	mistralai/mistral-7b-instruct	mistralai/mistral-7b-instruct	mistralai/Mistral-7B-Instruct-v0.3	Mistral: Mistral 7B Instruct	1716768000	A high-performing, industry-standard 7.3B parameter model, with optimizations for speed and context length.\n\n*Mistral 7B Instruct has multiple version variants, and this is intended to be the latest version.*	32768	{"modality": "text->text", "tokenizer": "Mistral", "instruct_type": "mistral", "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.0000002", "request": "0", "completion": "0.0000002", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 32768, "max_completion_tokens": 4096}	null	["frequency_penalty", "logit_bias", "max_tokens", "min_p", "presence_penalty", "repetition_penalty", "stop", "temperature", "top_k", "top_p"]	{"temperature": 0.3}	null
f87e5851-f5cf-4698-be4a-dfa2ce5db114	2026-01-16 11:18:55.898017+00	mistralai/mistral-7b-instruct-v0.3	mistralai/mistral-7b-instruct-v0.3	mistralai/Mistral-7B-Instruct-v0.3	Mistral: Mistral 7B Instruct v0.3	1716768000	A high-performing, industry-standard 7.3B parameter model, with optimizations for speed and context length.\n\nAn improved version of [Mistral 7B Instruct v0.2](/models/mistralai/mistral-7b-instruct-v0.2), with the following changes:\n\n- Extended vocabulary to 32768\n- Supports v3 Tokenizer\n- Supports function calling\n\nNOTE: Support for function calling depends on the provider.	32768	{"modality": "text->text", "tokenizer": "Mistral", "instruct_type": "mistral", "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.0000002", "request": "0", "completion": "0.0000002", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 32768, "max_completion_tokens": 4096}	null	["frequency_penalty", "logit_bias", "max_tokens", "min_p", "presence_penalty", "repetition_penalty", "stop", "temperature", "top_k", "top_p"]	{"temperature": 0.3}	null
8d8565a2-0811-4375-9e92-209a07f2e6cd	2026-01-16 11:18:55.898017+00	meta-llama/llama-guard-2-8b	meta-llama/llama-guard-2-8b	meta-llama/Meta-Llama-Guard-2-8B	Meta: LlamaGuard 2 8B	1715558400	This safeguard model has 8B parameters and is based on the Llama 3 family. Just like is predecessor, [LlamaGuard 1](https://huggingface.co/meta-llama/LlamaGuard-7b), it can do both prompt and response classification.\n\nLlamaGuard 2 acts as a normal LLM would, generating text that indicates whether the given input/output is safe/unsafe. If deemed unsafe, it will also share the content categories violated.\n\nFor best results, please use raw prompt input or the `/completions` endpoint, instead of the chat API.\n\nIt has demonstrated strong performance compared to leading closed-source models in human evaluations.\n\nTo read more about the model release, [click here](https://ai.meta.com/blog/meta-llama-3/). Usage of this model is subject to [Meta's Acceptable Use Policy](https://llama.meta.com/llama3/use-policy/).	8192	{"modality": "text->text", "tokenizer": "Llama3", "instruct_type": "none", "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.0000002", "request": "0", "completion": "0.0000002", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 8192, "max_completion_tokens": null}	null	["frequency_penalty", "logit_bias", "max_tokens", "min_p", "presence_penalty", "repetition_penalty", "stop", "temperature", "top_k", "top_p"]	{}	null
2da334a9-7b31-4fe3-9ba1-53171e710947	2026-01-16 11:18:55.898017+00	openai/gpt-4o-2024-05-13	openai/gpt-4o-2024-05-13	null	OpenAI: GPT-4o (2024-05-13)	1715558400	GPT-4o ("o" for "omni") is OpenAI's latest AI model, supporting both text and image inputs with text outputs. It maintains the intelligence level of [GPT-4 Turbo](/models/openai/gpt-4-turbo) while being twice as fast and 50% more cost-effective. GPT-4o also offers improved performance in processing non-English languages and enhanced visual capabilities.\n\nFor benchmarking against other models, it was briefly called ["im-also-a-good-gpt2-chatbot"](https://twitter.com/LiamFedus/status/1790064963966370209)\n\n#multimodal	128000	{"modality": "text+image->text", "tokenizer": "GPT", "instruct_type": null, "input_modalities": ["text", "image", "file"], "output_modalities": ["text"]}	{"prompt": "0.000005", "completion": "0.000015"}	{"is_moderated": true, "context_length": 128000, "max_completion_tokens": 4096}	null	["frequency_penalty", "logit_bias", "logprobs", "max_tokens", "presence_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_logprobs", "top_p", "web_search_options"]	{}	null
589452a7-6321-4259-bfe0-61504b63821f	2026-01-16 11:18:55.898017+00	openai/gpt-4o	openai/gpt-4o	null	OpenAI: GPT-4o	1715558400	GPT-4o ("o" for "omni") is OpenAI's latest AI model, supporting both text and image inputs with text outputs. It maintains the intelligence level of [GPT-4 Turbo](/models/openai/gpt-4-turbo) while being twice as fast and 50% more cost-effective. GPT-4o also offers improved performance in processing non-English languages and enhanced visual capabilities.\n\nFor benchmarking against other models, it was briefly called ["im-also-a-good-gpt2-chatbot"](https://twitter.com/LiamFedus/status/1790064963966370209)\n\n#multimodal	128000	{"modality": "text+image->text", "tokenizer": "GPT", "instruct_type": null, "input_modalities": ["text", "image", "file"], "output_modalities": ["text"]}	{"prompt": "0.0000025", "completion": "0.00001", "input_cache_read": "0.00000125"}	{"is_moderated": true, "context_length": 128000, "max_completion_tokens": 16384}	null	["frequency_penalty", "logit_bias", "logprobs", "max_tokens", "presence_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_logprobs", "top_p", "web_search_options"]	{}	null
f8b81e44-8101-4dd0-9964-f7a49ded5ac9	2026-01-16 11:18:55.898017+00	openai/gpt-4o:extended	openai/gpt-4o	null	OpenAI: GPT-4o (extended)	1715558400	GPT-4o ("o" for "omni") is OpenAI's latest AI model, supporting both text and image inputs with text outputs. It maintains the intelligence level of [GPT-4 Turbo](/models/openai/gpt-4-turbo) while being twice as fast and 50% more cost-effective. GPT-4o also offers improved performance in processing non-English languages and enhanced visual capabilities.\n\nFor benchmarking against other models, it was briefly called ["im-also-a-good-gpt2-chatbot"](https://twitter.com/LiamFedus/status/1790064963966370209)\n\n#multimodal	128000	{"modality": "text+image->text", "tokenizer": "GPT", "instruct_type": null, "input_modalities": ["text", "image", "file"], "output_modalities": ["text"]}	{"prompt": "0.000006", "completion": "0.000018"}	{"is_moderated": true, "context_length": 128000, "max_completion_tokens": 64000}	null	["frequency_penalty", "logit_bias", "logprobs", "max_tokens", "presence_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_logprobs", "top_p", "web_search_options"]	{}	null
d1a53560-c7ea-4cf5-b72a-4b4238cf4ffc	2026-01-16 11:18:55.898017+00	meta-llama/llama-3-70b-instruct	meta-llama/llama-3-70b-instruct	meta-llama/Meta-Llama-3-70B-Instruct	Meta: Llama 3 70B Instruct	1713398400	Meta's latest class of model (Llama 3) launched with a variety of sizes & flavors. This 70B instruct-tuned version was optimized for high quality dialogue usecases.\n\nIt has demonstrated strong performance compared to leading closed-source models in human evaluations.\n\nTo read more about the model release, [click here](https://ai.meta.com/blog/meta-llama-3/). Usage of this model is subject to [Meta's Acceptable Use Policy](https://llama.meta.com/llama3/use-policy/).	8192	{"modality": "text->text", "tokenizer": "Llama3", "instruct_type": "llama3", "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.0000004", "request": "0", "completion": "0.0000004", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 8192, "max_completion_tokens": null}	null	["frequency_penalty", "logit_bias", "max_tokens", "min_p", "presence_penalty", "repetition_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "top_k", "top_p"]	{}	null
5bbe51a6-a2d4-43c7-b376-4156ac2a8e90	2026-01-16 11:18:55.898017+00	meta-llama/llama-3-8b-instruct	meta-llama/llama-3-8b-instruct	meta-llama/Meta-Llama-3-8B-Instruct	Meta: Llama 3 8B Instruct	1713398400	Meta's latest class of model (Llama 3) launched with a variety of sizes & flavors. This 8B instruct-tuned version was optimized for high quality dialogue usecases.\n\nIt has demonstrated strong performance compared to leading closed-source models in human evaluations.\n\nTo read more about the model release, [click here](https://ai.meta.com/blog/meta-llama-3/). Usage of this model is subject to [Meta's Acceptable Use Policy](https://llama.meta.com/llama3/use-policy/).	8192	{"modality": "text->text", "tokenizer": "Llama3", "instruct_type": "llama3", "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000003", "request": "0", "completion": "0.00000006", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 8192, "max_completion_tokens": 16384}	null	["frequency_penalty", "logit_bias", "max_tokens", "min_p", "presence_penalty", "repetition_penalty", "response_format", "seed", "stop", "temperature", "tool_choice", "tools", "top_k", "top_p"]	{}	null
335d93bd-43a4-4709-9fcf-c0f22bf576f8	2026-01-16 11:18:55.898017+00	mistralai/mixtral-8x22b-instruct	mistralai/mixtral-8x22b-instruct	mistralai/Mixtral-8x22B-Instruct-v0.1	Mistral: Mixtral 8x22B Instruct	1713312000	Mistral's official instruct fine-tuned version of [Mixtral 8x22B](/models/mistralai/mixtral-8x22b). It uses 39B active parameters out of 141B, offering unparalleled cost efficiency for its size. Its strengths include:\n- strong math, coding, and reasoning\n- large context length (64k)\n- fluency in English, French, Italian, German, and Spanish\n\nSee benchmarks on the launch announcement [here](https://mistral.ai/news/mixtral-8x22b/).\n#moe	65536	{"modality": "text->text", "tokenizer": "Mistral", "instruct_type": "mistral", "input_modalities": ["text"], "output_modalities": ["text"]}	{"prompt": "0.000002", "completion": "0.000006"}	{"is_moderated": false, "context_length": 65536, "max_completion_tokens": null}	null	["frequency_penalty", "max_tokens", "presence_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_p"]	{"temperature": 0.3}	null
31807bd2-fee4-4d6c-9cb5-6f27256be644	2026-01-16 11:18:55.898017+00	microsoft/wizardlm-2-8x22b	microsoft/wizardlm-2-8x22b	microsoft/WizardLM-2-8x22B	WizardLM-2 8x22B	1713225600	WizardLM-2 8x22B is Microsoft AI's most advanced Wizard model. It demonstrates highly competitive performance compared to leading proprietary models, and it consistently outperforms all existing state-of-the-art opensource models.\n\nIt is an instruct finetune of [Mixtral 8x22B](/models/mistralai/mixtral-8x22b).\n\nTo read more about the model release, [click here](https://wizardlm.github.io/WizardLM2/).\n\n#moe	65536	{"modality": "text->text", "tokenizer": "Mistral", "instruct_type": "vicuna", "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000048", "request": "0", "completion": "0.00000048", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 65536, "max_completion_tokens": 16384}	null	["frequency_penalty", "max_tokens", "min_p", "presence_penalty", "repetition_penalty", "response_format", "seed", "stop", "temperature", "top_k", "top_p"]	{}	null
9d45e336-fb21-4bed-92f3-60fe8d3a9d78	2026-01-16 11:18:55.898017+00	openai/gpt-4-turbo	openai/gpt-4-turbo	null	OpenAI: GPT-4 Turbo	1712620800	The latest GPT-4 Turbo model with vision capabilities. Vision requests can now use JSON mode and function calling.\n\nTraining data: up to December 2023.	128000	{"modality": "text+image->text", "tokenizer": "GPT", "instruct_type": null, "input_modalities": ["text", "image"], "output_modalities": ["text"]}	{"prompt": "0.00001", "completion": "0.00003"}	{"is_moderated": true, "context_length": 128000, "max_completion_tokens": 4096}	null	["frequency_penalty", "logit_bias", "logprobs", "max_tokens", "presence_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_logprobs", "top_p"]	{}	null
f77c14f7-64cd-457c-868d-403234f6a408	2026-01-16 11:18:55.898017+00	anthropic/claude-3-haiku	anthropic/claude-3-haiku	null	Anthropic: Claude 3 Haiku	1710288000	Claude 3 Haiku is Anthropic's fastest and most compact model for\nnear-instant responsiveness. Quick and accurate targeted performance.\n\nSee the launch announcement and benchmark results [here](https://www.anthropic.com/news/claude-3-haiku)\n\n#multimodal	200000	{"modality": "text+image->text", "tokenizer": "Claude", "instruct_type": null, "input_modalities": ["text", "image"], "output_modalities": ["text"]}	{"image": "0.0004", "prompt": "0.00000025", "request": "0", "completion": "0.00000125", "web_search": "0", "input_cache_read": "0.00000003", "input_cache_write": "0.0000003", "internal_reasoning": "0"}	{"is_moderated": true, "context_length": 200000, "max_completion_tokens": 4096}	null	["max_tokens", "stop", "temperature", "tool_choice", "tools", "top_k", "top_p"]	{}	null
1993b323-ebe5-4626-94ce-e1f6d7367bf7	2026-01-16 11:18:55.898017+00	mistralai/mistral-large	mistralai/mistral-large	null	Mistral Large	1708905600	This is Mistral AI's flagship model, Mistral Large 2 (version `mistral-large-2407`). It's a proprietary weights-available model and excels at reasoning, code, JSON, chat, and more. Read the launch announcement [here](https://mistral.ai/news/mistral-large-2407/).\n\nIt supports dozens of languages including French, German, Spanish, Italian, Portuguese, Arabic, Hindi, Russian, Chinese, Japanese, and Korean, along with 80+ coding languages including Python, Java, C, C++, JavaScript, and Bash. Its long context window allows precise information recall from large documents.	128000	{"modality": "text->text", "tokenizer": "Mistral", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"prompt": "0.000002", "completion": "0.000006"}	{"is_moderated": false, "context_length": 128000, "max_completion_tokens": null}	null	["frequency_penalty", "max_tokens", "presence_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_p"]	{"temperature": 0.3}	null
a79e3e48-dc38-47fe-8334-0e1cfe5a4a08	2026-01-16 11:18:55.898017+00	openai/gpt-3.5-turbo-0613	openai/gpt-3.5-turbo-0613	null	OpenAI: GPT-3.5 Turbo (older v0613)	1706140800	GPT-3.5 Turbo is OpenAI's fastest model. It can understand and generate natural language or code, and is optimized for chat and traditional completion tasks.\n\nTraining data up to Sep 2021.	4095	{"modality": "text->text", "tokenizer": "GPT", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.000001", "request": "0", "completion": "0.000002", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 4095, "max_completion_tokens": 4096}	null	["frequency_penalty", "logit_bias", "logprobs", "max_tokens", "presence_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_logprobs", "top_p"]	{}	null
972a64a0-e989-4fb2-9748-0bee34e637d0	2026-01-16 11:18:55.898017+00	openai/gpt-4-turbo-preview	openai/gpt-4-turbo-preview	null	OpenAI: GPT-4 Turbo Preview	1706140800	The preview GPT-4 model with improved instruction following, JSON mode, reproducible outputs, parallel function calling, and more. Training data: up to Dec 2023.\n\n**Note:** heavily rate limited by OpenAI while in preview.	128000	{"modality": "text->text", "tokenizer": "GPT", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"prompt": "0.00001", "completion": "0.00003"}	{"is_moderated": true, "context_length": 128000, "max_completion_tokens": 4096}	null	["frequency_penalty", "logit_bias", "logprobs", "max_tokens", "presence_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_logprobs", "top_p"]	{}	null
73063e1f-0c45-47ef-adf7-46776277347f	2026-01-16 11:18:55.898017+00	mistralai/mistral-tiny	mistralai/mistral-tiny	null	Mistral Tiny	1704844800	Note: This model is being deprecated. Recommended replacement is the newer [Ministral 8B](/mistral/ministral-8b)\n\nThis model is currently powered by Mistral-7B-v0.2, and incorporates a "better" fine-tuning than [Mistral 7B](/models/mistralai/mistral-7b-instruct-v0.1), inspired by community work. It's best used for large batch processing tasks where cost is a significant factor but reasoning capabilities are not crucial.	32768	{"modality": "text->text", "tokenizer": "Mistral", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"prompt": "0.00000025", "completion": "0.00000025"}	{"is_moderated": false, "context_length": 32768, "max_completion_tokens": null}	null	["frequency_penalty", "max_tokens", "presence_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_p"]	{"temperature": 0.3}	null
9dbec897-3e4b-4850-b620-ad59b5f800ec	2026-01-16 11:18:55.898017+00	mistralai/mistral-7b-instruct-v0.2	mistralai/mistral-7b-instruct-v0.2	mistralai/Mistral-7B-Instruct-v0.2	Mistral: Mistral 7B Instruct v0.2	1703721600	A high-performing, industry-standard 7.3B parameter model, with optimizations for speed and context length.\n\nAn improved version of [Mistral 7B Instruct](/modelsmistralai/mistral-7b-instruct-v0.1), with the following changes:\n\n- 32k context window (vs 8k context in v0.1)\n- Rope-theta = 1e6\n- No Sliding-Window Attention	32768	{"modality": "text->text", "tokenizer": "Mistral", "instruct_type": "mistral", "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.0000002", "request": "0", "completion": "0.0000002", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 32768, "max_completion_tokens": null}	null	["frequency_penalty", "logit_bias", "max_tokens", "min_p", "presence_penalty", "repetition_penalty", "stop", "temperature", "top_k", "top_p"]	{"temperature": 0.3}	null
1206a649-f000-4bd4-9493-a810b9c6118b	2026-01-16 11:18:55.898017+00	mistralai/mixtral-8x7b-instruct	mistralai/mixtral-8x7b-instruct	mistralai/Mixtral-8x7B-Instruct-v0.1	Mistral: Mixtral 8x7B Instruct	1702166400	Mixtral 8x7B Instruct is a pretrained generative Sparse Mixture of Experts, by Mistral AI, for chat and instruction use. Incorporates 8 experts (feed-forward networks) for a total of 47 billion parameters.\n\nInstruct model fine-tuned by Mistral. #moe	32768	{"modality": "text->text", "tokenizer": "Mistral", "instruct_type": "mistral", "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000054", "request": "0", "completion": "0.00000054", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 32768, "max_completion_tokens": 16384}	null	["frequency_penalty", "logit_bias", "max_tokens", "min_p", "presence_penalty", "repetition_penalty", "response_format", "seed", "stop", "temperature", "tool_choice", "tools", "top_k", "top_p"]	{"temperature": 0.3}	null
faa4987f-2357-49a2-b8da-2e4a69b49b6c	2026-01-16 11:18:55.898017+00	neversleep/noromaid-20b	neversleep/noromaid-20b	NeverSleep/Noromaid-20b-v0.1.1	Noromaid 20B	1700956800	A collab between IkariDev and Undi. This merge is suitable for RP, ERP, and general knowledge.\n\n#merge #uncensored	4096	{"modality": "text->text", "tokenizer": "Llama2", "instruct_type": "alpaca", "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.000001", "request": "0", "completion": "0.00000175", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 4096, "max_completion_tokens": null}	null	["frequency_penalty", "logit_bias", "logprobs", "max_tokens", "min_p", "presence_penalty", "repetition_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "top_a", "top_k", "top_logprobs", "top_p"]	{}	null
9fafa5f8-4657-46e6-8ec1-b3b23807319a	2026-01-16 11:18:55.898017+00	alpindale/goliath-120b	alpindale/goliath-120b	alpindale/goliath-120b	Goliath 120B	1699574400	A large LLM created by combining two fine-tuned Llama 70B models into one 120B model. Combines Xwin and Euryale.\n\nCredits to\n- [@chargoddard](https://huggingface.co/chargoddard) for developing the framework used to merge the model - [mergekit](https://github.com/cg123/mergekit).\n- [@Undi95](https://huggingface.co/Undi95) for helping with the merge ratios.\n\n#merge	6144	{"modality": "text->text", "tokenizer": "Llama2", "instruct_type": "airoboros", "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.000006", "request": "0", "completion": "0.000008", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 6144, "max_completion_tokens": 1024}	null	["frequency_penalty", "logit_bias", "logprobs", "max_tokens", "min_p", "presence_penalty", "repetition_penalty", "response_format", "seed", "stop", "temperature", "top_a", "top_k", "top_logprobs", "top_p"]	{}	null
2a7cb52e-bf67-4c6b-8596-8013f78f0396	2026-01-16 11:18:55.898017+00	openrouter/auto	openrouter/auto	null	Auto Router	1699401600	Your prompt will be processed by a meta-model and routed to one of dozens of models (see below), optimizing for the best possible output.\n\nTo see which model was used, visit [Activity](/activity), or read the `model` attribute of the response. Your response will be priced at the same rate as the routed model.\n\nLearn more, including how to customize the models for routing, in our [docs](/docs/guides/routing/routers/auto-router).\n\nRequests will be routed to the following models:\n- [openai/gpt-5.1](/openai/gpt-5.1)\n- [openai/gpt-5](/openai/gpt-5)\n- [openai/gpt-5-mini](/openai/gpt-5-mini)\n- [openai/gpt-5-nano](/openai/gpt-5-nano)\n- [openai/gpt-4.1](/openai/gpt-4.1)\n- [openai/gpt-4.1-mini](/openai/gpt-4.1-mini)\n- [openai/gpt-4.1-nano](/openai/gpt-4.1-nano)\n- [openai/gpt-4o](/openai/gpt-4o)\n- [openai/gpt-4o-2024-05-13](/openai/gpt-4o-2024-05-13)\n- [openai/gpt-4o-2024-08-06](/openai/gpt-4o-2024-08-06)\n- [openai/gpt-4o-2024-11-20](/openai/gpt-4o-2024-11-20)\n- [openai/gpt-4o-mini](/openai/gpt-4o-mini)\n- [openai/gpt-4o-mini-2024-07-18](/openai/gpt-4o-mini-2024-07-18)\n- [openai/gpt-4-turbo](/openai/gpt-4-turbo)\n- [openai/gpt-4-turbo-preview](/openai/gpt-4-turbo-preview)\n- [openai/gpt-4-1106-preview](/openai/gpt-4-1106-preview)\n- [openai/gpt-4](/openai/gpt-4)\n- [openai/gpt-3.5-turbo](/openai/gpt-3.5-turbo)\n- [openai/gpt-oss-120b](/openai/gpt-oss-120b)\n- [anthropic/claude-opus-4.5](/anthropic/claude-opus-4.5)\n- [anthropic/claude-opus-4.1](/anthropic/claude-opus-4.1)\n- [anthropic/claude-opus-4](/anthropic/claude-opus-4)\n- [anthropic/claude-sonnet-4.5](/anthropic/claude-sonnet-4.5)\n- [anthropic/claude-sonnet-4](/anthropic/claude-sonnet-4)\n- [anthropic/claude-3.7-sonnet](/anthropic/claude-3.7-sonnet)\n- [anthropic/claude-haiku-4.5](/anthropic/claude-haiku-4.5)\n- [anthropic/claude-3.5-haiku](/anthropic/claude-3.5-haiku)\n- [anthropic/claude-3-haiku](/anthropic/claude-3-haiku)\n- [google/gemini-3-pro-preview](/google/gemini-3-pro-preview)\n- [google/gemini-2.5-pro](/google/gemini-2.5-pro)\n- [google/gemini-2.0-flash-001](/google/gemini-2.0-flash-001)\n- [google/gemini-2.5-flash](/google/gemini-2.5-flash)\n- [mistralai/mistral-large](/mistralai/mistral-large)\n- [mistralai/mistral-large-2407](/mistralai/mistral-large-2407)\n- [mistralai/mistral-large-2411](/mistralai/mistral-large-2411)\n- [mistralai/mistral-medium-3.1](/mistralai/mistral-medium-3.1)\n- [mistralai/mistral-nemo](/mistralai/mistral-nemo)\n- [mistralai/mistral-7b-instruct](/mistralai/mistral-7b-instruct)\n- [mistralai/mixtral-8x7b-instruct](/mistralai/mixtral-8x7b-instruct)\n- [mistralai/mixtral-8x22b-instruct](/mistralai/mixtral-8x22b-instruct)\n- [mistralai/codestral-2508](/mistralai/codestral-2508)\n- [x-ai/grok-4](/x-ai/grok-4)\n- [x-ai/grok-3](/x-ai/grok-3)\n- [x-ai/grok-3-mini](/x-ai/grok-3-mini)\n- [deepseek/deepseek-r1](/deepseek/deepseek-r1)\n- [meta-llama/llama-3.3-70b-instruct](/meta-llama/llama-3.3-70b-instruct)\n- [meta-llama/llama-3.1-405b-instruct](/meta-llama/llama-3.1-405b-instruct)\n- [meta-llama/llama-3.1-70b-instruct](/meta-llama/llama-3.1-70b-instruct)\n- [meta-llama/llama-3.1-8b-instruct](/meta-llama/llama-3.1-8b-instruct)\n- [meta-llama/llama-3-70b-instruct](/meta-llama/llama-3-70b-instruct)\n- [meta-llama/llama-3-8b-instruct](/meta-llama/llama-3-8b-instruct)\n- [qwen/qwen3-235b-a22b](/qwen/qwen3-235b-a22b)\n- [qwen/qwen3-32b](/qwen/qwen3-32b)\n- [qwen/qwen3-14b](/qwen/qwen3-14b)\n- [cohere/command-r-plus-08-2024](/cohere/command-r-plus-08-2024)\n- [cohere/command-r-08-2024](/cohere/command-r-08-2024)\n- [moonshotai/kimi-k2-thinking](/moonshotai/kimi-k2-thinking)\n- [perplexity/sonar](/perplexity/sonar)	2000000	{"modality": "text->text", "tokenizer": "Router", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"prompt": "-1", "completion": "-1"}	{"is_moderated": false, "context_length": null, "max_completion_tokens": null}	null	[]	{"top_p": null, "temperature": null, "frequency_penalty": null}	null
3af954c4-5764-49e7-816e-a6b0d8f20917	2026-01-16 11:18:55.898017+00	openai/gpt-4-1106-preview	openai/gpt-4-1106-preview	null	OpenAI: GPT-4 Turbo (older v1106)	1699228800	The latest GPT-4 Turbo model with vision capabilities. Vision requests can now use JSON mode and function calling.\n\nTraining data: up to April 2023.	128000	{"modality": "text->text", "tokenizer": "GPT", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"prompt": "0.00001", "completion": "0.00003"}	{"is_moderated": true, "context_length": 128000, "max_completion_tokens": 4096}	null	["frequency_penalty", "logit_bias", "logprobs", "max_tokens", "presence_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_logprobs", "top_p"]	{}	null
9308006d-82ee-4fc6-8864-3280909bfd08	2026-01-16 11:18:55.898017+00	openai/gpt-3.5-turbo-instruct	openai/gpt-3.5-turbo-instruct	null	OpenAI: GPT-3.5 Turbo Instruct	1695859200	This model is a variant of GPT-3.5 Turbo tuned for instructional prompts and omitting chat-related optimizations. Training data: up to Sep 2021.	4095	{"modality": "text->text", "tokenizer": "GPT", "instruct_type": "chatml", "input_modalities": ["text"], "output_modalities": ["text"]}	{"prompt": "0.0000015", "completion": "0.000002"}	{"is_moderated": true, "context_length": 4095, "max_completion_tokens": 4096}	null	["frequency_penalty", "logit_bias", "logprobs", "max_tokens", "presence_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "top_logprobs", "top_p"]	{}	null
1b9586ec-8c0c-4098-bdf6-2fca26cc5079	2026-01-16 11:18:55.898017+00	mistralai/mistral-7b-instruct-v0.1	mistralai/mistral-7b-instruct-v0.1	mistralai/Mistral-7B-Instruct-v0.1	Mistral: Mistral 7B Instruct v0.1	1695859200	A 7.3B parameter model that outperforms Llama 2 13B on all benchmarks, with optimizations for speed and context length.	2824	{"modality": "text->text", "tokenizer": "Mistral", "instruct_type": "mistral", "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000011", "request": "0", "completion": "0.00000019", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 2824, "max_completion_tokens": null}	null	["frequency_penalty", "max_tokens", "presence_penalty", "repetition_penalty", "seed", "temperature", "top_k", "top_p"]	{"temperature": 0.3}	null
0ddfdca9-8991-449a-9a9d-d156ae925265	2026-01-16 11:18:55.898017+00	openai/gpt-3.5-turbo-16k	openai/gpt-3.5-turbo-16k	null	OpenAI: GPT-3.5 Turbo 16k	1693180800	This model offers four times the context length of gpt-3.5-turbo, allowing it to support approximately 20 pages of text in a single request at a higher cost. Training data: up to Sep 2021.	16385	{"modality": "text->text", "tokenizer": "GPT", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"prompt": "0.000003", "completion": "0.000004"}	{"is_moderated": true, "context_length": 16385, "max_completion_tokens": 4096}	null	["frequency_penalty", "logit_bias", "logprobs", "max_tokens", "presence_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_logprobs", "top_p"]	{}	null
8251d118-d736-4a3d-9849-3fc90b03dfd2	2026-01-16 11:18:55.898017+00	mancer/weaver	mancer/weaver	null	Mancer: Weaver (alpha)	1690934400	An attempt to recreate Claude-style verbosity, but don't expect the same level of coherence or memory. Meant for use in roleplay/narrative situations.	8000	{"modality": "text->text", "tokenizer": "Llama2", "instruct_type": "alpaca", "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000075", "request": "0", "completion": "0.000001", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 8000, "max_completion_tokens": 2000}	null	["frequency_penalty", "logit_bias", "logprobs", "max_tokens", "min_p", "presence_penalty", "repetition_penalty", "response_format", "seed", "stop", "temperature", "top_a", "top_k", "top_logprobs", "top_p"]	{}	null
eaf9d72c-ffc7-4589-9f5d-cf719ace4a23	2026-01-16 11:18:55.898017+00	undi95/remm-slerp-l2-13b	undi95/remm-slerp-l2-13b	Undi95/ReMM-SLERP-L2-13B	ReMM SLERP 13B	1689984000	A recreation trial of the original MythoMax-L2-B13 but with updated models. #merge	6144	{"modality": "text->text", "tokenizer": "Llama2", "instruct_type": "alpaca", "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000045", "request": "0", "completion": "0.00000065", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 6144, "max_completion_tokens": null}	null	["frequency_penalty", "logit_bias", "logprobs", "max_tokens", "min_p", "presence_penalty", "repetition_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "top_a", "top_k", "top_logprobs", "top_p"]	{}	null
b845da0f-24be-479c-8edc-6411082e4b66	2026-01-16 11:18:55.898017+00	gryphe/mythomax-l2-13b	gryphe/mythomax-l2-13b	Gryphe/MythoMax-L2-13b	MythoMax 13B	1688256000	One of the highest performing and most popular fine-tunes of Llama 2 13B, with rich descriptions and roleplay. #merge	4096	{"modality": "text->text", "tokenizer": "Llama2", "instruct_type": "alpaca", "input_modalities": ["text"], "output_modalities": ["text"]}	{"image": "0", "prompt": "0.00000006", "request": "0", "completion": "0.00000006", "web_search": "0", "internal_reasoning": "0"}	{"is_moderated": false, "context_length": 4096, "max_completion_tokens": null}	null	["frequency_penalty", "logit_bias", "logprobs", "max_tokens", "min_p", "presence_penalty", "repetition_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "top_a", "top_k", "top_logprobs", "top_p"]	{}	null
5af4907e-8bad-4a92-8a25-e19682dd5381	2026-01-16 11:18:55.898017+00	openai/gpt-4-0314	openai/gpt-4-0314	null	OpenAI: GPT-4 (older v0314)	1685232000	GPT-4-0314 is the first version of GPT-4 released, with a context length of 8,192 tokens, and was supported until June 14. Training data: up to Sep 2021.	8191	{"modality": "text->text", "tokenizer": "GPT", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"prompt": "0.00003", "completion": "0.00006"}	{"is_moderated": true, "context_length": 8191, "max_completion_tokens": 4096}	null	["frequency_penalty", "logit_bias", "logprobs", "max_tokens", "presence_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_logprobs", "top_p"]	{}	null
eacd7782-d97b-4b76-bb53-d82fd5de1478	2026-01-16 11:18:55.898017+00	openai/gpt-4	openai/gpt-4	null	OpenAI: GPT-4	1685232000	OpenAI's flagship model, GPT-4 is a large-scale multimodal language model capable of solving difficult problems with greater accuracy than previous models due to its broader general knowledge and advanced reasoning capabilities. Training data: up to Sep 2021.	8191	{"modality": "text->text", "tokenizer": "GPT", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"prompt": "0.00003", "completion": "0.00006"}	{"is_moderated": true, "context_length": 8191, "max_completion_tokens": 4096}	null	["frequency_penalty", "logit_bias", "logprobs", "max_tokens", "presence_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_logprobs", "top_p"]	{}	null
ff8016cc-04d0-4dce-a3a7-4bf7c475032e	2026-01-16 11:18:55.898017+00	openai/gpt-3.5-turbo	openai/gpt-3.5-turbo	null	OpenAI: GPT-3.5 Turbo	1685232000	GPT-3.5 Turbo is OpenAI's fastest model. It can understand and generate natural language or code, and is optimized for chat and traditional completion tasks.\n\nTraining data up to Sep 2021.	16385	{"modality": "text->text", "tokenizer": "GPT", "instruct_type": null, "input_modalities": ["text"], "output_modalities": ["text"]}	{"prompt": "0.0000005", "completion": "0.0000015"}	{"is_moderated": true, "context_length": 16385, "max_completion_tokens": 4096}	null	["frequency_penalty", "logit_bias", "logprobs", "max_tokens", "presence_penalty", "response_format", "seed", "stop", "structured_outputs", "temperature", "tool_choice", "tools", "top_logprobs", "top_p"]	{}	null
\.


--
-- Name: __drizzle_migrations_id_seq; Type: SEQUENCE SET; Schema: drizzle; Owner: -
--

SELECT pg_catalog.setval('drizzle.__drizzle_migrations_id_seq', 1, true);


--
-- Name: __drizzle_migrations __drizzle_migrations_pkey; Type: CONSTRAINT; Schema: drizzle; Owner: -
--

ALTER TABLE ONLY drizzle.__drizzle_migrations
    ADD CONSTRAINT __drizzle_migrations_pkey PRIMARY KEY (id);


--
-- Name: AudioModel AudioModel_pkey; Type: CONSTRAINT; Schema: public; Owner: -
--

ALTER TABLE ONLY public."AudioModel"
    ADD CONSTRAINT "AudioModel_pkey" PRIMARY KEY (id);


--
-- Name: CardConfig CardConfig_pkey; Type: CONSTRAINT; Schema: public; Owner: -
--

ALTER TABLE ONLY public."CardConfig"
    ADD CONSTRAINT "CardConfig_pkey" PRIMARY KEY ("cardId");


--
-- Name: ChatModel ChatModel_pkey; Type: CONSTRAINT; Schema: public; Owner: -
--

ALTER TABLE ONLY public."ChatModel"
    ADD CONSTRAINT "ChatModel_pkey" PRIMARY KEY (id);


--
-- Name: ComplianceModel ComplianceModel_pkey; Type: CONSTRAINT; Schema: public; Owner: -
--

ALTER TABLE ONLY public."ComplianceModel"
    ADD CONSTRAINT "ComplianceModel_pkey" PRIMARY KEY (id);


--
-- Name: ComponentRole ComponentRole_pkey; Type: CONSTRAINT; Schema: public; Owner: -
--

ALTER TABLE ONLY public."ComponentRole"
    ADD CONSTRAINT "ComponentRole_pkey" PRIMARY KEY (id);


--
-- Name: CustomButton CustomButton_pkey; Type: CONSTRAINT; Schema: public; Owner: -
--

ALTER TABLE ONLY public."CustomButton"
    ADD CONSTRAINT "CustomButton_pkey" PRIMARY KEY (id);


--
-- Name: EmbeddingModel EmbeddingModel_pkey; Type: CONSTRAINT; Schema: public; Owner: -
--

ALTER TABLE ONLY public."EmbeddingModel"
    ADD CONSTRAINT "EmbeddingModel_pkey" PRIMARY KEY (id);


--
-- Name: FileIndex FileIndex_pkey; Type: CONSTRAINT; Schema: public; Owner: -
--

ALTER TABLE ONLY public."FileIndex"
    ADD CONSTRAINT "FileIndex_pkey" PRIMARY KEY ("filePath");


--
-- Name: ImageModel ImageModel_pkey; Type: CONSTRAINT; Schema: public; Owner: -
--

ALTER TABLE ONLY public."ImageModel"
    ADD CONSTRAINT "ImageModel_pkey" PRIMARY KEY (id);


--
-- Name: Job Job_pkey; Type: CONSTRAINT; Schema: public; Owner: -
--

ALTER TABLE ONLY public."Job"
    ADD CONSTRAINT "Job_pkey" PRIMARY KEY (id);


--
-- Name: KnowledgeVector KnowledgeVector_pkey; Type: CONSTRAINT; Schema: public; Owner: -
--

ALTER TABLE ONLY public."KnowledgeVector"
    ADD CONSTRAINT "KnowledgeVector_pkey" PRIMARY KEY (id);


--
-- Name: ModelFailure ModelFailure_pkey; Type: CONSTRAINT; Schema: public; Owner: -
--

ALTER TABLE ONLY public."ModelFailure"
    ADD CONSTRAINT "ModelFailure_pkey" PRIMARY KEY (id);


--
-- Name: ModelUsage ModelUsage_pkey; Type: CONSTRAINT; Schema: public; Owner: -
--

ALTER TABLE ONLY public."ModelUsage"
    ADD CONSTRAINT "ModelUsage_pkey" PRIMARY KEY (id);


--
-- Name: Model Model_pkey; Type: CONSTRAINT; Schema: public; Owner: -
--

ALTER TABLE ONLY public."Model"
    ADD CONSTRAINT "Model_pkey" PRIMARY KEY (id);


--
-- Name: PromptRefinement PromptRefinement_pkey; Type: CONSTRAINT; Schema: public; Owner: -
--

ALTER TABLE ONLY public."PromptRefinement"
    ADD CONSTRAINT "PromptRefinement_pkey" PRIMARY KEY (id);


--
-- Name: ProviderConfig ProviderConfig_pkey; Type: CONSTRAINT; Schema: public; Owner: -
--

ALTER TABLE ONLY public."ProviderConfig"
    ADD CONSTRAINT "ProviderConfig_pkey" PRIMARY KEY (id);


--
-- Name: RewardModel RewardModel_pkey; Type: CONSTRAINT; Schema: public; Owner: -
--

ALTER TABLE ONLY public."RewardModel"
    ADD CONSTRAINT "RewardModel_pkey" PRIMARY KEY (id);


--
-- Name: RoleAssessment RoleAssessment_pkey; Type: CONSTRAINT; Schema: public; Owner: -
--

ALTER TABLE ONLY public."RoleAssessment"
    ADD CONSTRAINT "RoleAssessment_pkey" PRIMARY KEY (id);


--
-- Name: RoleCategory RoleCategory_pkey; Type: CONSTRAINT; Schema: public; Owner: -
--

ALTER TABLE ONLY public."RoleCategory"
    ADD CONSTRAINT "RoleCategory_pkey" PRIMARY KEY (id);


--
-- Name: RoleTool RoleTool_pkey; Type: CONSTRAINT; Schema: public; Owner: -
--

ALTER TABLE ONLY public."RoleTool"
    ADD CONSTRAINT "RoleTool_pkey" PRIMARY KEY ("roleId", "toolId");


--
-- Name: RoleVariant RoleVariant_pkey; Type: CONSTRAINT; Schema: public; Owner: -
--

ALTER TABLE ONLY public."RoleVariant"
    ADD CONSTRAINT "RoleVariant_pkey" PRIMARY KEY (id);


--
-- Name: Role Role_pkey; Type: CONSTRAINT; Schema: public; Owner: -
--

ALTER TABLE ONLY public."Role"
    ADD CONSTRAINT "Role_pkey" PRIMARY KEY (id);


--
-- Name: Tool Tool_pkey; Type: CONSTRAINT; Schema: public; Owner: -
--

ALTER TABLE ONLY public."Tool"
    ADD CONSTRAINT "Tool_pkey" PRIMARY KEY (id);


--
-- Name: UnknownModel UnknownModel_pkey; Type: CONSTRAINT; Schema: public; Owner: -
--

ALTER TABLE ONLY public."UnknownModel"
    ADD CONSTRAINT "UnknownModel_pkey" PRIMARY KEY (id);


--
-- Name: VisionModel VisionModel_pkey; Type: CONSTRAINT; Schema: public; Owner: -
--

ALTER TABLE ONLY public."VisionModel"
    ADD CONSTRAINT "VisionModel_pkey" PRIMARY KEY (id);


--
-- Name: WorkOrderCard WorkOrderCard_pkey; Type: CONSTRAINT; Schema: public; Owner: -
--

ALTER TABLE ONLY public."WorkOrderCard"
    ADD CONSTRAINT "WorkOrderCard_pkey" PRIMARY KEY (id);


--
-- Name: Workspace Workspace_pkey; Type: CONSTRAINT; Schema: public; Owner: -
--

ALTER TABLE ONLY public."Workspace"
    ADD CONSTRAINT "Workspace_pkey" PRIMARY KEY (id);


--
-- Name: _prisma_migrations _prisma_migrations_pkey; Type: CONSTRAINT; Schema: public; Owner: -
--

ALTER TABLE ONLY public._prisma_migrations
    ADD CONSTRAINT _prisma_migrations_pkey PRIMARY KEY (id);


--
-- Name: cerebras_models cerebras_models_pkey; Type: CONSTRAINT; Schema: public; Owner: -
--

ALTER TABLE ONLY public.cerebras_models
    ADD CONSTRAINT cerebras_models_pkey PRIMARY KEY (_id);


--
-- Name: google_models_example google_models_example_pkey; Type: CONSTRAINT; Schema: public; Owner: -
--

ALTER TABLE ONLY public.google_models_example
    ADD CONSTRAINT google_models_example_pkey PRIMARY KEY (id);


--
-- Name: groq_models groq_models_pkey; Type: CONSTRAINT; Schema: public; Owner: -
--

ALTER TABLE ONLY public.groq_models
    ADD CONSTRAINT groq_models_pkey PRIMARY KEY (_id);


--
-- Name: mistral_models mistral_models_pkey; Type: CONSTRAINT; Schema: public; Owner: -
--

ALTER TABLE ONLY public.mistral_models
    ADD CONSTRAINT mistral_models_pkey PRIMARY KEY (_id);


--
-- Name: model_registry model_registry_pkey; Type: CONSTRAINT; Schema: public; Owner: -
--

ALTER TABLE ONLY public.model_registry
    ADD CONSTRAINT model_registry_pkey PRIMARY KEY (id);


--
-- Name: modelcapabilities modelcapabilities_pkey; Type: CONSTRAINT; Schema: public; Owner: -
--

ALTER TABLE ONLY public.modelcapabilities
    ADD CONSTRAINT modelcapabilities_pkey PRIMARY KEY (id);


--
-- Name: nvidia_models nvidia_models_pkey; Type: CONSTRAINT; Schema: public; Owner: -
--

ALTER TABLE ONLY public.nvidia_models
    ADD CONSTRAINT nvidia_models_pkey PRIMARY KEY (_id);


--
-- Name: ollama_models ollama_models_pkey; Type: CONSTRAINT; Schema: public; Owner: -
--

ALTER TABLE ONLY public.ollama_models
    ADD CONSTRAINT ollama_models_pkey PRIMARY KEY (_id);


--
-- Name: openrouter_models openrouter_models_pkey; Type: CONSTRAINT; Schema: public; Owner: -
--

ALTER TABLE ONLY public.openrouter_models
    ADD CONSTRAINT openrouter_models_pkey PRIMARY KEY (_id);


--
-- Name: AudioModel_modelId_key; Type: INDEX; Schema: public; Owner: -
--

CREATE UNIQUE INDEX "AudioModel_modelId_key" ON public."AudioModel" USING btree ("modelId");


--
-- Name: ChatModel_modelId_key; Type: INDEX; Schema: public; Owner: -
--

CREATE UNIQUE INDEX "ChatModel_modelId_key" ON public."ChatModel" USING btree ("modelId");


--
-- Name: ComplianceModel_modelId_key; Type: INDEX; Schema: public; Owner: -
--

CREATE UNIQUE INDEX "ComplianceModel_modelId_key" ON public."ComplianceModel" USING btree ("modelId");


--
-- Name: EmbeddingModel_modelId_key; Type: INDEX; Schema: public; Owner: -
--

CREATE UNIQUE INDEX "EmbeddingModel_modelId_key" ON public."EmbeddingModel" USING btree ("modelId");


--
-- Name: ImageModel_modelId_key; Type: INDEX; Schema: public; Owner: -
--

CREATE UNIQUE INDEX "ImageModel_modelId_key" ON public."ImageModel" USING btree ("modelId");


--
-- Name: KnowledgeVector_entityType_entityId_idx; Type: INDEX; Schema: public; Owner: -
--

CREATE INDEX "KnowledgeVector_entityType_entityId_idx" ON public."KnowledgeVector" USING btree ("entityType", "entityId");


--
-- Name: ModelFailure_providerId_modelId_key; Type: INDEX; Schema: public; Owner: -
--

CREATE UNIQUE INDEX "ModelFailure_providerId_modelId_key" ON public."ModelFailure" USING btree ("providerId", "modelId");


--
-- Name: Model_providerId_name_key; Type: INDEX; Schema: public; Owner: -
--

CREATE UNIQUE INDEX "Model_providerId_name_key" ON public."Model" USING btree ("providerId", name);


--
-- Name: RewardModel_modelId_key; Type: INDEX; Schema: public; Owner: -
--

CREATE UNIQUE INDEX "RewardModel_modelId_key" ON public."RewardModel" USING btree ("modelId");


--
-- Name: RoleCategory_name_key; Type: INDEX; Schema: public; Owner: -
--

CREATE UNIQUE INDEX "RoleCategory_name_key" ON public."RoleCategory" USING btree (name);


--
-- Name: Role_name_key; Type: INDEX; Schema: public; Owner: -
--

CREATE UNIQUE INDEX "Role_name_key" ON public."Role" USING btree (name);


--
-- Name: Tool_name_key; Type: INDEX; Schema: public; Owner: -
--

CREATE UNIQUE INDEX "Tool_name_key" ON public."Tool" USING btree (name);


--
-- Name: UnknownModel_modelId_key; Type: INDEX; Schema: public; Owner: -
--

CREATE UNIQUE INDEX "UnknownModel_modelId_key" ON public."UnknownModel" USING btree ("modelId");


--
-- Name: VisionModel_modelId_key; Type: INDEX; Schema: public; Owner: -
--

CREATE UNIQUE INDEX "VisionModel_modelId_key" ON public."VisionModel" USING btree ("modelId");


--
-- Name: modelcapabilities_modelId_key; Type: INDEX; Schema: public; Owner: -
--

CREATE UNIQUE INDEX "modelcapabilities_modelId_key" ON public.modelcapabilities USING btree ("modelId");


--
-- PostgreSQL database dump complete
--

\unrestrict vHk8bogOiRid6mmcTdpYmbj4m5GbNzj7zyTMeXWKI7f8we07X35ig6J4oEG5YZD

