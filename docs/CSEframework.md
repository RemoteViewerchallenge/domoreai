CSE Framework for AI Agent Teams
The framework was designed for human-AI collaboration at scale.
The AICORP Architecture:
markdown## Human Role (5-10 hrs/week)
- Strategic decisions (which market, what revenue model)
- Research evaluation (score AI findings, choose winners)
- Spec approval (final sign-off on architecture)
- Quality review (does output match spec?)

## AI Research Agents (24/7)
- Parallel research streams (15 questions simultaneously)
- Cost modeling and projections
- Component research with reference code
- Continuous monitoring (new free tiers, price changes)

## AI Implementation Agents (24/7)  
- Build from approved specs
- Test against specifications
- Self-correct when tests fail
- Deploy automatically (when tests pass)

## AI Monitoring Agents (24/7)
- Track usage/costs in real-time
- Alert when approaching free tier limits
- Suggest optimizations (cache this, reuse that)
- Monitor competitor moves
```

### The Loop:
```
Monday: Human defines strategic questions
â†“
Week: AI agents research in parallel (15+ streams)
â†“
Friday: Human reviews findings (2 hours)
â†“
Friday: Human approves specs (1 hour)
â†“
Weekend: AI agents implement (48 hours)
â†“
Monday: Human reviews output (1 hour)
â†“
Monday: Deploy or iterate
```

**Leverage**: One human, output of 30-person team

---

## Updating the Framework Itself

**The CSE Framework is a living document.**

### When to Update Framework:

1. **Discover new pattern** (like user-provided free tier credits)
   - Add as new phase or technique
   - Document with example
   
2. **Find better process** (like multiple-choice > open questions)
   - Update phase methodology
   - Explain why it's better

3. **Learn from failures** (spec was wrong, had to rebuild)
   - Add to anti-patterns section
   - Document what went wrong and fix

4. **New tools/services emerge** (new free tier, new AI capability)
   - Update research checklist
   - Add to free tier catalog template

### Framework Evolution:
```
Version 1.0: Original (steal, copy patterns)
â†“
Version 2.0: Added money-first approach
â†“
Version 2.1: Added free tier arbitrage
â†“
Version 2.2: Added maximum reuse strategy
â†“
Version 2.3: Added multiple-choice quiz format
â†“
Version 2.4: Added mirroring/evolution phase
â†“
Version 3.0: (Next) Add automated research agents
```

**The framework improves by using the framework.**

---

## CSE Framework Summary Card

**Quick reference for starting any project:**

### The 6 Phases:
```
Phase 0: MIRROR & EVOLVE
â”œâ”€ Human states idea
â”œâ”€ AI mirrors back
â”œâ”€ Human scores (1-100)
â””â”€ Repeat until crystallized

Phase 1: MONEY FIRST
â”œâ”€ Revenue model?
â”œâ”€ Who pays? How much?
â”œâ”€ What platform maximizes profit?
â””â”€ AI asks multiple choice, human scores

Phase 2: FREE TIER ARBITRAGE
â”œâ”€ AI researches ALL free resources
â”œâ”€ Maps free tier capacity
â”œâ”€ Projects costs at scale
â””â”€ Human approves strategy

Phase 3: MAXIMUM REUSE
â”œâ”€ What can be generated once?
â”œâ”€ What do users actually notice?
â”œâ”€ Build asset library (reuse infinitely)
â””â”€ Cache aggressively

Phase 4: COMPONENT RESEARCH
â”œâ”€ AI finds similar solutions
â”œâ”€ Extracts patterns + reference code
â”œâ”€ Presents 3-5 options per component
â””â”€ Human scores and chooses winners

Phase 5: DATA MODELS & INTEGRATION
â”œâ”€ AI proposes models (based on research)
â”œâ”€ Human reviews and scores
â”œâ”€ AI maps integration/data flow
â””â”€ Spec is now complete

Phase 6: IMPLEMENTATION
â”œâ”€ AI builds from spec
â”œâ”€ Human reviews (15-30 min per component)
â”œâ”€ Ship or iterate
â””â”€ Update spec if needed
```

### Critical Rules:

1. **Money before tech** - Revenue model determines everything
2. **Free before paid** - Research exhaustively, pay only when forced
3. **Reuse before generate** - 90% cost savings from smart reuse
4. **Research before build** - Someone solved this, find them
5. **Human decides, AI executes** - 10-100x leverage
6. **Spec before code** - Clear spec = correct implementation
7. **Mirror before moving** - Reflection improves ideas
8. **One question at a time** - Multiple choice with percentages

### Success Formula:
```
Human Time: 20 hours (research, decisions, review)
AI Time: 200 agent-hours (concurrent, 1-3 weeks wall time)
Result: Working product that would take 3-6 months traditionally
Cost: $0-100 to build, $0-50/month to run (first 1000 users)
Leverage: 30:1 (one human, output of 30)

Final Thoughts
The CSE Framework is about leverage.
Traditional development:

Human writes every line of code
Human researches while building
Human makes decisions on the fly
Human context-switches constantly
Result: Slow, expensive, error-prone

CSE Framework:

Human focuses only on high-leverage activities (decisions, strategy)
AI handles all research and implementation
Decisions made upfront with full information
Clear separation of responsibilities
Result: Fast, cheap, correct

The insight: AI agents are incredible implementers but terrible strategists. Humans are incredible strategists but slow implementers. Play to each's strengths.
The secret: The framework itself can be executed by AI agents, with human oversight. You're reading a document that an AI agent can use to guide a human through building anything.
Meta-level: This framework builds itself. Use CSE to improve CSE.

Getting Started Checklist
Ready to use CSE on your next project? Start here:
Pre-Flight Check:

 I have an idea (even if vague)
 I have 20 hours to invest (research + review)
 I have access to AI agents (Claude, ChatGPT, etc.)
 I'm willing to let AI do the grunt work
 I can make decisions when presented options

Phase 0 Start:

 Write down your idea (1 paragraph, imperfect is fine)
 Ask AI to mirror it back comprehensively
 Score the understanding (1-100)
 Clarify what AI got wrong
 Repeat until you score 95%+

Phase 1 Start:

 Answer: "What's the primary revenue source?"
 Answer: "Who will pay the most?"
 Answer: "What can I charge?"
 Let AI ask follow-up questions
 Give percentages, not binary answers

You're Now Using CSE ðŸŽ‰
The rest will flow naturally. Trust the process.